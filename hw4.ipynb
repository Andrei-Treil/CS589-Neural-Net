{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,nodes,lamb=0.0,alpha=0.1,eps=0.0):\n",
    "        '''\n",
    "        Constructor for neural net\n",
    "        nodes - list detailing number of nodes in each layer\n",
    "        lamb - regularization\n",
    "        alpha - learning rate\n",
    "        eps - cost function stopping condition\n",
    "        '''\n",
    "        self.nodes = nodes\n",
    "        self.lamb = lamb\n",
    "        self.alpha = alpha\n",
    "        self.weights = []\n",
    "        #initialize weights for each layer, include bias\n",
    "        for i in range(len(nodes)-1):\n",
    "            self.weights.append(np.random.normal(0,1,(nodes[i]+1,nodes[i+1])))\n",
    "    \n",
    "    def get_sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def deriv_sigmoid(self, x):\n",
    "        return x * (1-x)\n",
    "\n",
    "    def train(self, data):\n",
    "        prev_cost = -math.inf\n",
    "        total_delta = [0*len(self.weights)]\n",
    "        num_inst = len(data)\n",
    "        keep_learn = True\n",
    "\n",
    "        while(keep_learn):\n",
    "            J = 0\n",
    "            for instance,target in data:\n",
    "                #iterate through layers, vectorize forward pass\n",
    "                activations = [instance]\n",
    "                for i in range(len(self.weights)):\n",
    "                    activations.append(self.get_sigmoid(activations[i].dot(self.weights[i])))\n",
    "\n",
    "                guess = activations[-1]\n",
    "\n",
    "                #accumulate sum loss\n",
    "                J += np.sum((-target).dot(math.log(guess)) - (1-target).dot(math.log(1-guess)))\n",
    "\n",
    "                #begin backwards propogation\n",
    "                error = guess - target\n",
    "                delta_inst = [error * self.deriv_sigmoid(guess)]\n",
    "\n",
    "                #get delta values for all weights on current instance\n",
    "                for i in range(len(activations)-1, 1, -1):\n",
    "                    this_del = (self.weights[i].T.dot(delta_inst[-1])) * self.deriv_sigmoid(activations[i])\n",
    "                    delta_inst.append(this_del)\n",
    "\n",
    "                #reverse delta values\n",
    "                delta_inst = delta_inst[::-1]\n",
    "\n",
    "                #accumulate gradients\n",
    "                for i in range(len(activations)-1,0,-1):\n",
    "                    total_delta[i] += delta_inst[i+1].dot(activations[i].T)\n",
    "            \n",
    "            #regularize weights and update\n",
    "            for i in range(len(self.weights)-1,0,-1):\n",
    "                P = self.lamb.dot(self.weights[i])\n",
    "                total_delta[i] += P\n",
    "                total_delta[i] /= num_inst\n",
    "                self.weights[i] -= self.alpha * total_delta[i]\n",
    "\n",
    "            J /= num_inst\n",
    "            curr_s = np.sum(self.weights[1:]**2)\n",
    "            curr_s *= (self.lamb/(2*num_inst))\n",
    "            new_cost = J + curr_s\n",
    "\n",
    "            #if improvement in cost is less than epsilon, stop\n",
    "            if new_cost - prev_cost < self.eps:\n",
    "                keep_learn = False\n",
    "\n",
    "            prev_cost = new_cost\n",
    "\n",
    "    def calculate_loss(self,data,targets):\n",
    "        predictions = aaaa\n",
    "\n",
    "    def predict(self,instance):\n",
    "        pred = [np.ones(len(instance)),instance]\n",
    "\n",
    "        for i in len(self.weights):\n",
    "            pred = self.sigmoid(np.dot(pred,self.weights[i]))\n",
    "        \n",
    "        return pred\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "    - Implement calculate_loss\n",
    "    - Debug using sample data\n",
    "    - Swag, nae nae, and finesse\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "#implement backprogagation algorithm in a way that allows you to specify how many layers and neurons you would like your neural network to have\n",
    "#https://pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/\n",
    "test = [1,2,3,4,5]\n",
    "out = []\n",
    "for i in range(len(test),0,-1):\n",
    "    out.append(i)\n",
    "#print(':3')\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
