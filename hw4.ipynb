{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,nodes,lamb=0.0,alpha=0.1,eps=0.0):\n",
    "        '''\n",
    "        Constructor for neural net\n",
    "        nodes - list detailing number of nodes in each layer\n",
    "        lamb - regularization\n",
    "        alpha - learning rate\n",
    "        eps - cost function stopping condition\n",
    "        '''\n",
    "        self.nodes = nodes\n",
    "        self.lamb = lamb\n",
    "        self.alpha = alpha\n",
    "        self.weights = []\n",
    "        self.eps = eps\n",
    "        #initialize weights for each layer, include bias\n",
    "        for i in range(len(nodes)-1):\n",
    "            self.weights.append(np.random.normal(0,1,(nodes[i]+1,nodes[i+1])).T)\n",
    "    \n",
    "    def get_sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def deriv_sigmoid(self, x):\n",
    "        return x * (1-x)\n",
    "\n",
    "    def train(self, features, targs, batch_size, test_feat=None, test_targs=None, for_exam=False, get_costs=False):\n",
    "        '''\n",
    "        features - training data features\n",
    "        targs - training data targets\n",
    "        batch size - # of instances for mini batch\n",
    "        test_feat - test data features\n",
    "        test_targs - test data targets\n",
    "        for_exam - flag to print for back_prop examples\n",
    "        get_costs - flag to get J values for varying number of samples\n",
    "        '''\n",
    "        prev_cost = math.inf\n",
    "        gradients = [0]*len(self.weights)\n",
    "        num_inst = len(targs)\n",
    "        keep_learn = True\n",
    "        count = 1\n",
    "        curr_batch = 1\n",
    "        cost_j = []\n",
    "        cost_j_count = []\n",
    "\n",
    "        while(keep_learn):\n",
    "            J = 0\n",
    "            for instance,target in zip(features,targs):\n",
    "                #iterate through layers, vectorize forward pass\n",
    "                activations = [np.atleast_2d(instance)]\n",
    "                for i in range(len(self.weights)-1):\n",
    "                    try:\n",
    "                        this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "                    except:\n",
    "                        this_a = self.get_sigmoid(self.weights[i].T.dot(activations[i].T))\n",
    "                    activations.append(np.insert(this_a,0,1))\n",
    "                try:\n",
    "                    activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "                except:\n",
    "                    activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1].T)))\n",
    "                guess = activations[-1]\n",
    "\n",
    "                #accumulate sum loss\n",
    "                target = np.array(target)\n",
    "                cost = np.sum((np.array(-target)).dot(np.log(guess)) - (np.array(1-target)).dot(np.log(1-guess)))\n",
    "                J += cost\n",
    "\n",
    "                #begin backwards propogation\n",
    "                error = guess - target\n",
    "                delta_inst = [error]\n",
    "\n",
    "                #get delta values for all weights on current instance\n",
    "                for i in range(len(self.weights)-1, 0, -1):\n",
    "                    try:\n",
    "                        this_del = (self.weights[i].T.dot(delta_inst[-1])) * self.deriv_sigmoid(activations[i].T)\n",
    "                    except:\n",
    "                        this_del = (self.weights[i].dot(delta_inst[-1])) * self.deriv_sigmoid(activations[i].T)\n",
    "                    delta_inst.append(this_del[1:])\n",
    "\n",
    "                #reverse delta values\n",
    "                delta_inst = delta_inst[::-1]\n",
    "\n",
    "                #accumulate gradients\n",
    "                for i in range(len(self.weights)-1,-1,-1):\n",
    "                    try:\n",
    "                        gradients[i] += (delta_inst[i]*(activations[i].T)).T\n",
    "                    except:\n",
    "                        gradients[i] += (np.atleast_2d(delta_inst[i]).T*np.atleast_2d(activations[i].T))\n",
    "\n",
    "                #print for examples\n",
    "                if for_exam:\n",
    "                    print(f'OUTPUTS FOR INSTANCE {count}')\n",
    "                    print(f'activations: ')\n",
    "                    for i in range(len(activations)):\n",
    "                        print(f'a{i+1}: {activations[i]}')\n",
    "                    print()\n",
    "                    print(f'prediction: {guess}')\n",
    "                    print(f'expected: {target}')\n",
    "                    print(f'cost J: {cost}')\n",
    "                    print()\n",
    "                    print('delta for this instance: ')\n",
    "                    for i in range(len(delta_inst)):\n",
    "                        print(f'delta {i+2}: {delta_inst[i]}')\n",
    "                    print()\n",
    "                    print('gradients for this instance: ')\n",
    "                    for i in range(len(self.weights)):\n",
    "                        try:\n",
    "                            print_del = (delta_inst[i]*(activations[i].T)).T\n",
    "                        except:\n",
    "                            print_del = (np.atleast_2d(delta_inst[i]).T*np.atleast_2d(activations[i].T)).T\n",
    "                        print(f'theta {i+1}: {print_del}')\n",
    "                    print()\n",
    "\n",
    "                if curr_batch == batch_size or count == num_inst:\n",
    "                    #regularize weights and update\n",
    "                    for i in range(len(self.weights)-1,-1,-1):\n",
    "                        P = self.lamb * (self.weights[i])\n",
    "                        #set first column to all 0\n",
    "                        P[:,0] = 0\n",
    "                        try:\n",
    "                            gradients[i] = gradients[i] + P.T\n",
    "                        except:\n",
    "                            gradients[i] = gradients[i] + P\n",
    "                        gradients[i] = gradients[i] / num_inst\n",
    "                        learn_diff = self.alpha * (gradients[i])\n",
    "                        try:\n",
    "                            self.weights[i] = self.weights[i] - learn_diff\n",
    "                        except:\n",
    "                            self.weights[i] = self.weights[i] - learn_diff.T\n",
    "                    curr_batch = 0\n",
    "\n",
    "                    if get_costs:\n",
    "                        cost_j.append(self.cost_on_set(test_feat,test_targs))\n",
    "                        cost_j_count.append(count)\n",
    "\n",
    "                curr_batch += 1\n",
    "                count += 1\n",
    "\n",
    "            J /= num_inst\n",
    "            curr_s = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                curr_s += np.sum(self.weights[i][1:]**2)\n",
    "\n",
    "            #curr_s = np.sum(self.weights[1:]**2)\n",
    "            curr_s *= (self.lamb/(2*num_inst))\n",
    "            new_cost = J + curr_s\n",
    "\n",
    "            #if improvement in cost is less than epsilon, stop\n",
    "            if prev_cost - new_cost < self.eps:\n",
    "                keep_learn = False\n",
    "\n",
    "            prev_cost = new_cost\n",
    "\n",
    "            if for_exam:\n",
    "                print('regularized gradients: ')\n",
    "                for i in range(len(gradients)):\n",
    "                    print(f'theta {i+1}: {gradients[i]}')\n",
    "                keep_learn = False\n",
    "            if get_costs:\n",
    "                return cost_j,cost_j_count\n",
    "\n",
    "    #forward pass on one instance, returns an array where index of max val is the NN's guess and 0 for all else\n",
    "    #raw - True if wanting the raw outputs, false if wanting outputed in one hot encoding\n",
    "    def predict(self,instance,raw=True):\n",
    "        activations = [np.atleast_2d(instance)]\n",
    "        for i in range(len(self.weights)-1):\n",
    "            try:\n",
    "                this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "            except:\n",
    "                this_a = self.get_sigmoid(self.weights[i].T.dot(activations[i].T))\n",
    "            activations.append(np.insert(this_a,0,1))\n",
    "        try:\n",
    "            activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "        except:\n",
    "            activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1].T)))\n",
    "        guess = activations[-1]\n",
    "        pred = [0]*len(guess)\n",
    "        pred[np.argmax(guess)] = 1\n",
    "        \n",
    "        return guess if raw else pred\n",
    "    \n",
    "    def cost_on_set(self,instances,targets):\n",
    "        J = 0\n",
    "        for instance,target in zip(instances,targets):\n",
    "            guess = self.predict(instance)\n",
    "            target = np.array(target)\n",
    "            cost = np.sum((np.array(-target)).dot(np.log(guess)) - (np.array(1-target)).dot(np.log(1-guess)))\n",
    "            J += cost\n",
    "        J /= len(instances)\n",
    "        curr_s = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            curr_s += np.sum(self.weights[i][1:]**2)\n",
    "\n",
    "        curr_s *= (self.lamb/(2*len(instances)))\n",
    "        return J + curr_s\n",
    "    \n",
    "def test_decision(nn,test_set,vals):\n",
    "    test_copy = pd.DataFrame(test_set,copy=True)\n",
    "    to_guess = test_copy.drop('class',axis=1)\n",
    "    predictions = pd.DataFrame(to_guess.apply(lambda row: nn.predict(row.to_numpy(),raw=False), axis=1),columns=['predicted'])\n",
    "    predictions['actual'] = test_set.loc[predictions.index,'class']\n",
    "    prec,rec,f1 = [0,0,0]\n",
    "\n",
    "    for val in vals:\n",
    "        is_targ = predictions[predictions.predicted.apply(lambda x: x == val)]\n",
    "        not_targ = predictions[predictions.predicted.apply(lambda x: x != val)]\n",
    "        tp = len(is_targ[is_targ['predicted'] == is_targ['actual']])\n",
    "        fp = len(is_targ[is_targ['predicted'] != is_targ['actual']])\n",
    "        fn = len(not_targ[not_targ.actual.apply(lambda x: x == val)])\n",
    "        tn = len(not_targ[not_targ.actual.apply(lambda x: x != val)])\n",
    "        this_prec = (tp/(tp+fp)) if (tp+fp) > 0 else 0\n",
    "        this_rec = (tp/(tp+fn)) if (tp+fn) > 0 else 0\n",
    "        f1 += (this_prec*this_rec*2)/(this_rec+this_prec) if (this_rec+this_prec) > 0 else 0\n",
    "        prec += this_prec\n",
    "        rec += this_rec\n",
    "\n",
    "    avg_f1 = f1/len(vals)\n",
    "    accuracy = len(predictions[predictions['predicted'] == predictions['actual']])/len(test_set)\n",
    "    return accuracy,avg_f1\n",
    "\n",
    "np.random.seed(1)\n",
    "k = 10\n",
    "#function to do cross fold validation\n",
    "def k_fold(fold,vals,nn_arc,lamb,eps,alpha,batch_size,get_j=False):\n",
    "    fold_metrics = defaultdict(list)\n",
    "    #iterate through folds, taking turns being test fold\n",
    "    for i in range(k):\n",
    "        test_fold = fold[i]\n",
    "        test_targs = test_fold['class']\n",
    "        test_feat = test_fold.drop('class',axis=1)\n",
    "        train_fold = fold[0:i]\n",
    "        train_fold.extend(fold[i+1:len(fold)])\n",
    "        train_data = pd.concat(train_fold)\n",
    "       \n",
    "        #iterate through architectures\n",
    "        for arc in nn_arc:\n",
    "            np_targs = train_data['class'].to_numpy()\n",
    "            np_inst = train_data.drop('class',axis=1).to_numpy()\n",
    "            this_nn = NeuralNet(arc,lamb,alpha,eps)\n",
    "            if get_j:\n",
    "                return this_nn.train(np_inst,np_targs,batch_size,test_feat.to_numpy(),test_targs.to_numpy(),get_costs=True)\n",
    "            this_nn.train(np_inst,np_targs,batch_size)\n",
    "            fold_metrics[str(arc)].append(test_decision(this_nn,test_fold,vals))\n",
    "            \n",
    "    return fold_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Implementation\n",
    "- Stopping criteria: change in cost function, epsilon\n",
    "- The ```train()``` function of the NeuralNet class is the entirety of my training implementation\n",
    "- My implementation is the vectorized form of neural networks\n",
    "\n",
    "### Correctness Verification\n",
    "\n",
    "- Below I have included 2 functions: ```train_on_first()``` and ```train_on_sec()```\n",
    "- These functions hard code the inputs and print the desired outputs to stdout. If the output is too large for your IDE, set the max lines of your output to 100. To run these functions, simply call them without arguement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS FOR INSTANCE 1\n",
      "activations: \n",
      "a1: [[1.   0.13]]\n",
      "a2: [1.        0.601807  0.5807858]\n",
      "a3: [0.79402743]\n",
      "\n",
      "prediction: [0.79402743]\n",
      "expected: [0.9]\n",
      "cost J: 0.36557477431084995\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [-0.01269739 -0.01548092]\n",
      "delta 3: [-0.10597257]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[-0.01269739 -0.00165066]\n",
      " [-0.01548092 -0.00201252]]\n",
      "theta 2: [-0.10597257 -0.06377504 -0.06154737]\n",
      "\n",
      "OUTPUTS FOR INSTANCE 2\n",
      "activations: \n",
      "a1: [[1.   0.42]]\n",
      "a2: [1.         0.60873549 0.59483749]\n",
      "a3: [0.79596607]\n",
      "\n",
      "prediction: [0.79596607]\n",
      "expected: [0.23]\n",
      "cost J: 1.2763768066887786\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [0.06739994 0.08184068]\n",
      "delta 3: [0.56596607]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[0.06739994 0.02830797]\n",
      " [0.08184068 0.03437309]]\n",
      "theta 2: [0.56596607 0.34452363 0.33665784]\n",
      "\n",
      "regularized gradients: \n",
      "theta 1: [[0.02735127 0.01332866]\n",
      " [0.03317988 0.01618028]]\n",
      "theta 2: [[0.22999675 0.1403743  0.13755523]]\n"
     ]
    }
   ],
   "source": [
    "#function to train on backprop_example1.txt\n",
    "#outputs to stdout, if full output cannot be viewed when calling this function, increase number of lines shown in output to 100\n",
    "def train_on_first():\n",
    "\ttrain_nn = NeuralNet([1,2,1],eps=0.001)\n",
    "\ttrain_nn.weights[0] = np.array([[0.40000,0.10000 ],[0.30000,0.20000 ]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.7],[0.5],[0.6]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\t#NOTE: X values are preprocessed to include bias term (1) as first element\n",
    "\tX = np.array([[1,0.13000],[1,0.42000]])\n",
    "\tY = np.array([[0.90000],[0.23000]])\n",
    "\ttrain_df = pd.DataFrame(data=train_set_1)\n",
    "\ttrain_df.insert(0,'bias',np.ones)\n",
    "\ttrain_nn.train(X,Y,2,for_exam=True)\n",
    "\n",
    "train_on_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS FOR INSTANCE 1\n",
      "activations: \n",
      "a1: [[1.   0.32 0.68]]\n",
      "a2: [1.         0.67699586 0.75384029 0.5881687  0.70566042]\n",
      "a3: [1.         0.87519469 0.89296181 0.81480444]\n",
      "a4: [0.83317658 0.84131543]\n",
      "\n",
      "prediction: [0.83317658 0.84131543]\n",
      "expected: [0.75 0.98]\n",
      "cost J: 0.7907366961135718\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [-0.00086743 -0.00133354 -0.00053312 -0.00070163]\n",
      "delta 3: [ 0.00638937 -0.00925379 -0.00778767]\n",
      "delta 4: [ 0.08317658 -0.13868457]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[-0.00086743 -0.00027758 -0.00058985]\n",
      " [-0.00133354 -0.00042673 -0.00090681]\n",
      " [-0.00053312 -0.0001706  -0.00036252]\n",
      " [-0.00070163 -0.00022452 -0.00047711]]\n",
      "theta 2: [[ 0.00638937 -0.00925379 -0.00778767]\n",
      " [ 0.00432557 -0.00626478 -0.00527222]\n",
      " [ 0.00481656 -0.00697588 -0.00587066]\n",
      " [ 0.00375802 -0.00544279 -0.00458046]\n",
      " [ 0.00450872 -0.00653003 -0.00549545]]\n",
      "theta 3: [[ 0.08317658 -0.13868457]\n",
      " [ 0.0727957  -0.121376  ]\n",
      " [ 0.07427351 -0.12384003]\n",
      " [ 0.06777264 -0.1130008 ]]\n",
      "\n",
      "OUTPUTS FOR INSTANCE 2\n",
      "activations: \n",
      "a1: [[1.   0.83 0.02]]\n",
      "a2: [1.         0.63471542 0.69291867 0.54391158 0.64659376]\n",
      "a3: [1.         0.86020091 0.88336451 0.79790763]\n",
      "a4: [0.82952703 0.83831889]\n",
      "\n",
      "prediction: [0.82952703 0.83831889]\n",
      "expected: [0.75 0.28]\n",
      "cost J: 1.9437823352945294\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [0.01694006 0.01465141 0.01998824 0.01622017]\n",
      "delta 3: [0.01503437 0.05808969 0.06891698]\n",
      "delta 4: [0.07952703 0.55831889]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[0.01694006 0.01406025 0.0003388 ]\n",
      " [0.01465141 0.01216067 0.00029303]\n",
      " [0.01998824 0.01659024 0.00039976]\n",
      " [0.01622017 0.01346274 0.0003244 ]]\n",
      "theta 2: [[0.01503437 0.05808969 0.06891698]\n",
      " [0.00954254 0.03687042 0.04374267]\n",
      " [0.01041759 0.04025143 0.04775386]\n",
      " [0.00817737 0.03159565 0.03748474]\n",
      " [0.00972113 0.03756043 0.04456129]]\n",
      "theta 3: [[0.07952703 0.55831889]\n",
      " [0.06840922 0.48026642]\n",
      " [0.07025135 0.4931991 ]\n",
      " [0.06345522 0.44548691]]\n",
      "\n",
      "regularized gradients: \n",
      "theta 1: [[0.00803632 0.02564134 0.04987447]\n",
      " [0.00665894 0.01836697 0.06719311]\n",
      " [0.00972756 0.03195982 0.05251862]\n",
      " [0.00775927 0.05036911 0.08492365]]\n",
      "theta 2: [[0.01071187 0.09068406 0.02511708 0.1259677  0.11586492]\n",
      " [0.02441795 0.06780282 0.04163777 0.05307643 0.1267652 ]\n",
      " [0.03056466 0.08923522 0.1209416  0.10270214 0.03078292]]\n",
      "theta 3: [[0.0813518  0.17935246 0.12476243 0.13186393]\n",
      " [0.20981716 0.19194521 0.30342954 0.25249305]]\n"
     ]
    }
   ],
   "source": [
    "#function to train on backprop_example2.txt\n",
    "#outputs to stdout, if full output cannot be viewed when calling this function, increase number of lines shown in output to 100\n",
    "def train_on_sec():\n",
    "\ttrain_nn = NeuralNet([2,4,3,2],eps=0.001,lamb=0.250)\n",
    "\ttrain_nn.weights[0] = np.array([[0.42000,0.15000,0.40000],[0.72000,0.10000,0.54000],[0.01000,0.19000,0.42000],[0.30000,0.35000,0.68000]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.21000,0.67000,0.14000,0.96000,0.87000],[0.87000,0.42000,0.20000,0.32000,0.89000],[0.03000,0.56000,0.80000,0.69000,0.09000]])\n",
    "\ttrain_nn.weights[2] = np.array([[0.04000,0.87000,0.42000,0.53000],[0.17000,0.10000,0.95000,0.69000]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\t#NOTE: X values are preprocessed to include bias term (1) as first element\n",
    "\tX = np.array([[1,0.32000,0.68000],[1,0.83000,0.02000]])\n",
    "\tY = np.array([[0.75000,0.98000],[0.75000,0.28000]])\n",
    "\ttrain_nn.train(X,Y,batch_size=2,for_exam=True)\n",
    "\n",
    "#train_on_first()\n",
    "train_on_sec()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of neural net architectures\n",
    "wine_nn_arc = [[13,4,3],[13,8,3],[13,8,8,3],[13,8,16,3],[13,8,16,8,3],[13,4,8,8,4,3]]\n",
    "wine_df = pd.read_csv('datasets/hw3_wine.csv',delimiter='\\t')\n",
    "norm_wine_df = ((wine_df-wine_df.min())/(wine_df.max()-wine_df.min()))\n",
    "#insert column of ones to act as bias\n",
    "norm_wine_df.insert(0,'bias',1)\n",
    "\n",
    "#split data by class into k groups then combine into folds\n",
    "wine_class_1 = norm_wine_df.loc[norm_wine_df['class'] == 0].sample(frac=1)\n",
    "wine_class_1['class'] = [[1,0,0]]*len(wine_class_1)\n",
    "wc1_split = np.array_split(wine_class_1,k)\n",
    "wine_class_2 = norm_wine_df.loc[norm_wine_df['class'] == 0.5].sample(frac=1)\n",
    "wine_class_2['class'] = [[0,1,0]]*len(wine_class_2)\n",
    "wc2_split = np.array_split(wine_class_2,k)\n",
    "wine_class_3 = norm_wine_df.loc[norm_wine_df['class'] == 1].sample(frac=1)\n",
    "wine_class_3['class'] = [[0,0,1]]*len(wine_class_3)\n",
    "wc3_split = np.array_split(wine_class_3,k)\n",
    "wine_vals = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "\n",
    "#list to hold folds\n",
    "wine_fold = []\n",
    "for i in range(k):\n",
    "    this_fold = [wc1_split[i],wc2_split[i],wc3_split[i]]\n",
    "    wine_fold.append(pd.concat(this_fold))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3**  create, for each dataset and for each of the metrics\n",
    "described above, a table summarizing the corresponding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.1 eps = 0.001 alpha = 3 batch_size = 5\n",
      "          Architecture  Accuracy        F1\n",
      "0           [13, 4, 3]  0.960382  0.961588\n",
      "1           [13, 8, 3]  0.972515  0.973781\n",
      "2        [13, 8, 8, 3]  0.678993  0.608028\n",
      "3       [13, 8, 16, 3]  0.746616  0.699122\n",
      "4    [13, 8, 16, 8, 3]  0.399254  0.190154\n",
      "5  [13, 4, 8, 8, 4, 3]  0.399254  0.190154\n",
      "lamb = 0.1 eps = 0.001 alpha = 5 batch_size = 5\n",
      "          Architecture  Accuracy        F1\n",
      "0           [13, 4, 3]  0.966265  0.967731\n",
      "1           [13, 8, 3]  0.966265  0.967731\n",
      "2        [13, 8, 8, 3]  0.622457  0.512777\n",
      "3       [13, 8, 16, 3]  0.703049  0.644960\n",
      "4    [13, 8, 16, 8, 3]  0.415921  0.217700\n",
      "5  [13, 4, 8, 8, 4, 3]  0.399254  0.190154\n",
      "lamb = 0.05 eps = 0.0001 alpha = 3 batch_size = 5\n",
      "          Architecture  Accuracy        F1\n",
      "0           [13, 4, 3]  0.983333  0.984025\n",
      "1           [13, 8, 3]  0.983333  0.984025\n",
      "2        [13, 8, 8, 3]  0.925439  0.903778\n",
      "3       [13, 8, 16, 3]  0.977778  0.978430\n",
      "4    [13, 8, 16, 8, 3]  0.753158  0.685844\n",
      "5  [13, 4, 8, 8, 4, 3]  0.399254  0.190154\n",
      "lamb = 0.05 eps = 0.0001 alpha = 5 batch_size = 5\n",
      "          Architecture  Accuracy        F1\n",
      "0           [13, 4, 3]  0.983333  0.984025\n",
      "1           [13, 8, 3]  0.978070  0.979034\n",
      "2        [13, 8, 8, 3]  0.961404  0.961823\n",
      "3       [13, 8, 16, 3]  0.966959  0.967869\n",
      "4    [13, 8, 16, 8, 3]  0.712448  0.638396\n",
      "5  [13, 4, 8, 8, 4, 3]  0.399254  0.190154\n"
     ]
    }
   ],
   "source": [
    "def wine_test(lamb,eps,alpha,batch_size):\n",
    "    wine_res = k_fold(wine_fold,wine_vals,wine_nn_arc,lamb,eps,alpha,batch_size)\n",
    "    arc_dict = defaultdict(list)\n",
    "    print(f'lamb = {lamb} eps = {eps} alpha = {alpha} batch_size = {batch_size}')\n",
    "\n",
    "    for arc,perf in wine_res.items():\n",
    "        avg_acc,avg_f1 = [0,0]\n",
    "        for res in perf:\n",
    "            avg_acc += res[0]\n",
    "            avg_f1 += res[1]\n",
    "        arc_dict['Architecture'].append(arc)\n",
    "        arc_dict['Accuracy'].append(avg_acc/10)\n",
    "        arc_dict['F1'].append(avg_f1/10)\n",
    "\n",
    "    arc_table = pd.DataFrame(arc_dict)\n",
    "    print(arc_table)\n",
    "\n",
    "hyper_params = [[0.1,0.001,3,5],[0.1,0.001,5,5],[0.05,0.0001,3,5],[0.05,0.0001,5,5]]\n",
    "for params in hyper_params:\n",
    "    wine_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4** Discuss (on a high level) what contributed the most to improving performance: changing the\n",
    "regularization parameter; adding more layers; having deeper networks with many layers but few\n",
    "neurons per layer? designing networks with few layers but many neurons per layer? Discuss\n",
    "any patterns that you may have encountered. Also, discuss whether there is a point where\n",
    "constructing and training more “sophisticated”/complex networks—i.e., larger networks—no\n",
    "longer improves performance (or worsens performance)\n",
    "\n",
    ">From running 4 different combinations of hyper parameters, I found that setting lambda = 0.05 yielded the best results. Additionally, having > 1 hidden layer in the network showed a significant decrease in performance for this data set. Of the architectures tested with 1 hidden layer, using 8 nodes yielded the highest results, however using 4 nodes would yield higher accuracy in certain conditions. Alpha = 3  and eps = 0.0001 seemed to yield the best results. In general, using an architecture with > 1 layer had the highest impact on performance out of all adjustable parameters that I tested.\n",
    "\n",
    "**1.5** Based on the analyses above, discuss which neural network architecture you would select if you\n",
    "had to deploy such a classifier in real life. Explain your reasoning\n",
    "\n",
    ">If I had to deploy this classifier in real life, I would choose an architecture with 1 hidden layer containing 8 nodes as this architecture performed best on average accross various settings of hyper parameters. Additionally, I would use a lambda value of 0.05 as this also improved the accuracy of architectures with > 1 hidden layer.\n",
    "\n",
    "**1.6** After identifying the best neural network architecture for each one of the datasets, train it once\n",
    "again on the corresponding dataset and create a learning curve where the y axis shows the\n",
    "network’s performance (J) on a test set, and where the x axis indicates the number of training\n",
    "samples given to the network.\n",
    "\n",
    "> Using architecture [13,8,3] with alpha = 3, testing cost J every 5 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wine_j,wine_count = k_fold(wine_fold,wine_vals,[[13,8,3]],0.05,0.0001,3,5,get_j=True)\n",
    "plt.plot(wine_count,wine_j)\n",
    "plt.xlabel('Number of Training Instances')\n",
    "plt.ylabel('Performance (J) on Test Set')\n",
    "plt.title('Wine Data Performance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. House Votes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of neural net architectures\n",
    "house_nn_arc = [[16,32,2],[16,8,16,2],[16,32,32,2],[16,32,16,8,2],[16,32,64,32,2],[16,8,16,32,8,2]]\n",
    "house_df = pd.read_csv('datasets/hw3_house_votes_84.csv',delimiter=',')\n",
    "norm_house_df = ((house_df-house_df.min())/(house_df.max()-house_df.min()))\n",
    "#insert column of ones to act as bias\n",
    "norm_house_df.insert(0,'bias',1)\n",
    "\n",
    "#split data by class into k groups then combine into folds\n",
    "house_class_1 = norm_house_df.loc[norm_house_df['class'] == 0].sample(frac=1)\n",
    "house_class_1['class'] = [[1,0]]*len(house_class_1)\n",
    "hc1_split = np.array_split(house_class_1,k)\n",
    "house_class_2 = norm_house_df.loc[norm_house_df['class'] == 1].sample(frac=1)\n",
    "house_class_2['class'] = [[0,1]]*len(house_class_2)\n",
    "hc2_split = np.array_split(house_class_2,k)\n",
    "house_vals = [[1,0],[0,1]]\n",
    "\n",
    "#list to hold folds\n",
    "house_fold = []\n",
    "for i in range(k):\n",
    "    this_fold = [hc1_split[i],hc2_split[i]]\n",
    "    house_fold.append(pd.concat(this_fold))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.025 eps = 0.0001 alpha = 1 batch_size = 5\n",
      "            Architecture  Accuracy        F1\n",
      "0            [16, 32, 2]  0.933552  0.929769\n",
      "1         [16, 8, 16, 2]  0.942751  0.939358\n",
      "2        [16, 32, 32, 2]  0.938098  0.934409\n",
      "3     [16, 32, 16, 8, 2]  0.945130  0.941814\n",
      "4    [16, 32, 64, 32, 2]  0.940532  0.937075\n",
      "5  [16, 8, 16, 32, 8, 2]  0.906441  0.879995\n",
      "lamb = 0.025 eps = 0.0001 alpha = 1 batch_size = 10\n",
      "            Architecture  Accuracy        F1\n",
      "0            [16, 32, 2]  0.935825  0.932259\n",
      "1         [16, 8, 16, 2]  0.935878  0.931851\n",
      "2        [16, 32, 32, 2]  0.938259  0.934084\n",
      "3     [16, 32, 16, 8, 2]  0.945238  0.942118\n",
      "4    [16, 32, 64, 32, 2]  0.951948  0.948932\n",
      "5  [16, 8, 16, 32, 8, 2]  0.879060  0.830429\n",
      "lamb = 0.025 eps = 1e-05 alpha = 1 batch_size = 5\n",
      "            Architecture  Accuracy        F1\n",
      "0            [16, 32, 2]  0.935770  0.932195\n",
      "1         [16, 8, 16, 2]  0.942857  0.939264\n",
      "2        [16, 32, 32, 2]  0.947458  0.944295\n",
      "3     [16, 32, 16, 8, 2]  0.944969  0.941794\n",
      "4    [16, 32, 64, 32, 2]  0.940532  0.937135\n",
      "5  [16, 8, 16, 32, 8, 2]  0.876787  0.827177\n",
      "lamb = 0.025 eps = 1e-05 alpha = 1 batch_size = 10\n",
      "            Architecture  Accuracy        F1\n",
      "0            [16, 32, 2]  0.935825  0.931867\n",
      "1         [16, 8, 16, 2]  0.935717  0.932071\n",
      "2        [16, 32, 32, 2]  0.938151  0.934400\n",
      "3     [16, 32, 16, 8, 2]  0.942804  0.939393\n",
      "4    [16, 32, 64, 32, 2]  0.940207  0.936385\n",
      "5  [16, 8, 16, 32, 8, 2]  0.829493  0.755607\n"
     ]
    }
   ],
   "source": [
    "def house_test(lamb,eps,alpha,batch_size):\n",
    "    house_res = k_fold(house_fold,house_vals,house_nn_arc,lamb,eps,alpha,batch_size)\n",
    "    print(f'lamb = {lamb} eps = {eps} alpha = {alpha} batch_size = {batch_size}')\n",
    "    arc_dict_h = defaultdict(list)\n",
    "\n",
    "    for arc,perf in house_res.items():\n",
    "        avg_acc,avg_f1 = [0,0]\n",
    "        for res in perf:\n",
    "            avg_acc += res[0]\n",
    "            avg_f1 += res[1]\n",
    "        arc_dict_h['Architecture'].append(arc)\n",
    "        arc_dict_h['Accuracy'].append(avg_acc/10)\n",
    "        arc_dict_h['F1'].append(avg_f1/10)\n",
    "\n",
    "    arc_table_h = pd.DataFrame(arc_dict_h)\n",
    "    print(arc_table_h)\n",
    "\n",
    "#hyper_params = [[0.05,0.001,3,5],[0.05,0.0001,5,5],[0.05,0.0001,3,5],[0.05,0.0001,5,5]]\n",
    "hyper_params_h = [[0.025,0.0001,1,5],[0.025,0.0001,1,10],[0.025,0.00001,1,5],[0.025,0.00001,1,10]]\n",
    "for params in hyper_params_h:\n",
    "    house_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Discuss (on a high level) what contributed the most to improving performance: changing the\n",
    "regularization parameter; adding more layers; having deeper networks with many layers but few\n",
    "neurons per layer? designing networks with few layers but many neurons per layer? Discuss\n",
    "any patterns that you may have encountered. Also, discuss whether there is a point where\n",
    "constructing and training more “sophisticated”/complex networks—i.e., larger networks—no\n",
    "longer improves performance (or worsens performance)\n",
    "\n",
    ">From running testing a variety of parameters I found that using lambda = 0.25 improved accuracy consistently accross my models. Additionally, I found that seting alpha > 1 lead to significant decreases in accuracy. I was able to achieve my best results by using 3 nodes in my hidden layer, with the structures [16,32,64,32,2] with a batch size of 10. Higher batch sizes seemed to affect the accuracy of more complicated models. For this data set, using more complex architectures, with higher amounts of nodes and layers yielded the best results, and changing lambda and batch size had the highest impact on accuracy.\n",
    "\n",
    "**2.5** Based on the analyses above, discuss which neural network architecture you would select if you\n",
    "had to deploy such a classifier in real life. Explain your reasoning\n",
    "\n",
    ">If I had to deploy this classifier in real life, I would choose an architecture with 3 hidden layer [16,32,64,32,2]. Additionally, I would choose a lambda of 0.025 with an epsilon of 0.0001, alpha = 1, and batch size = 10. Accross all tests I ran, this configuration consistently yieled ~94% accuracy, which is why I would deploy it in real life. It is also important to note that the other architecture I tested with 3 hidden layers yielded similarly accurate results, so I would also consider using it in real deployment.\n",
    "\n",
    "**2.6** After identifying the best neural network architecture for each one of the datasets, train it once\n",
    "again on the corresponding dataset and create a learning curve where the y axis shows the\n",
    "network’s performance (J) on a test set, and where the x axis indicates the number of training\n",
    "samples given to the network.\n",
    "\n",
    "> Using architecture [16,32,64,32,2] with alpha = 1, testing cost J every 10 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "house_j,house_count = k_fold(house_fold,house_vals,[[16,32,64,32,2]],0.025,0.0001,1,10,get_j=True)\n",
    "plt.plot(house_count,house_j)\n",
    "plt.xlabel('Number of Training Instances')\n",
    "plt.ylabel('Performance (J) on Test Set')\n",
    "plt.title('House Votes Data Performance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52fef9f371462526e6c16fbedf2b5b9bc32d90752958a415cf9672d41d7c1c70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
