{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO: \\n    - Implement calculate_loss\\n    - Debug using sample data\\n    - Swag, nae nae, and finesse\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,nodes,lamb=0.0,alpha=0.1,eps=0.0):\n",
    "        '''\n",
    "        Constructor for neural net\n",
    "        nodes - list detailing number of nodes in each layer\n",
    "        lamb - regularization\n",
    "        alpha - learning rate\n",
    "        eps - cost function stopping condition\n",
    "        '''\n",
    "        self.nodes = nodes\n",
    "        self.lamb = lamb\n",
    "        self.alpha = alpha\n",
    "        self.weights = []\n",
    "        self.eps = eps\n",
    "        #initialize weights for each layer, include bias\n",
    "        for i in range(len(nodes)-1):\n",
    "            self.weights.append(np.random.normal(0,1,(nodes[i]+1,nodes[i+1])))\n",
    "    \n",
    "    def get_sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def deriv_sigmoid(self, x):\n",
    "        return x * (1-x)\n",
    "\n",
    "    def train(self, features, targs):\n",
    "        prev_cost = -math.inf\n",
    "        total_delta = [0]*len(self.weights)\n",
    "        num_inst = len(targs)\n",
    "        keep_learn = True\n",
    "\n",
    "        while(keep_learn):\n",
    "            J = 0\n",
    "            for instance,target in zip(features,targs):\n",
    "                #iterate through layers, vectorize forward pass\n",
    "                activations = [np.atleast_2d(instance)]\n",
    "                for i in range(len(self.weights)-1):\n",
    "                    this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "                    activations.append(np.insert(this_a,0,1))\n",
    "                activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "                guess = activations[-1]\n",
    "\n",
    "                #accumulate sum loss\n",
    "                J += np.sum((-target).dot(math.log(guess)) - (1-target).dot(math.log(1-guess)))\n",
    "\n",
    "                #begin backwards propogation\n",
    "                error = guess - target\n",
    "                delta_inst = [error]\n",
    "\n",
    "                #get delta values for all weights on current instance\n",
    "                for i in range(len(self.weights)-1, 0, -1):\n",
    "                    #this_del = (self.weights[i].T.dot(delta_inst[-1])) * self.deriv_sigmoid(activations[i])\n",
    "                    this_del = (self.weights[i].T*(delta_inst[-1])) * self.deriv_sigmoid(activations[i])\n",
    "                    delta_inst.append(this_del[0][1:])\n",
    "\n",
    "                #reverse delta values\n",
    "                delta_inst = delta_inst[::-1]\n",
    "\n",
    "                #accumulate gradients\n",
    "                for i in range(len(self.weights)-1,-1,-1):\n",
    "                    total_delta[i] += (delta_inst[i]*(activations[i].T)).T\n",
    "            \n",
    "            #regularize weights and update\n",
    "            for i in range(len(self.weights)-1,-1,-1):\n",
    "                P = self.lamb * (self.weights[i]).T\n",
    "                total_delta[i] = total_delta[i] + P\n",
    "                total_delta[i] = total_delta[i] / num_inst\n",
    "                learn_diff = self.alpha * total_delta[i]\n",
    "                self.weights[i] = self.weights[i] - learn_diff.T\n",
    "\n",
    "            J /= num_inst\n",
    "            curr_s = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                curr_s += np.sum(self.weights[i][1:]**2)\n",
    "\n",
    "            #curr_s = np.sum(self.weights[1:]**2)\n",
    "            curr_s *= (self.lamb/(2*num_inst))\n",
    "            new_cost = J + curr_s\n",
    "\n",
    "            #if improvement in cost is less than epsilon, stop\n",
    "            if new_cost - prev_cost < self.eps:\n",
    "                keep_learn = False\n",
    "\n",
    "            prev_cost = new_cost\n",
    "\n",
    "    #def calculate_loss(self,data,targets):\n",
    "        #predictions = aaaa\n",
    "\n",
    "    def predict(self,instance):\n",
    "        pred = [np.ones(len(instance)),instance]\n",
    "\n",
    "        for i in len(self.weights):\n",
    "            pred = self.sigmoid(np.dot(pred,self.weights[i]))\n",
    "        \n",
    "        return pred\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "    - Implement calculate_loss\n",
    "    - Debug using sample data\n",
    "    - Swag, nae nae, and finesse\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training set\n",
    "\tTraining instance 1\n",
    "\t\tx: [0.13000]\n",
    "\t\ty: [0.90000]\n",
    "\tTraining instance 2\n",
    "\t\tx: [0.42000]\n",
    "\t\ty: [0.23000]\n",
    "        Initial Theta1 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
    "\t0.40000  0.10000  \n",
    "\t0.30000  0.20000  \n",
    "\n",
    "Initial Theta2 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
    "\t0.70000  0.50000  0.60000  \n",
    "'''\n",
    "np.array([[0.40000,0.10000 ],[0.30000,0.20000 ]])\n",
    "np.array([[0.7],[0.5],[0.6]])\n",
    "train_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "X = np.array([[1,0.13000],[1,0.42000]])\n",
    "Y = np.array([[0.90000],[0.23000]])\n",
    "train_df = pd.DataFrame(data=train_set_1)\n",
    "train_df.insert(0,'bias',np.ones)\n",
    "train_nn = NeuralNet([1,2,1],eps=0.001)\n",
    "train_nn.train(X,Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "#implement backprogagation algorithm in a way that allows you to specify how many layers and neurons you would like your neural network to have\n",
    "#https://pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/\n",
    "test = [1,2,3,4,5]\n",
    "out = []\n",
    "for i in range(len(test),0,-1):\n",
    "    out.append(i)\n",
    "#print(':3')\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
