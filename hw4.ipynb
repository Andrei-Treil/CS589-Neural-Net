{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO: \\n    - Implement calculate_loss\\n    - Debug using sample data\\n    - Swag, nae nae, and finesse\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,nodes,lamb=0.0,alpha=0.1,eps=0.0):\n",
    "        '''\n",
    "        Constructor for neural net\n",
    "        nodes - list detailing number of nodes in each layer\n",
    "        lamb - regularization\n",
    "        alpha - learning rate\n",
    "        eps - cost function stopping condition\n",
    "        '''\n",
    "        self.nodes = nodes\n",
    "        self.lamb = lamb\n",
    "        self.alpha = alpha\n",
    "        self.weights = []\n",
    "        self.eps = eps\n",
    "        #initialize weights for each layer, include bias\n",
    "        for i in range(len(nodes)-1):\n",
    "            self.weights.append(np.random.normal(0,1,(nodes[i]+1,nodes[i+1])))\n",
    "    \n",
    "    def get_sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def deriv_sigmoid(self, x):\n",
    "        return x * (1-x)\n",
    "\n",
    "    def train(self, features, targs, for_exam=False):\n",
    "        prev_cost = -math.inf\n",
    "        gradients = [0]*len(self.weights)\n",
    "        num_inst = len(targs)\n",
    "        keep_learn = True\n",
    "        count = 1\n",
    "\n",
    "        while(keep_learn):\n",
    "            J = 0\n",
    "            for instance,target in zip(features,targs):\n",
    "                #iterate through layers, vectorize forward pass\n",
    "                activations = [np.atleast_2d(instance)]\n",
    "                for i in range(len(self.weights)-1):\n",
    "                    this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "                    activations.append(np.insert(this_a,0,1))\n",
    "                activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "                guess = activations[-1]\n",
    "\n",
    "                #accumulate sum loss\n",
    "                cost = np.sum((-target).dot(math.log(guess)) - (1-target).dot(math.log(1-guess)))\n",
    "                J += cost\n",
    "\n",
    "                #begin backwards propogation\n",
    "                error = guess - target\n",
    "                delta_inst = [error]\n",
    "\n",
    "                #get delta values for all weights on current instance\n",
    "                for i in range(len(self.weights)-1, 0, -1):\n",
    "                    this_del = (self.weights[i].T*(delta_inst[-1])) * self.deriv_sigmoid(activations[i])\n",
    "                    delta_inst.append(this_del[0][1:])\n",
    "\n",
    "                #reverse delta values\n",
    "                delta_inst = delta_inst[::-1]\n",
    "\n",
    "                #accumulate gradients\n",
    "                for i in range(len(self.weights)-1,-1,-1):\n",
    "                    gradients[i] += (delta_inst[i]*(activations[i].T)).T\n",
    "\n",
    "                #print for examples\n",
    "                if for_exam:\n",
    "                    print(f'OUTPUTS FOR INSTANCE {count}')\n",
    "                    print(f'activations: ')\n",
    "                    for i in range(len(activations)):\n",
    "                        print(f'a{i+1}: {activations[i]}')\n",
    "                    print()\n",
    "                    print(f'prediction: {guess}')\n",
    "                    print(f'expected: {target}')\n",
    "                    print(f'cost J: {cost}')\n",
    "                    print()\n",
    "                    print('delta for this instance: ')\n",
    "                    for i in range(len(delta_inst)):\n",
    "                        print(f'delta {i+2}: {delta_inst[i]}')\n",
    "                    print()\n",
    "                    print('gradients for this instance: ')\n",
    "                    for i in range(len(self.weights)):\n",
    "                        print_del = (delta_inst[i]*(activations[i].T)).T\n",
    "                        print(f'theta {i+1}: {print_del}')\n",
    "                    #print(f'gradients for this instance: {gradients}')\n",
    "                    print()\n",
    "                    count += 1\n",
    "            \n",
    "            #regularize weights and update\n",
    "            for i in range(len(self.weights)-1,-1,-1):\n",
    "                P = self.lamb * (self.weights[i]).T\n",
    "                gradients[i] = gradients[i] + P\n",
    "                gradients[i] = gradients[i] / num_inst\n",
    "                learn_diff = self.alpha * gradients[i]\n",
    "                self.weights[i] = self.weights[i] - learn_diff.T\n",
    "\n",
    "            J /= num_inst\n",
    "            curr_s = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                curr_s += np.sum(self.weights[i][1:]**2)\n",
    "\n",
    "            #curr_s = np.sum(self.weights[1:]**2)\n",
    "            curr_s *= (self.lamb/(2*num_inst))\n",
    "            new_cost = J + curr_s\n",
    "\n",
    "            #if improvement in cost is less than epsilon, stop\n",
    "            if new_cost - prev_cost < self.eps:\n",
    "                keep_learn = False\n",
    "\n",
    "            prev_cost = new_cost\n",
    "\n",
    "            if for_exam:\n",
    "                print('regularized gradients: ')\n",
    "                for i in range(len(gradients)):\n",
    "                    print(f'theta {i+1}: {gradients[i]}')\n",
    "                keep_learn = False\n",
    "\n",
    "    #def calculate_loss(self,data,targets):\n",
    "        #predictions = aaaa\n",
    "\n",
    "    def predict(self,instance):\n",
    "        pred = [np.ones(len(instance)),instance]\n",
    "\n",
    "        for i in len(self.weights):\n",
    "            pred = self.sigmoid(np.dot(pred,self.weights[i]))\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    #def calc_cost(self,guess,target,)\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "    - Implement calculate_loss\n",
    "    - Debug using sample data\n",
    "    - Swag, nae nae, and finesse\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS FOR INSTANCE 1\n",
      "activations: \n",
      "a1: [[1.   0.13]]\n",
      "a2: [1.        0.601807  0.5807858]\n",
      "a3: [0.79402743]\n",
      "\n",
      "prediction: [0.79402743]\n",
      "expected: [0.9]\n",
      "cost J: 0.36557477431084995\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [-0.01269739 -0.01548092]\n",
      "delta 3: [-0.10597257]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[-0.01269739 -0.00165066]\n",
      " [-0.01548092 -0.00201252]]\n",
      "theta 2: [-0.10597257 -0.06377504 -0.06154737]\n",
      "\n",
      "OUTPUTS FOR INSTANCE 2\n",
      "activations: \n",
      "a1: [[1.   0.42]]\n",
      "a2: [1.         0.60873549 0.59483749]\n",
      "a3: [0.79596607]\n",
      "\n",
      "prediction: [0.79596607]\n",
      "expected: [0.23]\n",
      "cost J: 1.2763768066887786\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [0.06739994 0.08184068]\n",
      "delta 3: [0.56596607]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[0.06739994 0.02830797]\n",
      " [0.08184068 0.03437309]]\n",
      "theta 2: [0.56596607 0.34452363 0.33665784]\n",
      "\n",
      "regularized gradients: \n",
      "theta 1: [[0.02735127 0.01332866]\n",
      " [0.03317988 0.01618028]]\n",
      "theta 2: [[0.22999675 0.1403743  0.13755523]]\n"
     ]
    }
   ],
   "source": [
    "#function to train on backprop_example1.txt\n",
    "#outputs to stdout, if full output cannot be viewed when calling this function, increase number of lines shown in output to 100\n",
    "def train_on_first():\n",
    "\ttrain_nn = NeuralNet([1,2,1],eps=0.001)\n",
    "\ttrain_nn.weights[0] = np.array([[0.40000,0.10000 ],[0.30000,0.20000 ]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.7],[0.5],[0.6]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\t#NOTE: X values are preprocessed to include bias term (1) as first element\n",
    "\tX = np.array([[1,0.13000],[1,0.42000]])\n",
    "\tY = np.array([[0.90000],[0.23000]])\n",
    "\ttrain_df = pd.DataFrame(data=train_set_1)\n",
    "\ttrain_df.insert(0,'bias',np.ones)\n",
    "\ttrain_nn.train(X,Y,True)\n",
    "\n",
    "train_on_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,) and (2,4) not aligned: 4 (dim 0) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\OneDrive\\Desktop\\CS\\589\\CS589-HW4\\hw4.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \ttrain_df\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m,np\u001b[39m.\u001b[39mones)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \ttrain_nn\u001b[39m.\u001b[39mtrain(X,Y,\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m train_on_sec()\n",
      "\u001b[1;32mc:\\Users\\andre\\OneDrive\\Desktop\\CS\\589\\CS589-HW4\\hw4.ipynb Cell 4\u001b[0m in \u001b[0;36mtrain_on_sec\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39mtrain_set_1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_df\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m,np\u001b[39m.\u001b[39mones)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_nn\u001b[39m.\u001b[39;49mtrain(X,Y,\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\andre\\OneDrive\\Desktop\\CS\\589\\CS589-HW4\\hw4.ipynb Cell 4\u001b[0m in \u001b[0;36mNeuralNet.train\u001b[1;34m(self, features, targs, for_exam)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     this_a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_sigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[i]\u001b[39m.\u001b[39mdot(activations[i]\u001b[39m.\u001b[39mT))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     activations\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39minsert(this_a,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m activations\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_sigmoid(activations[\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights)\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights[\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights)\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m guess \u001b[39m=\u001b[39m activations[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/OneDrive/Desktop/CS/589/CS589-HW4/hw4.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#accumulate sum loss\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,) and (2,4) not aligned: 4 (dim 0) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "#function to train on backprop_example2.txt\n",
    "#outputs to stdout, if full output cannot be viewed when calling this function, increase number of lines shown in output to 100\n",
    "def train_on_sec():\n",
    "\ttrain_nn = NeuralNet([2,4,3,2],eps=0.001)\n",
    "\ttrain_nn.weights[0] = np.array([[0.42000,0.15000,0.40000],[0.72000,0.10000,0.54000],[0.01000,0.19000,0.42000],[0.30000,0.35000,0.68000]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.21000,0.67000,0.14000,0.96000,0.87000],[0.87000,0.42000,0.20000,0.32000,0.89000],[0.03000,0.56000,0.80000,0.69000,0.09000]])\n",
    "\ttrain_nn.weights[2] = np.array([[0.04000,0.87000,0.42000,0.53000],[0.17000,0.10000,0.95000,0.69000 ]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\t#NOTE: X values are preprocessed to include bias term (1) as first element\n",
    "\tX = np.array([[1,0.32000,0.68000],[1,0.83000,0.02000]])\n",
    "\tY = np.array([[0.75000,0.98000],[0.75000,0.28000]])\n",
    "\ttrain_df = pd.DataFrame(data=train_set_1)\n",
    "\ttrain_df.insert(0,'bias',np.ones)\n",
    "\ttrain_nn.train(X,Y,True)\n",
    "\n",
    "train_on_sec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
