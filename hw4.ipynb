{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,nodes,lamb=0.0,alpha=0.1,eps=0.0):\n",
    "        '''\n",
    "        Constructor for neural net\n",
    "        nodes - list detailing number of nodes in each layer\n",
    "        lamb - regularization\n",
    "        alpha - learning rate\n",
    "        eps - cost function stopping condition\n",
    "        '''\n",
    "        self.nodes = nodes\n",
    "        self.lamb = lamb\n",
    "        self.alpha = alpha\n",
    "        self.weights = []\n",
    "        self.eps = eps\n",
    "        #initialize weights for each layer, include bias\n",
    "        for i in range(len(nodes)-1):\n",
    "            self.weights.append(np.random.normal(0,1,(nodes[i]+1,nodes[i+1])).T)\n",
    "    \n",
    "    def get_sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def deriv_sigmoid(self, x):\n",
    "        return x * (1-x)\n",
    "\n",
    "    def train(self, features, targs, batch_size, test_feat=None, test_targs=None, for_exam=False, get_costs=False):\n",
    "        '''\n",
    "        features - training data features\n",
    "        targs - training data targets\n",
    "        batch size - # of instances for mini batch\n",
    "        test_feat - test data features\n",
    "        test_targs - test data targets\n",
    "        for_exam - flag to print for back_prop examples\n",
    "        get_costs - flag to get J values for varying number of samples\n",
    "        '''\n",
    "        prev_cost = math.inf\n",
    "        gradients = [0]*len(self.weights)\n",
    "        num_inst = len(targs)\n",
    "        keep_learn = True\n",
    "        count = 1\n",
    "        curr_batch = 1\n",
    "        cost_j = []\n",
    "        cost_j_count = []\n",
    "\n",
    "        while(keep_learn):\n",
    "            J = 0\n",
    "            for instance,target in zip(features,targs):\n",
    "                #iterate through layers, vectorize forward pass\n",
    "                activations = [np.atleast_2d(instance)]\n",
    "                for i in range(len(self.weights)-1):\n",
    "                    try:\n",
    "                        this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "                    except:\n",
    "                        this_a = self.get_sigmoid(self.weights[i].T.dot(activations[i].T))\n",
    "                    activations.append(np.insert(this_a,0,1))\n",
    "                try:\n",
    "                    activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "                except:\n",
    "                    activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1].T)))\n",
    "                guess = activations[-1]\n",
    "\n",
    "                #accumulate sum loss\n",
    "                target = np.array(target)\n",
    "                cost = np.sum((np.array(-target)).dot(np.log(guess)) - (np.array(1-target)).dot(np.log(1-guess)))\n",
    "                J += cost\n",
    "\n",
    "                #begin backwards propogation\n",
    "                error = guess - target\n",
    "                delta_inst = [error]\n",
    "\n",
    "                #get delta values for all weights on current instance\n",
    "                for i in range(len(self.weights)-1, 0, -1):\n",
    "                    try:\n",
    "                        this_del = (self.weights[i].T.dot(delta_inst[-1])) * self.deriv_sigmoid(activations[i].T)\n",
    "                    except:\n",
    "                        this_del = (self.weights[i].dot(delta_inst[-1])) * self.deriv_sigmoid(activations[i].T)\n",
    "                    delta_inst.append(this_del[1:])\n",
    "\n",
    "                #reverse delta values\n",
    "                delta_inst = delta_inst[::-1]\n",
    "\n",
    "                #accumulate gradients\n",
    "                for i in range(len(self.weights)-1,-1,-1):\n",
    "                    try:\n",
    "                        gradients[i] += (delta_inst[i]*(activations[i].T)).T\n",
    "                    except:\n",
    "                        gradients[i] += (np.atleast_2d(delta_inst[i]).T*np.atleast_2d(activations[i].T))\n",
    "\n",
    "                #print for examples\n",
    "                if for_exam:\n",
    "                    print(f'OUTPUTS FOR INSTANCE {count}')\n",
    "                    print(f'activations: ')\n",
    "                    for i in range(len(activations)):\n",
    "                        print(f'a{i+1}: {activations[i]}')\n",
    "                    print()\n",
    "                    print(f'prediction: {guess}')\n",
    "                    print(f'expected: {target}')\n",
    "                    print(f'cost J: {cost}')\n",
    "                    print()\n",
    "                    print('delta for this instance: ')\n",
    "                    for i in range(len(delta_inst)):\n",
    "                        print(f'delta {i+2}: {delta_inst[i]}')\n",
    "                    print()\n",
    "                    print('gradients for this instance: ')\n",
    "                    for i in range(len(self.weights)):\n",
    "                        try:\n",
    "                            print_del = (delta_inst[i]*(activations[i].T)).T\n",
    "                        except:\n",
    "                            print_del = (np.atleast_2d(delta_inst[i]).T*np.atleast_2d(activations[i].T)).T\n",
    "                        print(f'theta {i+1}: {print_del}')\n",
    "                    print()\n",
    "\n",
    "                if curr_batch == batch_size or count == num_inst:\n",
    "                    #regularize weights and update\n",
    "                    for i in range(len(self.weights)-1,-1,-1):\n",
    "                        P = self.lamb * (self.weights[i])\n",
    "                        #set first column to all 0\n",
    "                        P[:,0] = 0\n",
    "                        try:\n",
    "                            gradients[i] = gradients[i] + P.T\n",
    "                        except:\n",
    "                            gradients[i] = gradients[i] + P\n",
    "                        gradients[i] = gradients[i] / num_inst\n",
    "                        learn_diff = self.alpha * (gradients[i])\n",
    "                        try:\n",
    "                            self.weights[i] = self.weights[i] - learn_diff\n",
    "                        except:\n",
    "                            self.weights[i] = self.weights[i] - learn_diff.T\n",
    "                    curr_batch = 0\n",
    "\n",
    "                    if get_costs:\n",
    "                        cost_j.append(self.cost_on_set(test_feat,test_targs))\n",
    "                        cost_j_count.append(count)\n",
    "\n",
    "                curr_batch += 1\n",
    "                count += 1\n",
    "\n",
    "            J /= num_inst\n",
    "            curr_s = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                curr_s += np.sum(self.weights[i][1:]**2)\n",
    "\n",
    "            #curr_s = np.sum(self.weights[1:]**2)\n",
    "            curr_s *= (self.lamb/(2*num_inst))\n",
    "            new_cost = J + curr_s\n",
    "\n",
    "            #if improvement in cost is less than epsilon, stop\n",
    "            if prev_cost - new_cost < self.eps:\n",
    "                keep_learn = False\n",
    "\n",
    "            prev_cost = new_cost\n",
    "\n",
    "            if for_exam:\n",
    "                print('regularized gradients: ')\n",
    "                for i in range(len(gradients)):\n",
    "                    print(f'theta {i+1}: {gradients[i]}')\n",
    "                keep_learn = False\n",
    "            if get_costs:\n",
    "                return cost_j,cost_j_count\n",
    "\n",
    "    #forward pass on one instance, returns an array where index of max val is the NN's guess and 0 for all else\n",
    "    #for example a guess for class 0 of 3 possible classes 0,1,2 would be [1,0,0]\n",
    "    #raw - True if wanting the raw outputs, false if wanting fixed as discussed above\n",
    "    def predict(self,instance,raw=True):\n",
    "        activations = [np.atleast_2d(instance)]\n",
    "        for i in range(len(self.weights)-1):\n",
    "            try:\n",
    "                this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "            except:\n",
    "                this_a = self.get_sigmoid(self.weights[i].T.dot(activations[i].T))\n",
    "            activations.append(np.insert(this_a,0,1))\n",
    "        try:\n",
    "            activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "        except:\n",
    "            activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1].T)))\n",
    "        guess = activations[-1]\n",
    "        pred = [0]*len(guess)\n",
    "        pred[np.argmax(guess)] = 1\n",
    "        \n",
    "        return guess if raw else pred\n",
    "    \n",
    "    def cost_on_set(self,instances,targets):\n",
    "        J = 0\n",
    "        for instance,target in zip(instances,targets):\n",
    "            guess = self.predict(instance)\n",
    "            target = np.array(target)\n",
    "            cost = np.sum((np.array(-target)).dot(np.log(guess)) - (np.array(1-target)).dot(np.log(1-guess)))\n",
    "            J += cost\n",
    "        J /= len(instances)\n",
    "        curr_s = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            curr_s += np.sum(self.weights[i][1:]**2)\n",
    "\n",
    "        curr_s *= (self.lamb/(2*len(instances)))\n",
    "        return J + curr_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decision(nn,test_set,vals):\n",
    "    test_copy = pd.DataFrame(test_set,copy=True)\n",
    "    #test_copy.insert(0,'bias',1)\n",
    "    to_guess = test_copy.drop('class',axis=1)\n",
    "    predictions = pd.DataFrame(to_guess.apply(lambda row: nn.predict(row.to_numpy(),raw=False), axis=1),columns=['predicted'])\n",
    "    predictions['actual'] = test_set.loc[predictions.index,'class']\n",
    "    prec,rec,f1 = [0,0,0]\n",
    "\n",
    "    for val in vals:\n",
    "        is_targ = predictions[predictions.predicted.apply(lambda x: x == val)]\n",
    "        not_targ = predictions[predictions.predicted.apply(lambda x: x != val)]\n",
    "        tp = len(is_targ[is_targ['predicted'] == is_targ['actual']])\n",
    "        fp = len(is_targ[is_targ['predicted'] != is_targ['actual']])\n",
    "        fn = len(not_targ[not_targ.actual.apply(lambda x: x == val)])\n",
    "        tn = len(not_targ[not_targ.actual.apply(lambda x: x != val)])\n",
    "        this_prec = (tp/(tp+fp)) if (tp+fp) > 0 else 0\n",
    "        this_rec = (tp/(tp+fn)) if (tp+fn) > 0 else 0\n",
    "        f1 += (this_prec*this_rec*2)/(this_rec+this_prec) if (this_rec+this_prec) > 0 else 0\n",
    "        prec += this_prec\n",
    "        rec += this_rec\n",
    "\n",
    "    avg_f1 = f1/len(vals)\n",
    "    accuracy = len(predictions[predictions['predicted'] == predictions['actual']])/len(test_set)\n",
    "    return accuracy,avg_f1\n",
    "\n",
    "np.random.seed(1)\n",
    "k = 10\n",
    "#function to do cross fold validation\n",
    "def k_fold(fold,vals,nn_arc,lamb,eps,alpha,batch_size,get_j=False):\n",
    "    fold_metrics = defaultdict(list)\n",
    "    #iterate through folds, taking turns being test fold\n",
    "    for i in range(k):\n",
    "        test_fold = fold[i]\n",
    "        #test_fold.insert(0,'bias',1)\n",
    "        test_targs = test_fold['class']\n",
    "        test_feat = test_fold.drop('class',axis=1)\n",
    "        train_fold = fold[0:i]\n",
    "        train_fold.extend(fold[i+1:len(fold)])\n",
    "        train_data = pd.concat(train_fold)\n",
    "       \n",
    "        #train_data.insert(0,'bias',1)\n",
    "        #iterate through architectures\n",
    "        for arc in nn_arc:\n",
    "            np_targs = train_data['class'].to_numpy()\n",
    "            np_inst = train_data.drop('class',axis=1).to_numpy()\n",
    "            this_nn = NeuralNet(arc,lamb,alpha,eps)\n",
    "            if get_j:\n",
    "                return this_nn.train(np_inst,np_targs,batch_size,test_feat.to_numpy(),test_targs.to_numpy(),get_costs=True)\n",
    "            this_nn.train(np_inst,np_targs,batch_size)\n",
    "            fold_metrics[str(arc)].append(test_decision(this_nn,test_fold,vals))\n",
    "            \n",
    "    return fold_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of neural net architectures\n",
    "wine_nn_arc = [[13,4,3],[13,8,3],[13,8,8,3],[13,8,16,3],[13,8,16,8,3],[13,4,8,8,4,3]]\n",
    "wine_df = pd.read_csv('datasets/hw3_wine.csv',delimiter='\\t')\n",
    "norm_wine_df = ((wine_df-wine_df.min())/(wine_df.max()-wine_df.min()))\n",
    "#insert column of ones to act as bias\n",
    "norm_wine_df.insert(0,'bias',1)\n",
    "\n",
    "#split data by class into k groups then combine into folds\n",
    "wine_class_1 = norm_wine_df.loc[norm_wine_df['class'] == 0].sample(frac=1)\n",
    "wine_class_1['class'] = [[1,0,0]]*len(wine_class_1)\n",
    "wc1_split = np.array_split(wine_class_1,k)\n",
    "wine_class_2 = norm_wine_df.loc[norm_wine_df['class'] == 0.5].sample(frac=1)\n",
    "wine_class_2['class'] = [[0,1,0]]*len(wine_class_2)\n",
    "wc2_split = np.array_split(wine_class_2,k)\n",
    "wine_class_3 = norm_wine_df.loc[norm_wine_df['class'] == 1].sample(frac=1)\n",
    "wine_class_3['class'] = [[0,0,1]]*len(wine_class_3)\n",
    "wc3_split = np.array_split(wine_class_3,k)\n",
    "wine_vals = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "\n",
    "#list to hold folds\n",
    "wine_fold = []\n",
    "for i in range(k):\n",
    "    this_fold = [wc1_split[i],wc2_split[i],wc3_split[i]]\n",
    "    wine_fold.append(pd.concat(this_fold))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3**  create, for each dataset and for each of the metrics\n",
    "described above, a table summarizing the corresponding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.1 eps = 0.001 alpha = 3 batch_size = 5\n",
      "          Architecture  Accuracy        F1\n",
      "0           [13, 4, 3]  0.960382  0.961588\n",
      "1           [13, 8, 3]  0.972515  0.973781\n",
      "2        [13, 8, 8, 3]  0.678993  0.608028\n",
      "3       [13, 8, 16, 3]  0.746616  0.699122\n",
      "4    [13, 8, 16, 8, 3]  0.399254  0.190154\n",
      "5  [13, 4, 8, 8, 4, 3]  0.399254  0.190154\n",
      "lamb = 0.1 eps = 0.001 alpha = 5 batch_size = 5\n",
      "          Architecture  Accuracy        F1\n",
      "0           [13, 4, 3]  0.966265  0.967731\n",
      "1           [13, 8, 3]  0.966265  0.967731\n",
      "2        [13, 8, 8, 3]  0.622457  0.512777\n",
      "3       [13, 8, 16, 3]  0.703049  0.644960\n",
      "4    [13, 8, 16, 8, 3]  0.415921  0.217700\n",
      "5  [13, 4, 8, 8, 4, 3]  0.399254  0.190154\n",
      "lamb = 0.05 eps = 0.0001 alpha = 3 batch_size = 5\n",
      "          Architecture  Accuracy        F1\n",
      "0           [13, 4, 3]  0.983333  0.984025\n",
      "1           [13, 8, 3]  0.983333  0.984025\n",
      "2        [13, 8, 8, 3]  0.925439  0.903778\n",
      "3       [13, 8, 16, 3]  0.977778  0.978430\n",
      "4    [13, 8, 16, 8, 3]  0.753158  0.685844\n",
      "5  [13, 4, 8, 8, 4, 3]  0.399254  0.190154\n",
      "lamb = 0.05 eps = 0.0001 alpha = 5 batch_size = 5\n",
      "          Architecture  Accuracy        F1\n",
      "0           [13, 4, 3]  0.983333  0.984025\n",
      "1           [13, 8, 3]  0.978070  0.979034\n",
      "2        [13, 8, 8, 3]  0.961404  0.961823\n",
      "3       [13, 8, 16, 3]  0.966959  0.967869\n",
      "4    [13, 8, 16, 8, 3]  0.712448  0.638396\n",
      "5  [13, 4, 8, 8, 4, 3]  0.399254  0.190154\n"
     ]
    }
   ],
   "source": [
    "def wine_test(lamb,eps,alpha,batch_size):\n",
    "    wine_res = k_fold(wine_fold,wine_vals,wine_nn_arc,lamb,eps,alpha,batch_size)\n",
    "    arc_dict = defaultdict(list)\n",
    "    print(f'lamb = {lamb} eps = {eps} alpha = {alpha} batch_size = {batch_size}')\n",
    "\n",
    "    for arc,perf in wine_res.items():\n",
    "        avg_acc,avg_f1 = [0,0]\n",
    "        for res in perf:\n",
    "            avg_acc += res[0]\n",
    "            avg_f1 += res[1]\n",
    "        arc_dict['Architecture'].append(arc)\n",
    "        arc_dict['Accuracy'].append(avg_acc/10)\n",
    "        arc_dict['F1'].append(avg_f1/10)\n",
    "\n",
    "    arc_table = pd.DataFrame(arc_dict)\n",
    "    print(arc_table)\n",
    "\n",
    "hyper_params = [[0.1,0.001,3,5],[0.1,0.001,5,5],[0.05,0.0001,3,5],[0.05,0.0001,5,5]]\n",
    "for params in hyper_params:\n",
    "    wine_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4** Discuss (on a high level) what contributed the most to improving performance: changing the\n",
    "regularization parameter; adding more layers; having deeper networks with many layers but few\n",
    "neurons per layer? designing networks with few layers but many neurons per layer? Discuss\n",
    "any patterns that you may have encountered. Also, discuss whether there is a point where\n",
    "constructing and training more “sophisticated”/complex networks—i.e., larger networks—no\n",
    "longer improves performance (or worsens performance)\n",
    "\n",
    ">From running 4 different combinations of hyper parameters, I found that setting lambda = 0.05 yielded the best results. Additionally, having > 1 hidden layer in the network showed a significant decrease in performance for this data set. Of the architectures tested with 1 hidden layer, using 8 nodes yielded the highest results, however using 4 nodes would yield higher accuracy in certain conditions. Alpha = 3  and eps = 0.0001 seemed to yield the best results. In general, using an architecture with > 1 layer had the highest impact on performance out of all adjustable parameters that I tested.\n",
    "\n",
    "**1.5** Based on the analyses above, discuss which neural network architecture you would select if you\n",
    "had to deploy such a classifier in real life. Explain your reasoning\n",
    "\n",
    ">If I had to deploy this classifier in real life, I would choose an architecture with 1 hidden layer containing 8 nodes as this architecture performed best on average accross various settings of hyper parameters. Additionally, I would use a lambda value of 0.05 as this also improved the accuracy of architectures with > 1 hidden layer.\n",
    "\n",
    "**1.6** After identifying the best neural network architecture for each one of the datasets, train it once\n",
    "again on the corresponding dataset and create a learning curve where the y axis shows the\n",
    "network’s performance (J) on a test set, and where the x axis indicates the number of training\n",
    "samples given to the network.\n",
    "\n",
    "> Using architecture [13,8,3] with alpha = 3, testing cost J every 5 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wine_j,wine_count = k_fold(wine_fold,wine_vals,[[13,8,3]],0.05,0.0001,3,5,get_j=True)\n",
    "plt.plot(wine_count,wine_j)\n",
    "plt.xlabel('Number of Training Instances')\n",
    "plt.ylabel('Performance (J) on Test Set')\n",
    "plt.title('Wine Data Performance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. House Votes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of neural net architectures\n",
    "house_nn_arc = [[16,8,2],[16,16,2],[16,8,16,2],[16,4,8,2],[16,10,8,6,2],[16,8,16,32,8,2]]\n",
    "house_df = pd.read_csv('datasets/hw3_house_votes_84.csv',delimiter=',')\n",
    "norm_house_df = ((house_df-house_df.min())/(house_df.max()-house_df.min()))\n",
    "#insert column of ones to act as bias\n",
    "norm_house_df.insert(0,'bias',1)\n",
    "\n",
    "#split data by class into k groups then combine into folds\n",
    "house_class_1 = norm_house_df.loc[norm_house_df['class'] == 0].sample(frac=1)\n",
    "house_class_1['class'] = [[1,0]]*len(house_class_1)\n",
    "hc1_split = np.array_split(house_class_1,k)\n",
    "house_class_2 = norm_house_df.loc[norm_house_df['class'] == 1].sample(frac=1)\n",
    "house_class_2['class'] = [[0,1]]*len(house_class_2)\n",
    "hc2_split = np.array_split(house_class_2,k)\n",
    "house_vals = [[1,0],[0,1]]\n",
    "\n",
    "#list to hold folds\n",
    "house_fold = []\n",
    "for i in range(k):\n",
    "    this_fold = [hc1_split[i],hc2_split[i]]\n",
    "    house_fold.append(pd.concat(this_fold))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.1 eps = 0.001 alpha = 3 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.925871  0.922119\n",
      "1            [16, 16, 2]  0.921434  0.918382\n",
      "2         [16, 8, 16, 2]  0.923545  0.919980\n",
      "3        [16, 16, 32, 2]  0.919161  0.916066\n",
      "4       [16, 4, 8, 8, 2]  0.808084  0.713565\n",
      "5  [16, 8, 16, 32, 8, 2]  0.825818  0.753695\n",
      "lamb = 0.1 eps = 0.001 alpha = 5 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.921325  0.918117\n",
      "1            [16, 16, 2]  0.930633  0.927698\n",
      "2         [16, 8, 16, 2]  0.923598  0.921091\n",
      "3        [16, 16, 32, 2]  0.916727  0.913530\n",
      "4       [16, 4, 8, 8, 2]  0.779729  0.661743\n",
      "5  [16, 8, 16, 32, 8, 2]  0.791093  0.696824\n",
      "lamb = 0.05 eps = 0.001 alpha = 3 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.937667  0.935208\n",
      "1            [16, 16, 2]  0.935015  0.932224\n",
      "2         [16, 8, 16, 2]  0.930577  0.927469\n",
      "3        [16, 16, 32, 2]  0.916727  0.913824\n",
      "4       [16, 4, 8, 8, 2]  0.718366  0.554352\n",
      "5  [16, 8, 16, 32, 8, 2]  0.803647  0.708512\n",
      "lamb = 0.05 eps = 0.001 alpha = 5 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.930472  0.927885\n",
      "1            [16, 16, 2]  0.930633  0.927898\n",
      "2         [16, 8, 16, 2]  0.930633  0.927937\n",
      "3        [16, 16, 32, 2]  0.921273  0.917900\n",
      "4       [16, 4, 8, 8, 2]  0.775871  0.656473\n",
      "5  [16, 8, 16, 32, 8, 2]  0.778091  0.659640\n",
      "lamb = 0.1 eps = 0.0001 alpha = 3 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.930633  0.927553\n",
      "1            [16, 16, 2]  0.921436  0.917639\n",
      "2         [16, 8, 16, 2]  0.923815  0.920240\n",
      "3        [16, 16, 32, 2]  0.935339  0.932054\n",
      "4       [16, 4, 8, 8, 2]  0.741780  0.600204\n",
      "5  [16, 8, 16, 32, 8, 2]  0.771273  0.652852\n",
      "lamb = 0.05 eps = 0.0001 alpha = 3 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.932905  0.929891\n",
      "1            [16, 16, 2]  0.925818  0.922159\n",
      "2         [16, 8, 16, 2]  0.928468  0.925488\n",
      "3        [16, 16, 32, 2]  0.932905  0.929831\n",
      "4       [16, 4, 8, 8, 2]  0.786547  0.668521\n",
      "5  [16, 8, 16, 32, 8, 2]  0.837287  0.765016\n"
     ]
    }
   ],
   "source": [
    "def house_test(lamb,eps,alpha,batch_size):\n",
    "    house_res = k_fold(house_fold,house_vals,house_nn_arc,lamb,eps,alpha,batch_size)\n",
    "    print(f'lamb = {lamb} eps = {eps} alpha = {alpha} batch_size = {batch_size}')\n",
    "    arc_dict_h = defaultdict(list)\n",
    "\n",
    "    for arc,perf in house_res.items():\n",
    "        avg_acc,avg_f1 = [0,0]\n",
    "        for res in perf:\n",
    "            avg_acc += res[0]\n",
    "            avg_f1 += res[1]\n",
    "        arc_dict_h['Architecture'].append(arc)\n",
    "        arc_dict_h['Accuracy'].append(avg_acc/10)\n",
    "        arc_dict_h['F1'].append(avg_f1/10)\n",
    "\n",
    "    arc_table_h = pd.DataFrame(arc_dict_h)\n",
    "    print(arc_table_h)\n",
    "\n",
    "#hyper_params = [[0.05,0.001,3,5],[0.05,0.0001,5,5],[0.05,0.0001,3,5],[0.05,0.0001,5,5]]\n",
    "hyper_params_h = [[0.1,0.001,3,15],[0.1,0.001,5,15],[0.05,0.001,3,15],[0.05,0.001,5,15],[0.1,0.0001,3,15],[0.05,0.0001,3,15]]\n",
    "for params in hyper_params_h:\n",
    "    house_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.1 eps = 0.001 alpha = 3 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.908391  0.883147\n",
      "1            [16, 16, 2]  0.933499  0.931065\n",
      "2         [16, 8, 16, 2]  0.857742  0.807939\n",
      "3        [16, 16, 32, 2]  0.915046  0.912326\n",
      "4      [16, 10, 8, 4, 2]  0.790967  0.695404\n",
      "5  [16, 8, 16, 32, 8, 2]  0.847028  0.797539\n",
      "lamb = 0.1 eps = 0.001 alpha = 5 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.886422  0.859976\n",
      "1            [16, 16, 2]  0.926573  0.923774\n",
      "2         [16, 8, 16, 2]  0.926626  0.924006\n",
      "3        [16, 16, 32, 2]  0.935717  0.933457\n",
      "4      [16, 10, 8, 4, 2]  0.763820  0.645991\n",
      "5  [16, 8, 16, 32, 8, 2]  0.761547  0.643771\n",
      "lamb = 0.05 eps = 0.001 alpha = 3 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.922136  0.919185\n",
      "1            [16, 16, 2]  0.933391  0.930838\n",
      "2         [16, 8, 16, 2]  0.935664  0.933009\n",
      "3        [16, 16, 32, 2]  0.915101  0.912052\n",
      "4      [16, 10, 8, 4, 2]  0.782002  0.664112\n",
      "5  [16, 8, 16, 32, 8, 2]  0.746431  0.627089\n",
      "lamb = 0.05 eps = 0.001 alpha = 5 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.928846  0.925527\n",
      "1            [16, 16, 2]  0.926573  0.923676\n",
      "2         [16, 8, 16, 2]  0.891400  0.875199\n",
      "3        [16, 16, 32, 2]  0.919649  0.916658\n",
      "4      [16, 10, 8, 4, 2]  0.772911  0.653398\n",
      "5  [16, 8, 16, 32, 8, 2]  0.800924  0.704990\n",
      "lamb = 0.1 eps = 0.001 alpha = 3 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.926573  0.923682\n",
      "1            [16, 16, 2]  0.928846  0.926149\n",
      "2         [16, 8, 16, 2]  0.933391  0.930936\n",
      "3        [16, 16, 32, 2]  0.924245  0.921583\n",
      "4      [16, 10, 8, 4, 2]  0.761547  0.643431\n",
      "5  [16, 8, 16, 32, 8, 2]  0.794881  0.700651\n",
      "lamb = 0.05 eps = 0.001 alpha = 5 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.933391  0.930779\n",
      "1            [16, 16, 2]  0.933391  0.930881\n",
      "2         [16, 8, 16, 2]  0.926412  0.923342\n",
      "3        [16, 16, 32, 2]  0.935664  0.933197\n",
      "4      [16, 10, 8, 4, 2]  0.823560  0.751895\n",
      "5  [16, 8, 16, 32, 8, 2]  0.714613  0.549838\n"
     ]
    }
   ],
   "source": [
    "hyper_params_h = [[0.1,0.001,3,15],[0.1,0.001,5,15],[0.05,0.001,3,15],[0.05,0.001,5,15],[0.1,0.001,3,15],[0.05,0.001,5,15]]\n",
    "for params in hyper_params_h:\n",
    "    house_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb = 0.1 eps = 0.01 alpha = 5 batch_size = 1\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.636656  0.427821\n",
      "1            [16, 16, 2]  0.851090  0.822906\n",
      "2         [16, 8, 16, 2]  0.617175  0.423470\n",
      "3          [16, 4, 8, 2]  0.613820  0.380349\n",
      "4      [16, 10, 8, 6, 2]  0.613820  0.380349\n",
      "5  [16, 8, 16, 32, 8, 2]  0.613820  0.380349\n",
      "lamb = 0.1 eps = 0.001 alpha = 5 batch_size = 1\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.718350  0.586573\n",
      "1            [16, 16, 2]  0.878415  0.857617\n",
      "2         [16, 8, 16, 2]  0.611656  0.403706\n",
      "3          [16, 4, 8, 2]  0.613820  0.380349\n",
      "4      [16, 10, 8, 6, 2]  0.613820  0.380349\n",
      "5  [16, 8, 16, 32, 8, 2]  0.613820  0.380349\n",
      "lamb = 0.1 eps = 0.001 alpha = 5 batch_size = 5\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.919538  0.915942\n",
      "1            [16, 16, 2]  0.917213  0.913613\n",
      "2         [16, 8, 16, 2]  0.912615  0.909290\n",
      "3          [16, 4, 8, 2]  0.861260  0.812301\n",
      "4      [16, 10, 8, 6, 2]  0.613820  0.380349\n",
      "5  [16, 8, 16, 32, 8, 2]  0.613820  0.380349\n",
      "lamb = 0.1 eps = 0.01 alpha = 5 batch_size = 15\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.917266  0.912468\n",
      "1            [16, 16, 2]  0.914832  0.908911\n",
      "2         [16, 8, 16, 2]  0.898110  0.884912\n",
      "3          [16, 4, 8, 2]  0.776591  0.658872\n",
      "4      [16, 10, 8, 6, 2]  0.721167  0.579307\n",
      "5  [16, 8, 16, 32, 8, 2]  0.787129  0.714424\n",
      "lamb = 0.1 eps = 0.001 alpha = 1 batch_size = 1\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.666093  0.490431\n",
      "1            [16, 16, 2]  0.700453  0.557636\n",
      "2         [16, 8, 16, 2]  0.613820  0.380349\n",
      "3          [16, 4, 8, 2]  0.613820  0.380349\n",
      "4      [16, 10, 8, 6, 2]  0.613820  0.380349\n",
      "5  [16, 8, 16, 32, 8, 2]  0.613820  0.380349\n",
      "lamb = 0.1 eps = 0.0001 alpha = 1 batch_size = 1\n",
      "            Architecture  Accuracy        F1\n",
      "0             [16, 8, 2]  0.625184  0.405957\n",
      "1            [16, 16, 2]  0.737136  0.625575\n",
      "2         [16, 8, 16, 2]  0.613820  0.380349\n",
      "3          [16, 4, 8, 2]  0.613820  0.380349\n",
      "4      [16, 10, 8, 6, 2]  0.613820  0.380349\n",
      "5  [16, 8, 16, 32, 8, 2]  0.613820  0.380349\n"
     ]
    }
   ],
   "source": [
    "hyper_params_h = [[0.1,0.01,5,1],[0.1,0.001,5,1],[0.1,0.001,5,5],[0.1,0.01,5,15],[0.1,0.001,1,1],[0.1,0.0001,1,1]]\n",
    "for params in hyper_params_h:\n",
    "    house_test(params[0],params[1],params[2],params[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5CElEQVR4nO3deXxdVbn/8c83U9OMHZK2STpP0NBS6ARlLCiKyCgoIIKAyiT89CqKeK/C9eXsVVEQGQTKPMgsIgJCoYwd6FzoPKZTmk5J2qYZnt8fe6c9DcnJaZuTc9o879drv3LO2tOzd5LznL3W3mvJzHDOOedakpLoAJxzziU3TxTOOeei8kThnHMuKk8UzjnnovJE4ZxzLipPFM4556LyROFcBybp55I2SlqX6Fhc8vJE0QFJWi7ps03KLpf0TqJiiiTpWEnVknKamTdD0vWtrD9J0jfjENflkuolVYXTMkkPSBq6D9uYKOnnBxDDrZJqw/1vkfSepPH7ua2+wPeBUjPrtb8xuUOfJwqXdMzsA2A1cEFkuaThQCnweCLiCr1vZjlAPvBZYAcwPYytvTwZxlAIvAM8K0n7sgFJaUBfoMLMNuxrAOH6roPwROGaJWlY+M18i6R5ks6OmLfXN/bIqxEF/ihpg6RtkuY0fohK6iTp/yStlLRe0l2SOrcQwoPAZU3KLgNeNrMKScdJmippa/jzuHAfvwBOBO4Iv3XfEZYfLuk1SZskLZD0lYj4z5A0X1KlpDJJN7Z2fsys3syWmNl1wFvArRHb+7ukdWFsb0s6Iiy/CrgE+GEY2z/C8h9JWhLuf76k81rbfxhDbXieegHdJeVLuk/S2vA4fi4pNdzH5ZLeDX83FcAk4DWgOIxlYrjc2eHve0v4ex4WcVzLJd0kaTZQLWmwJJN0haRVkjZLukbSWEmzw23cEbH+IElvSKoIq7seldSlyfZvDNfdKulJSZkR88+RNDP8u1oi6fSwvMXjdm3EzHzqYBOwHPhsk7LLgXfC1+nAYuDHQAZwKlAJHBbOnwR8s4V1Pw9MB7oAAoYBReG8PwIvAt2AXOAfwK9aiLEPUAf0Cd+nEFxlnBuuvxm4FEgDLg7fd28hvmxgFXBFuPzRwEaCKheAtcCJ4euuwKgWYtp9nE3KrwTWN3mfC3QCbgNmRsybCPy8yfpfBorDY7wQqG48Z83s61bgkfB1J+B3wMrw/XPA3eHx9gCmAFdHxF4H3BCeg87ABGB1xLaHhvs+Lfwb+GH4d5AR8XczM/zddAb6AwbcBWQCnwN2As+H+y8BNgAnh+sPDrfdieBq6G3gtiZ/l1PCc9EN+Bi4Jpw3Dtgarp8Sbvvw1o7bpzb6zEh0AD4l4Jce/ENWAVsipu3s+bA/EVgHpESs8zhwa/h6Ei0nilOBhcCxTdZX+CE0KKJsPLAsSpyvAz8OX58GlIcfYJcCU5os+z5weQvxXQhMbrL83cAt4euVwNVAXivnbfdxNik/HahtYZ0u4Ydpfvh+Ik0SRTPrzATOaWHercCu8He2AXgDGA30BGqAzhHLXgy8GRH7yibbmsDeieInwFMR71OAMmBCxN/NlRHz+4fHVhJRVgFcGPH+GeC7LRzLucCMJn+XX4t4/1vgrojf1x+b2UbU4/apbSaveuq4zjWzLo0TcF3EvGJglZk1RJStIPgWF5WZvQHcAfwF2CDpHkl5BN8gswjq87dI2gK8Epa35EGCpED48wkLqluKw3giRYuvH3BM437DfV9CUGUDcD5wBrBC0lva98bhEmATgKRUSb8Oq0a2EXz4ARS0tLKky8IqlcbYhkdbnuDDvIuZ9TCzU81seniM6cDaiO3cTfANu9GqVo5jr/Ma/v5Xsfd5bW4b6yNe72jmfQ6ApJ6Sngirh7YBjzRznJF3X21vXJfgKmZJM/uO5bjdAfJE4ZqzBugjKfLvoy/Bt0sIrgyyIubtdceMmf3ZzEYTNDwPBX5AUNWzAzgiIkHlW9Ao25Jngd6STgG+RJA4GuPr12TZyPiadom8CngrMjGaWY6ZXRvGO9XMziH4cHkeeCpKTM05D5gcvv4qcA5BQ3c+wbduCK6oPhWbpH7AvcD1BFVnXYC5EcvHahXBN+uCiGPMM7MjIpZpravovc6rJBF8QJdFLHMg3U3/Mlx/hJnlAV8j9uNcBQxqoby143YHyBOFa86HBN/mfigpXdIE4CzgiXD+TOBLkrIkDQa+0bhi2JB5jKR0goSyE2gIv53eC/xRUo9w2RJJn28pCDOrBp4GHgBWmNm0cNbLwFBJX5WUJulCgqT0Ujh/PTAwYlMvhctfGh5PehjnMEkZki6RlB9erWwDIq+kmhVeOQyQdDtBFc7/hrNyCT64KgiS6S+brNo0tmyCD8/ycLtXEFxR7BMzWwu8CvxeUp6klLDx+OR92MxTwBclfSb8/X0/PJb39jWeFuQSVHlulVRC8AUiVvcBV4SxpYR/O4e30XG7VniicJ9iZrsIEsMXCK4E7gQuM7NPwkX+SFBPvp7gW/6jEavnESSEzQTVGBUEDa4ANxE0jn4QVj28DhzWSjgPEnzLfSgivgrgTIIPsgqCRtczzWxjuMifgAvCu3D+bGaVBA2tFxF8a14H/IagURWCaq3lYUzXEFRLtWS8pCqChDIpPN6xZjYnnP9QeNxlwHzggybr3weUhtUkz5vZfOD3BG0s64ERwLutnJOWXEZw88F8gvP/NFAU68pmtoDgW/7tBL/3s4Czwr+HtvC/wCiCRul/ElwxxhrbFIKbEf4Yrv8We65+Dui4Xetk5gMXOeeca5lfUTjnnIvKE4VzzrmoPFE455yLyhOFc865qA6pjr0KCgqsf//+iQ7DOecOGtOnT99oZtEefD20EkX//v2ZNm1a6ws655wDQFLTXg4+xauenHPOReWJwjnnXFSeKJxzzkXlicI551xUniicc85F5YnCOedcVJ4onHPORdXhE0VtfQN/nbSEtxeWJzoU55xLSh0+UaSliLvfXsK/5q5NdCjOOZeUOnyikMSwXnnMX7Mt0aE451xS6vCJAqC0OI9P1lVSV9/qCJjOOdfheKIAhhXlUVPXwPKK6kSH4pxzSccTBTCsKBeA+WsrExyJc84lH08UwJAeuaSnio/XejuFc8415YkCyEhLYVBhjjdoO+dcMzxRhEqL8/yKwjnnmuGJIlRalMeGyho2VtUkOhTnnEsqnihCpUV5AH5V4ZxzTXiiCA3zROGcc83yRBHqmp1Br7xMb9B2zrkmPFFECBq0/VkK55yL5IkiwrCiXBaXV7Gztj7RoTjnXNLwRBGhtCif+gZj8YaqRIfinHNJwxNFhD1deXg7hXPONfJEEaFf92w6p6d6g7ZzzkXwRBEhNUUcXpTrt8g651wETxRNDCvKY/7abZhZokNxzrmk4ImiidKiPCp31lG2ZUeiQ3HOuaTgiaKJPU9o+/MUzjkHcUwUku6XtEHS3Bbm/0DSzHCaK6leUrdw3umSFkhaLOlH8YqxOYf3ykXCG7Sdcy4UzyuKicDpLc00s9+Z2VFmdhRwM/CWmW2SlAr8BfgCUApcLKk0jnHuJbtTGv27Z3uDtnPOheKWKMzsbWBTjItfDDwevh4HLDazpWa2C3gCOCcOIbZoWFGuP0vhnHOhhLdRSMoiuPJ4JiwqAVZFLLI6LGs3pUV5rNy0ncqdte25W+ecS0oJTxTAWcC7Zhbr1cdeJF0laZqkaeXl5W0SUGOD9oJ13qDtnHPJkCguYk+1E0AZ0Cfife+wrFlmdo+ZjTGzMYWFhW0SUGlxkCi8+sk55xKcKCTlAycDL0QUTwWGSBogKYMgkbzYnnH1ysukS1a6N2g75xyQFq8NS3ocmAAUSFoN3AKkA5jZXeFi5wGvmll143pmVifpeuDfQCpwv5nNi1ecLcTOsF55fousc84Rx0RhZhfHsMxEgttom5a/DLzc9lHFrrQ4j0c/XEF9g5GaokSG4pxzCZUMbRRJaVhRHjtrG1i2sbr1hZ1z7hDmiaIFpUXeoO2cc+CJokWDe+SQnipv0HbOdXitJgpJx8dSdqjJSEthcI9cb9B2znV4sVxR3B5j2SFnmA9i5JxzLd/1JGk8cBxQKOl7EbPyCG5bPeSVFuXx7EdlbKyqoSCnU6LDcc65hIh2RZEB5BAkk9yIaRtwQfxDS7zS3WNT+FWFc67javGKwszeAt6SNNHMVkjKMrPt7Rhbwg2LSBQnDmmb7kGcc+5gE0sbRbGk+cAnAJJGSrozvmElh67ZGRTlZ3qDtnOuQ4slUdwGfB6oADCzWcBJcYwpqQwryvNhUZ1zHVpMz1GY2aomRfVxiCUpDSvKZUl5FTtrO8whO+fcXmJJFKskHQeYpHRJNwIfxzmupFFalE9dg7F4Q1WiQ3HOuYSIJVFcA3ybYJS5NcBR4fsOYVhRLuBdeTjnOq5We481s43AJe0QS1Lq1z2brIxUb9B2znVYLV5RSPqWpCHha0m6X9JWSbMljWq/EBMrNUUc1suf0HbOdVzRqp6+AywPX18MjAQGAt8D/hTfsJJLaVEeH6/dhpklOhTnnGt30RJFnZnVhq/PBB4yswozex3Ijn9oyWNYUR7bdtZRtmVHokNxzrl2Fy1RNEgqkpQJfAZ4PWJe5/iGlVz2PKHtz1M45zqeaInip8A0guqnFxvHrZZ0MrA0/qElj8N75SLhDdrOuQ4pWl9PL0nqB+Sa2eaIWdOAC+MeWRLJ7pRG/+7Z3qDtnOuQot4ea2Z1wOYmZR1yEOnSojzmrtma6DCcc67d+VCoMRpWlMuKiu1U7qxtfeEm6uobqKnzLkCccwcnTxQxamzQXrBu3xq0q2vq+Mrd73PJvR/GIyznnIu7WMbM/k8sZYe60uIgUexLVx41dfVc88h0Plq5hRmrtnjHgs65g1K0J7MzJXUDCiR1ldQtnPoT9PvUofTKy6RLVnrMDdr1Dcb3nprF5EUb+cLwXtQ3GJ/s49WIc84lg2hXFFcD04HDw5+N0wvAHfEPLblIorQoj/kxPEthZvz0hbn8c/ZafnzG4fz3F4cBMKfMG8OdcwefaLfH/gn4k6QbzOz2dowpaQ0ryuPRD1dQ32CkpqjF5f742kIe/XAl15w8iKtOGoSZ0SUrnbmrPVE45w4+sTRmr5OUCyDpfyQ925E6BYxUWpTHztoGlm1s+Q7h+99Zxp/fWMyFY/pw0+mHAcHVyIiSfL+91jl3UIolUfzEzColnQB8FrgP+GtrK4W9zW6QNDfKMhMkzZQ0T9JbEeXLJc0J502L5UDaQ+OdTy01aD83YzU/e2k+nz+iJ784bzjSnquO4SX5LFxf6bfJOucOOrEkisZPti8C95jZP4GMGNabCJze0kxJXYA7gbPN7Ajgy00WOcXMjjKzMTHsq10M7pFDeqqabdB+45P13Pj32Ywf2J0/XXQ0aal7n9oRJfnU1ts+317rnHOJFkuiKJN0N0G3HS9L6hTLemb2NrApyiJfBZ41s5Xh8htiiCWhMtJSGNzj02NTTFm2iWsf+YjSojzuuWw0mempn1p3eHE+4A3azrmDTyyJ4ivAv4HPm9kWoBvwgzbY91Cgq6RJkqZLuixingGvhuVXRduIpKskTZM0rby8vA3Cim5YUe5enQPOX7ONbzw4lZIunZl4xVhyM9ObXa9Pt87kd05nbpn3F+WcO7jEcmWwHdgAnBAW1QGL2mDfacBogiqtzwM/kTQ0nHeCmY0CvgB8W9JJUeK7x8zGmNmYwsLCNggrutKiPDZU1rCxqoYVFdVcdv8UsjPSePibx9A9p1OL60lieEkec/2Kwjl3kInlyexbgJuAm8OidOCRNtj3auDfZlYdjsv9NsEoephZWfhzA/AcMK4N9tcmSsMG7bcWlPO1+z6krqGBh78xjpIurQ/RMbwknwXrKtlV1xDvMJ1zrs3EUvV0HnA2UA1gZmuA3DbY9wvACZLSJGUBxwAfS8qOuB03G/gc0OKdU+2t8c6nm56ZTUXVLh64fCxDesZ2OoYX57OrvoGF671B2zl38IjazXhol5mZJIPdH96tkvQ4MIGgC5DVwC0EVyOY2V1m9rGkV4DZQAPwNzObK2kg8Fx4a2ka8JiZvbKPxxU3XbMzKMrPZGNVDXdfOpqj+3aNed0RJUGD9tyyrQwPXzvnXLJrMVFI+qWZ/Rh4KrzrqYukbwFXAve2tmEzuziGZX4H/K5J2VLCKqhk9bNzhpOVkcrxgwv2ab1+3bPIzUxjTtlWLopTbM4519aiXVGcDvzYzP5P0mnANuAw4Kdm9lq7RJekTivtuV/rSWJ4cb43aDvnDirREkWqpK6A2NMhIACSuplZtGckXAuGl+Tx4PsrqK1vID3VhwNxziW/aImisddYETzX0Kjx/cA4xnXIGl6Sz666Bhatr9o9xoVzziWzaIlivpkd3W6RdBCRDdqeKJxzBwOv+2hn/btnk9MpzbvycM4dNKIlij+1WxQdSEqKKC3O80ThnDtotJgozGxiO8bRoYwoyefjtduoq/cntJ1zyc+rnhJgREk+NXUNLC6vSnQozjnXKk8UCdD4VPYcHxrVOXcQaLULD0mFwLeA/pHLm9mV8Qvr0DagIJusjFTmrdn2qdGanHMu2cTS19MLwGTgdfaMducOQGqKOMIbtJ1zB4lYEkWWmd0U90g6mOEl+TwxZRX1DUZqilpfwTnnEiSWNoqXJJ0R90g6mOHF+eyorWeJN2g755JcLIniOwTJYqekynDy8TwP0Ijee57Qds65ZBbLUKi5ZpZiZpnh61wz874nDtCgwhw6p6d6O4VzLunF0kaBpLOBxnGrJ5nZS/ELqWNIDZ/Q9isK51yyi2XM7F8TVD/ND6fvSPpVvAPrCIYX5zFvzTbqG6z1hZ1zLkFiaaM4AzjNzO43s/sJBjT6YnzD6hiGl+SzfVc9yzZWJzoU55xrUaxPZneJeO2DPbcRb9B2zh0MYkkUvwJmSJoo6UGCwYx+Ed+wOobBhTl0SkvxBm3nXFJrtTHbzB6XNAkYGxbdZGbr4hpVB5GWmsKwIn9C2zmX3GK668nM1gIvxjmWDmlEST7PzSijocFI8Se0nXNJyHuPTbARJflU1dSxvMIbtJ1zyckTRYLt7nLcq5+cc0kqpkQhqaukIyQNlOTJpQ0N6ZlDRlqK3/nknEtaLbZRSMoHvg1cDGQA5UAm0FPSB8CdZvZmu0R5CEtPTWFYr1zmlnn3Wc655BStMftp4CHgRDPbEjlD0mjgUkkDzey+OMbXIQwvyefFWWswMyRv0HbOJZcWq5HM7DQze7hpkgjnTTez70ZLEpLul7RB0twoy0yQNFPSPElvRZSfLmmBpMWSfrQPx3NQGlGST+XOOlZUbE90KM459ynRqp5GRVmvBlhpZpVRlpkI3EFwVdLc9rsAdwKnm9lKST3C8lTgL8BpwGpgqqQXzWx+lH0d1CIbtPsXZCc4Guec21u0qqfft7JeX0l/MbPfNreAmb0tqX+UbXwVeNbMVobLbwjLxwGLzWwpgKQngHMIOiQ8JA3tmUtGagpz12zlrJHFiQ7HOef20mKiMLNToq0oqRMwA2g2UcRgKJAePvWdC/zJzB4CSoBVEcutBo6JEsdVwFUAffv23c9QEisjLYXDeuX6nU/OuaTUYhuFpBNaWbcTcPUB7DsNGE3QE+3ngZ9IGrqvGzGze8xsjJmNKSwsPIBwEmt4ST5zy7Zh5l2OO+eSS7Sqp/Ml/RZ4haAjwMbbYwcDpwD9gO8fwL5XAxVmVg1US3obGBmW94lYrjdQdgD7OSgML8nj8SkrWbVpB327ZyU6HOec2y1a1dN/SeoGnA98GSgCdgAfA3eb2TsHuO8XgDskpRE8p3EM8EfgE2CIpAEECeIigvaMQ9qIsEF77pqtniicc0klaqeAZrYJuDec9omkx4EJQIGk1cAtQHq43bvM7GNJrwCzgQbgb2Y2N1z3euDfQCpwv5nN29f9H2wO65VLWoqYU7aVM0YUJToc55zbLabeY/eHmV0cwzK/A37XTPnLwMvxiCtZdUpLZWhPb9B2ziUf77cpiYwoyWdO2VZv0HbOJRVPFElkeO98tmyvpWzLjkSH4pxzu7WaKCRlSfqJpHvD90MknRn/0Dqe3Q3aXv3knEsisVxRPEDQZcf48H0Z8PO4RdSBHd4rl9SwQds555JFLIliUNhNRy2AmW0HvIvTOMhMT2VIjxzvctw5l1RiSRS7JHUGDEDSIIIrDBcHI0rymesN2s65JBJLoriF4OnsPpIeBf4D/DCuUXVgI3rnU1G9i7VbdyY6FOecA2J4jsLMXpP0EXAsQZXTd8xsY9wj66COKN7T5Xhxl84JjsY552K76+k8oM7M/mlmLwF1ks6Ne2QdVGlRHimCed6g7ZxLEjFVPZnZ7k+tcMS7W+IWUQfXOSOVIT1y/c4n51zSiCVRNLdM3Lr+cEGX47NXb6W+wRu0nXOJF0uimCbpD5IGhdMfCLodd3FyWmkPKqp38a+5axMdinPOxZQobgB2AU+GUw3w7XgG1dF9rrQXgwqz+cubS/w2WedcwrWaKMys2sx+1DiKnJndHA425OIkJUVcc/IgPl67jUkLyhMdjnOug4vlrqehku6R9KqkNxqn9giuIzv36BJKunTmjjcX+1WFcy6hYmmU/jtwF/A3oD6+4bhG6akpXHXSQG55cR5Tlm3imIHdEx2Sc66DiqWNos7M/mpmU8xseuMU98gcF47tQ0FOBndOWpLoUJxzHVgsieIfkq6TVCSpW+MU98gcmempXHnCAN5aWO5djzvnEiaWRPF14AfAewS3xU4HpsUzKLfH147tR26nNO6ctDjRoTjnOqhY+noa0B6BuOblZaZz2XH9uHPSEhZvqGJwj5xEh+Sc62BiGgpV0nBJX5F0WeMU78DcHlccP4BOaSnc9Za3VTjn2l8st8feAtweTqcAvwXOjnNcLkJBTicuGtuX52eU+Xjazrl2F8sVxQXAZ4B1ZnYFMBLIj2tU7lOuOmkgAPe+vTTBkTjnOppYEsUOM2sg6F48D9gA9IlvWK6p4i6d+dKoEh6fspKNVT7AoHOu/cTaKWAX4F6CO54+At6PZ1CueVefPIhd9Q3c/86yA9pOdU0dz88oo7a+oY0ic84dymK56+m68OVdkl4B8sxsdnzDcs0ZVJjDGcOLePj9FVx98iDyO6fv8zY2Ve/iiolTmbVqC9t31fPVY/rGIVLn3KEk1ruejpR0NjAKGCzpS/ENy7Xk2gmDqKyp45EPVuzzumu27ODLd73Hx2u30TOvE09OWxWHCJ1zh5pY7nq6H7gfOB84K5zOjHNcrgXDS/KZcFgh97+zjB27Yu96a/GGKi7463ts2FbDw1eO4+qTBjFr1RY+XrstjtE65w4FsVxRHBt2L/51M7sinK5sbSVJ90vaIGluC/MnSNoqaWY4/TRi3nJJc8Jyfwq8iW+fMpiK6l08OXVlTMvPXLWFL9/1HrvqjSeuPpZjBnbnvKNLyEhN4cmpflXhnIsulkTxvqTS/dj2ROD0VpaZbGZHhdPPmsw7JSwfsx/7PqSN7d+Ncf27cc/bS9lVF71BevKicr567wfkZKbxzLXjOaI4uLO5a3YGnzuiJ8/NKGNnrXcK7JxrWSyJ4iGCZLFA0uzwm36rjdlm9jaw6YAjdM269pRBrNm6k+dnlrW4zEuz13DlxKn07ZbFM9ccR7/u2XvNv2hsX7buqOXV+evjHa5z7iAWS6K4D7iU4OqgsX3irDba/3hJsyT9S9IREeUGvCppuqSrom1A0lWSpkmaVl7ecUaDmzC0kNKiPO6atIT6hk8PbPTw+8u54fEZHNWnC09ePZ4eeZmfWua4Qd3p3bVzzFVYzrmOKZZEUW5mL5rZMjNb0Ti1wb4/AvqZ2UiC7kGej5h3gpmNAr4AfFvSSS1txMzuaRymtbCwsA3COjhI4tunDGbpxmpembtud7mZcdvrC/nJC/P4zOE9ePgbx7R4G21KirhwTB/eXVzByort7RW6c+4gE0uimCHpMUkXS/pS43SgOzazbWZWFb5+GUiXVBC+Lwt/bgCeA8Yd6P4ORacP78XAgmzunBQMl9rQYNz64jxue30R54/qzV1fG01memrUbVwwpjcpgqf8VlnnXAtiSRSdgRrgc7Th7bGSeklS+HpcGEuFpGxJuWF5drjfZu+c6uhSU8Q1EwYxb802Xv94A995ciYPvr+Cb504gN9dcCRpqa3/eovyO3Py0EL+Pn0Vdf6ktnOuGVGfzJaUClSY2Y37umFJjwMTgAJJq4FbgHQAM7uLoLPBayXVATuAi8zMJPUEngtzSBrwmJm9sq/77yjOPaqE215byLWPTKeuwfjRFw7nmpMH7dM2Lhzbl2semc7bi8o59fCecYrUOXewipoozKxe0vH7s2Ezu7iV+XcAdzRTvpSgh1oXg4y0FK4/dQg/eWEuvz3/SL4ydt/7a/zMsB4U5GTwxJRVniicc5/Sal9PwExJLwJ/B6obC83s2bhF5fbJV4/py9lHFZPTKZZf56elp6Zw/uje/G3yMjZU7qRH7qfvkHLOdVyxtFFkAhXAqXgXHklrf5NEowvH9KG+wXhmesvPZTjnOqZYeo+9oj0CcYk1sDCHcf278dS0VVxz8kDCNiLnnIupU8Dekp4L+23aIOkZSb3bIzjXvi4c24dlG6uZsswfqHfO7RFL1dMDwItAcTj9Iyxzh5gzRhSR2ynNOwp0zu0llkRRaGYPmFldOE0EOs4j0B1I54xUzjm6mH/OWcvWHbWJDsc5lyRiSRQVkr4mKTWcvkbQuO0OQReN7UtNXQMvRuls0DnXscSSKK4EvgKsA9YSPCjnDdyHqOEl+ZQW5fGEVz8550ItJgpJvwlfjjOzs82s0Mx6mNm5ZubdjR7CLhrXh3lrtjG3bGuiQ3HOJYFoVxRnhH0x3dxewbjkcM7IEjql+eh3zrlAtETxCrAZOFLSNkmVkT/bKT6XAPlZ6ZwxoojnZ5bt07jczrlDU4uJwsx+YGZdgH+aWZ6Z5Ub+bL8QXSJcOLYPlTvr+NfctYkOxTmXYFEbs8PeYz0pdEDHDOhG/+5ZXv3knIueKMysHmiQlN9O8bgkIYmvjO3Dh8s2sbS8KtHhOOcSKJbbY6uAOZLuk/TnxinegbnEu2BUb1JTxFPTVic6FOdcAsXS5eiz4eQ6mB55mZx6eA+enr6a739uKOkxjJjnnDv0xNJ77IOSOgN9zWxBO8TkkshFY/vw2vz1vPHJBj5/RK+47GNnbT1/+s8iBhXmcMFo72/SuWQTS++xZwEzCW6XRdJR4UBGrgM4eWghPfM68VScGrWXbazmvDvf46+TlnDzs7NZsK4yLvtxzu2/WOoSbgXGAVsAzGwmMDBuEbmkkpaawgWje/Pmgg2s2bKjTbf90uw1nHX7O6zduoM/fGUkeZnp3Pj3WdTVN7TpfpxzByaWRFFrZk37cvD/5A7kwjF9SU0RZ93+Do98sOKAP8hr6uq55YW5XP/YDIb0zOGf/+9EvjSqNz87ZzhzyrZyz+SlbRS5c64txJIo5kn6KpAqaYik24H34hyXSyJ9u2fx7LXHM6hHDv/z/FxO/9Nk3vhkPWa2z9tatWk7X77rfR58fwXfOGEAT141npIunQH44pFFnDGiF7e9tohF670KyrlkEUuiuAE4AqgBHgO2At+NY0wuCY3onc+TVx3L3ZeOpq6+gSsnTuNr933I/DWx9+by6rx1nPHnySzbWM3dl47mJ2eWkpG295/gz84ZTnanVG58erZXQTmXJNTSt0JJmcA1wGBgDnCfmdW1Y2z7bMyYMTZt2rREh3HI21XXwCMfrODPbyxi645aLhjVmxs/fxg98zKbXb62voHf/OsT/vbOMkaU5POXr46ib/esFrf/wswyvvPETG7+wuFcffKgeB2Gcw6QNN3MxkRdJkqieBKoBSYDXwCWm9l32zrItuSJon1t3V7LHW8uYuJ7y0lLSeGqkwZy9ckDycrYc9d12ZYdXP/YR8xYuYWvj+/Hj784jE5pqVG3a2Zc/fB0Ji0s51/fOZFBhTnxPhTnOqwDTRRzzGxE+DoNmGJmo9o+zLbjiSIxVlZs5zevfMI/56ylR24nbvzcYZw/ujdvLyznv56aSV298evzR3DmkcUxb3ND5U5O+8PbDO6Rw1NXjyc1RXE8Auc6rlgSRbQ2it2DJid7lZNLrL7ds/jLJaN4+prxFHfpzA+fmc0p/zeJKyZOpSi/M/+44YR9ShIAPXIzufXsUqav2MwD7y6LU+TOuVhESxQjw/EntkmqpMm4FO0VoDt4jOnfjeeuO47bLz6ajLQULjmmL89ddxwDCrL3a3vnHlXCZw7vwf+9uoBlG6vbOFrnXKyijUeRGo4/0TgGRdq+jEch6X5JGyTNbWH+BElbJc0Mp59GzDtd0gJJiyX9aP8OzSWCJM4aWczr3zuZX5w3gsz06O0RrW3rF+eNID01hZuenk1Dw77fjuucO3Dx7OVtInB6K8tMNrOjwulnsHsMjL8QNKCXAhdLKo1jnC6J9crP5KdnljJl+SYeen95osNxrkOKW6Iws7eBTfux6jhgsZktNbNdwBPAOW0anDuoXDC6NxMOK+Q3ryxgRYVXQTnX3hLdb/R4SbMk/UvSEWFZCRDZA93qsKxZkq6SNE3StPLy8njG6hJEEr88bwRpKeKmZ7wKyrn2lshE8RHQz8xGArcDz+/PRszsHjMbY2ZjCgsL2zI+l0SKu3Tmv784jA+WbuLRKSsTHY5zHUrCEoWZbTOzqvD1y0C6pAKgDOgTsWjvsMx1cBeO7cOJQwr41csfs2rT9kSH41yHkbBEIamXJIWvx4WxVABTgSGSBkjKAC4CfPwLhyR+9aURCPjRs7P3q1PCWNXWN1DWxt2qO3ewiluikPQ48D5wmKTVkr4h6RpJ14SLXADMlTQL+DNwkQXqgOuBfwMfA0+Z2bx4xekOLr27ZnHzGcN4d3EF97y9lNo27jhw/bad3Pb6Qk74zRsc/+s3+OukJXFNSM4dDFrswuNg5F14dAwNDcbXH5jC5EUb6ZadwRkjenH2yBLG9OtKyn509WFmfLB0E498sIJ/z1tHXYNx8tBCMtJSeG3+ei4c04efnzfcxwx3h6RYuvBodcxs55JNSoq47+tjeXPBBl6ctYanp6/mkQ9WUpyfyZkjizl7ZDFHFOcR1my2qHJnLc/NKOPh91ewaEMV+Z3TueL4/lxyTD/6F2TT0GD88fWF3P7GYlZt3s5fLxlNflZ6Ox2lc8nDryjcQa+qpo7X56/nxVlreHthOXUNxsDCbM46spizjyr+VO+zn6zbxsPvr+C5GWVs31XPiJJ8Lh3fj7NHFjf7JPnT01dz87Oz6dstiwcuHxe1i3TnDjYH1HvswcgThdtcvYt/zV3Hi7PK+HDZJszgiOI8zh5ZTM+8TB77cCVTlm8iIy2Fs0cWc+mx/RjZp0ur2/1gaQVXPzyd1BRx72WjGd2vW/wPxrl24InCdWjrtu7kpdlr+MesNcxaHQz73rdbFl87ti9fHt2HrtkZ+7S9peVVXDlxKmu27uR3FxzJOUe1+ByocwcNTxTOhZZvrGZDZc1+N3g32ly9i6sfns6U5Zv43mlDueHUwa22hTiXzA50PArnDhn9C7IZN6DbASUJgK7ZGTz8zXF86egS/vDaQr7/1Cxq6urbKErnkpPf9eTcPuqUlsrvvzKSAQXZ/P61hazevIO7Lx29z1VZzh0sPFE4tx8kccNnhtC3exY/eHo25935LvdfPpaBbTy+d01dPdOXb2by4o1MXbaJYUV5XHXSQPp08zuvXPvxNgrnDtD0FZv41kPTqa1v4MQhBRxRnE9pUR6lxXn0yO20T20YZsbC9VVMXlTO5EUbmbJsEztq60lLEaXFeXy8dhsNBmePLObaCYMY2jM3jkfmOgJvzHaunays2M5v//0Js1dvZWVEh4XdszMoLc7bnTiOKM5jQEEOqRFtJRsqd/Lu4o1MXriRdxZvZENlDQADC7M5aUghJwwu4NhB3cnplMa6rTv52+SlPDZlJdt31XNaaU+umzCIo/t2bfdjdocGTxTOJcC2nbV8sraS+Wu2Mn/tNuat2cai9VXsCvulykxP4bBeeQwqyGb+2m18sq4SgK5Z6Rw/uCBIDkMKKO7SucV9bK7excT3ljPxveVs3VHL+IHdue6UQZwwuMDvwnL7xBOFc0liV10DS8qrmL9mG/PXbmP+mm0sLq9iSI8cThgSJIfSorx9viurqqaOxz9cyb2Tl7KhsoYje+dz3YRBfK601wHf4eU6Bk8UznUQNXX1PPtRGXe9tYQVFdsZVJjNtRMGc9bIIjqlfbpbEucaeaJwroOpq2/g5bnruPPNxXyyrpK0FDGwMJuhPXM5vFcuQ3vmclivXPp0zdrnK476BmP9tp2s3LSdlZu2s2rTdnbsqmdIzxyG9sxlSM9ccjr5jZQHG+891rkOJi016MPqrCOLeHvRRj5cWsHC9ZXMWr2Fl2av3b1c5/RUhoYf8IeFCeTwXrl0zkjdnQSCnzt2v1+9ecfudhaA1BSRliJq6vaU9e7aea+ENLRnLgMLs/2qJo6mLNvEso1VfHl0n7hVN/oVhXMdRHVNHQvXV7JwfSUL1lWxcH0ln6yrZGNVTYvr5HdOp2+3LPp2y6JP+LNxKuqSSarE6s07WLC+kgXrtrFgfRUL11WypLyKuobgsyUtRQwoyGZor1yG9MhhQEE2/bpn069bFl2y0r3xvQVmRl2DUR9Odbt/NgQ/64Oybz00jZq6el77r5Ob7f24NV715JxrVUVVDQvXV7Fg3TZq6hp2J4U+3bLI77x/42/sqmtgeUU1n6yrZOG6ShaECWrlpu1EfuTkZaYFSaN7VjgFCaR/QXbUZ1Bq6xuoqWtgZ239np+1Deysq6e6po7KnXVU7ayjsib8ubOWqppPv6+uqadbdgYDC7MZWJAT/CzMZkBBNlkZ7VfhUlffwH8+2cBD7y9n6vLN1NU30LAPH80PXD6WUw7vsV/79kThnEsqO2vrWbVpOysqtrO8opqVm7azvGI7KyuqWbV5B/URn46d01Mp6pKJGXsnhLqGvZaLRVZGKjmd0sjNTCMnM53c8HVWRhobKneytLyaNVt37JXEivIzmySQHAYWZFPSpXObVfFsqt7FE1NX8ugHKynbsoOi/ExOH96L7Iy03VV7qanhz5QUUgWpqSnh+6C8T7csxvbf/27vvY3COZdUMtNTGRI2fDdVV9/Ami07WV5RzYqKalZUbGfN1h2kpaTQKS2FzPRUOqWl0Ck9hcy01OBnWLZnXkRC6JRGbqd0sjulkhbDMLY7a+tZtrGapeXVLC2vYunG4OfzM8qorKnbvVy37AyOG9Sd4wcXcMLggv3qTmXO6q08+P5yXpy1hl11DYwf2J2fnDmMzw7rGVOs7c2vKJxzLgozo7yqhqXl1Swpr2L6is28u3gj67cFbTt9u2Vx/OACjh/cneMGFdCthc4hd9U18PKctTz4/nJmrNxCVkYqXxpVwmXj+ye0KxavenLOuTgwM5aUV/Hu4greWbyRD5ZU7L7qOKI4jxMGF3Dc4ALG9e/G1h21PPbhCh6bsoqNVTUMKMjmsvH9OH90b/IyEz8GuycK55xrB3X1Dcwu28q7izby7pKNTF+xmdp6IyM1hXozGsw49bAeXHZcf04cXJBUT817G4VzzrWDtNQURvXtyqi+XbnhM0PYvquOqcuDKqq0FHHR2L707X7wdg3vicI559pYVkYaJw8t5OShhYkOpU0kX/O6c865pOKJwjnnXFSeKJxzzkXlicI551xUcUsUku6XtEHS3FaWGyupTtIFEWX1kmaG04vxitE551zr4nnX00TgDuChlhaQlAr8Bni1yawdZnZU3CJzzjkXs7hdUZjZ28CmVha7AXgG2BCvOJxzzh2YhLVRSCoBzgP+2szsTEnTJH0g6dxWtnNVuOy08vLyeITqnHMdWiIfuLsNuMnMGprpc76fmZVJGgi8IWmOmS1pbiNmdg9wD4CkckkrwlkFwMb4hN4mkj0+SP4Ykz0+SP4YPb4Dl+wxthZfv9Y2kMhEMQZ4IkwSBcAZkurM7HkzKwMws6WSJgFHA80mikhmtvsxSEnTWuu/JJGSPT5I/hiTPT5I/hg9vgOX7DG2RXwJq3oyswFm1t/M+gNPA9eZ2fOSukrqBCCpADgemJ+oOJ1zrqOL2xWFpMeBCUCBpNXALUA6gJndFWXVYcDdkhoIEtmvzcwThXPOJUjcEoWZXbwPy14e8fo9YEQbhHBPG2wjnpI9Pkj+GJM9Pkj+GD2+A5fsMR5wfIfUeBTOOefannfh4ZxzLipPFM4556I65BKFpNMlLZC0WNKPEh1PI0nLJc0J+6+aFpZ1k/SapEXhz67tGM+n+uJqKR4F/hye09mSRiUwxlsllUX0BXZGxLybwxgXSPp8O8TXR9KbkuZLmifpO2F5UpzHKPEl0znMlDRF0qwwxv8NywdI+jCM5UlJGWF5p/D94nB+/wTFN1HSsohzeFRYnpD/lXDfqZJmSHopfN9259DMDpkJSCV43mIgkAHMAkoTHVcY23KgoEnZb4Efha9/BPymHeM5CRgFzG0tHuAM4F+AgGOBDxMY463Ajc0sWxr+vjsBA8K/g9Q4x1cEjApf5wILwziS4jxGiS+ZzqGAnPB1OvBheG6eAi4Ky+8Crg1fXwfcFb6+CHgyQfFNBC5oZvmE/K+E+/4e8BjwUvi+zc7hoXZFMQ5YbGZLzWwX8ARwToJjiuYc4MHw9YPAue21Y2u+L66W4jkHeMgCHwBdJBUlKMaWnAM8YWY1ZrYMWEzw9xA3ZrbWzD4KX1cCHwMlJMl5jBJfSxJxDs3MqsK36eFkwKkEz1fBp89h47l9GviM9OmuHdohvpYk5H9FUm/gi8DfwveiDc/hoZYoSoBVEe9XE/0foz0Z8Kqk6ZKuCst6mtna8PU6oGdiQtutpXiS7bxeH17W3x9RXZfQGMPL96MJvnEm3XlsEh8k0TkMq0xmEnQO+hrBlcwWM6trJo7dMYbztwLd2zM+M2s8h78Iz+EfFT4kTOJ+x7cBPwQawvfdacNzeKglimR2gpmNAr4AfFvSSZEzLbgOTJp7lZMtngh/BQYBRwFrgd8nNBpAUg5BL8jfNbNtkfOS4Tw2E19SnUMzq7dgWIHeBFcwhycynqaaxidpOHAzQZxjgW7ATYmKT9KZwAYzmx6vfRxqiaIM6BPxvndYlnC2p/+qDcBzBP8Q6xsvS8Ofie5uvaV4kua8mtn68B+3AbiXPVUjCYlRUjrBh/CjZvZsWJw057G5+JLtHDYysy3Am8B4giqbxgeCI+PYHWM4Px+oaOf4Tg+r9czMaoAHSOw5PB44W9Jygur2U4E/0Ybn8FBLFFOBIWFrfwZBQ03CR8iTlC0pt/E18DlgLkFsXw8X+zrwQmIi3K2leF4ELgvv6DgW2BpRtdKumtT3nkdwHiGI8aLwjo4BwBBgSpxjEXAf8LGZ/SFiVlKcx5biS7JzWCipS/i6M3AaQVvKm0DjqJdNz2Hjub0AeCO8amvP+D6J+CIggrr/yHPYrv8rZnazmfW2oN+8iwjOySW05TmMd0t8e08Edx0sJKjn/O9ExxPGNJDgbpJZwLzGuAjqBf8DLAJeB7q1Y0yPE1Q71BLUX36jpXgI7uD4S3hO5wBjEhjjw2EMs8M/+KKI5f87jHEB8IV2iO8Egmql2cDMcDojWc5jlPiS6RweCcwIY5kL/DQsH0iQpBYDfwc6heWZ4fvF4fyBCYrvjfAczgUeYc+dUQn5X4mIdwJ77npqs3PoXXg455yL6lCrenLOOdfGPFE455yLyhOFc865qDxROOeci8oThXPOuag8Ubg2Ickk/T7i/Y2Sbm2jbU+UdEHrSx7wfr4s6WNJb0aUjYjoIXRTRI+hr8e4zbPVSi/GkoolPR1tmVhJulzSHfu57o/bIgZ36PFE4dpKDfAlSQWJDiRSxJOpsfgG8C0zO6WxwMzmmNlRFnTh8CLwg/D9Z2PZh5m9aGa/jrZTM1tjZnFPhDHwROGa5YnCtZU6grF5/6vpjKZXBJKqwp8TJL0l6QVJSyX9WtIlCvr/nyNpUMRmPitpmqSFYd82jZ21/U7S1LBztqsjtjtZ0ovA/GbiuTjc/lxJvwnLfkrwgNp9kn7X2sFKmiTpNgVji3xH0lkK+vafIel1ST3D5XZ/ww/Pw58lvRce7wVheX+FY26Eyz8r6RUF41n8NmKf3wiPf4qke1u7coiyvyJJb4dXRnMlnSjp10DnsOzRcLnnFXRiOU97OrJEUpWkXygYo+GDiGPtKem5sHyWpOPC8q+FMc+UdHf4e0sN45sb/i4+9Xfjkse+fNtyrjV/AWZHfrjFYCQwjKA78aXA38xsnIJBdm4Avhsu15+gP51BwJuSBgOXEXSRMFZB753vSno1XH4UMNyC7rJ3k1QM/AYYDWwm6NH3XDP7maRTCcZpmBZj7BlmNibcblfgWDMzSd8k6Mnz+82sU0SQkA4nuEJprsrpKIKeXmuABZJuB+qBn4THVUnwZPCsGGJsbn9fBf5tZr+QlApkmdlkSdeHV06NrjSzTQq6rpgq6RkzqwCygQ/M7L/D3/W3gJ8DfwbeMrPzwu3mSBoGXAgcb2a1ku4ELiHooaDEzIaH569LDMfiEsQThWszZrZN0kPA/wN2xLjaVAv7wpG0BGj8oJ8DnBKx3FMWdGK3SNJSgg++zwFHRlyt5BP0T7QLmNI0SYTGApPMrDzc56MEAyQ9H2O8kZ6MeN0beFJBH0AZQHP7Bng+PI75jd/Em/EfM9saxjcf6AcUEHwIbwrL/w4MjSHG5vY3FbhfQYeBz5vZzBbW/X+Szgtf9yE4txUE5/elsHw6Qf9HEHRGdxkEPa4CWyVdSpCUpyoY8qAzQSeJ/wAGhknwn+z5vbsk5FVPrq3dRlDXnx1RVkf4tyYpheCDtFFNxOuGiPcN7P1FpmlfM0bQr84NjW0IZjbAzBo/cKoP5CBiFLmP24E7zGwEcDVBfzrNiTzelgaLiVymngP7Qvep/VkwINRJBL2ITpR0WdOVJE0APguMN7ORBP0dNR5Tre3p+6e1+AQ8GPE7OszMbjWzzQRXk5OAawgH3HHJyROFa1PhN96nCJJFo+UE3yoBziYYJWxffVlSSthuMZCg07p/A9eG34yRNFRB77zRTAFOllQQVo9cDLy1H/E0lc+ebpy/Hm3B/TSVIO6uChrPz9/fDUnqB6w3s3sJPqAbx3WubTyXBMez2cy2SzqcYFjP1vwHuDbcR6qk/LDsAkk9wvJukvopuOkhxcyeAf4nIgaXhLzqycXD74HrI97fC7wgaRbwCvv3bX8lwYd8HnCNme2U9DeCtouPFNRrlNPKcLJmtlbB7apvEnzb/aeZtUX37rcCf5e0maD9YEAbbHM3MyuT9EuCc7AJ+IRgZLL9MQH4gaRaoIqwuojgZoTZkj4CrgSukfQxQVL+IIbtfge4R9I3CK40rjWz9yX9D0FbUApBT8DfJqiafCAsg2AgIJekvPdY5w4SknLMrCq8ongOuN/Mnkt0XO7Q51VPzh08blUwdvNcgsby5xMajesw/IrCOedcVH5F4ZxzLipPFM4556LyROGccy4qTxTOOeei8kThnHMuqv8PO2hUnaAUqUUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "house_j,house_count = k_fold(house_fold,house_vals,[[16,8,2]],0.1,0.01,3,15,get_j=True)\n",
    "plt.plot(house_count,house_j)\n",
    "plt.xlabel('Number of Training Instances')\n",
    "plt.ylabel('Performance (J) on Test Set')\n",
    "plt.title('House Votes Data Performance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Correctness Verification**\n",
    "\n",
    "Below I have included 2 functions: ```train_on_first()``` and ```train_on_sec()```\n",
    "\n",
    "These functions hard code the inputs and print the desired outputs to stdout. If the output is too large for your IDE, set the max lines of your output to 100. To run these functions, simply call them without arguement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS FOR INSTANCE 1\n",
      "activations: \n",
      "a1: [[1.   0.13]]\n",
      "a2: [1.        0.601807  0.5807858]\n",
      "a3: [0.79402743]\n",
      "\n",
      "prediction: [0.79402743]\n",
      "expected: [0.9]\n",
      "cost J: 0.36557477431084995\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [-0.01269739 -0.01548092]\n",
      "delta 3: [-0.10597257]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[-0.01269739 -0.00165066]\n",
      " [-0.01548092 -0.00201252]]\n",
      "theta 2: [-0.10597257 -0.06377504 -0.06154737]\n",
      "\n",
      "OUTPUTS FOR INSTANCE 2\n",
      "activations: \n",
      "a1: [[1.   0.42]]\n",
      "a2: [1.         0.60873549 0.59483749]\n",
      "a3: [0.79596607]\n",
      "\n",
      "prediction: [0.79596607]\n",
      "expected: [0.23]\n",
      "cost J: 1.2763768066887786\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [0.06739994 0.08184068]\n",
      "delta 3: [0.56596607]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[0.06739994 0.02830797]\n",
      " [0.08184068 0.03437309]]\n",
      "theta 2: [0.56596607 0.34452363 0.33665784]\n",
      "\n",
      "regularized gradients: \n",
      "theta 1: [[0.02735127 0.01332866]\n",
      " [0.03317988 0.01618028]]\n",
      "theta 2: [[0.22999675 0.1403743  0.13755523]]\n"
     ]
    }
   ],
   "source": [
    "#function to train on backprop_example1.txt\n",
    "#outputs to stdout, if full output cannot be viewed when calling this function, increase number of lines shown in output to 100\n",
    "def train_on_first():\n",
    "\ttrain_nn = NeuralNet([1,2,1],eps=0.001)\n",
    "\ttrain_nn.weights[0] = np.array([[0.40000,0.10000 ],[0.30000,0.20000 ]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.7],[0.5],[0.6]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\t#NOTE: X values are preprocessed to include bias term (1) as first element\n",
    "\tX = np.array([[1,0.13000],[1,0.42000]])\n",
    "\tY = np.array([[0.90000],[0.23000]])\n",
    "\ttrain_df = pd.DataFrame(data=train_set_1)\n",
    "\ttrain_df.insert(0,'bias',np.ones)\n",
    "\ttrain_nn.train(X,Y,2,for_exam=True)\n",
    "\n",
    "train_on_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS FOR INSTANCE 1\n",
      "activations: \n",
      "a1: [[1.   0.32 0.68]]\n",
      "a2: [1.         0.67699586 0.75384029 0.5881687  0.70566042]\n",
      "a3: [1.         0.87519469 0.89296181 0.81480444]\n",
      "a4: [0.83317658 0.84131543]\n",
      "\n",
      "prediction: [0.83317658 0.84131543]\n",
      "expected: [0.75 0.98]\n",
      "cost J: 0.7907366961135718\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [-0.00086743 -0.00133354 -0.00053312 -0.00070163]\n",
      "delta 3: [ 0.00638937 -0.00925379 -0.00778767]\n",
      "delta 4: [ 0.08317658 -0.13868457]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[-0.00086743 -0.00027758 -0.00058985]\n",
      " [-0.00133354 -0.00042673 -0.00090681]\n",
      " [-0.00053312 -0.0001706  -0.00036252]\n",
      " [-0.00070163 -0.00022452 -0.00047711]]\n",
      "theta 2: [[ 0.00638937 -0.00925379 -0.00778767]\n",
      " [ 0.00432557 -0.00626478 -0.00527222]\n",
      " [ 0.00481656 -0.00697588 -0.00587066]\n",
      " [ 0.00375802 -0.00544279 -0.00458046]\n",
      " [ 0.00450872 -0.00653003 -0.00549545]]\n",
      "theta 3: [[ 0.08317658 -0.13868457]\n",
      " [ 0.0727957  -0.121376  ]\n",
      " [ 0.07427351 -0.12384003]\n",
      " [ 0.06777264 -0.1130008 ]]\n",
      "\n",
      "OUTPUTS FOR INSTANCE 2\n",
      "activations: \n",
      "a1: [[1.   0.83 0.02]]\n",
      "a2: [1.         0.63471542 0.69291867 0.54391158 0.64659376]\n",
      "a3: [1.         0.86020091 0.88336451 0.79790763]\n",
      "a4: [0.82952703 0.83831889]\n",
      "\n",
      "prediction: [0.82952703 0.83831889]\n",
      "expected: [0.75 0.28]\n",
      "cost J: 1.9437823352945294\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [0.01694006 0.01465141 0.01998824 0.01622017]\n",
      "delta 3: [0.01503437 0.05808969 0.06891698]\n",
      "delta 4: [0.07952703 0.55831889]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[0.01694006 0.01406025 0.0003388 ]\n",
      " [0.01465141 0.01216067 0.00029303]\n",
      " [0.01998824 0.01659024 0.00039976]\n",
      " [0.01622017 0.01346274 0.0003244 ]]\n",
      "theta 2: [[0.01503437 0.05808969 0.06891698]\n",
      " [0.00954254 0.03687042 0.04374267]\n",
      " [0.01041759 0.04025143 0.04775386]\n",
      " [0.00817737 0.03159565 0.03748474]\n",
      " [0.00972113 0.03756043 0.04456129]]\n",
      "theta 3: [[0.07952703 0.55831889]\n",
      " [0.06840922 0.48026642]\n",
      " [0.07025135 0.4931991 ]\n",
      " [0.06345522 0.44548691]]\n",
      "\n",
      "regularized gradients: \n",
      "theta 1: [[0.00803632 0.02564134 0.04987447]\n",
      " [0.00665894 0.01836697 0.06719311]\n",
      " [0.00972756 0.03195982 0.05251862]\n",
      " [0.00775927 0.05036911 0.08492365]]\n",
      "theta 2: [[0.01071187 0.09068406 0.02511708 0.1259677  0.11586492]\n",
      " [0.02441795 0.06780282 0.04163777 0.05307643 0.1267652 ]\n",
      " [0.03056466 0.08923522 0.1209416  0.10270214 0.03078292]]\n",
      "theta 3: [[0.0813518  0.17935246 0.12476243 0.13186393]\n",
      " [0.20981716 0.19194521 0.30342954 0.25249305]]\n"
     ]
    }
   ],
   "source": [
    "#function to train on backprop_example2.txt\n",
    "#outputs to stdout, if full output cannot be viewed when calling this function, increase number of lines shown in output to 100\n",
    "def train_on_sec():\n",
    "\ttrain_nn = NeuralNet([2,4,3,2],eps=0.001,lamb=0.250)\n",
    "\ttrain_nn.weights[0] = np.array([[0.42000,0.15000,0.40000],[0.72000,0.10000,0.54000],[0.01000,0.19000,0.42000],[0.30000,0.35000,0.68000]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.21000,0.67000,0.14000,0.96000,0.87000],[0.87000,0.42000,0.20000,0.32000,0.89000],[0.03000,0.56000,0.80000,0.69000,0.09000]])\n",
    "\ttrain_nn.weights[2] = np.array([[0.04000,0.87000,0.42000,0.53000],[0.17000,0.10000,0.95000,0.69000]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\t#NOTE: X values are preprocessed to include bias term (1) as first element\n",
    "\tX = np.array([[1,0.32000,0.68000],[1,0.83000,0.02000]])\n",
    "\tY = np.array([[0.75000,0.98000],[0.75000,0.28000]])\n",
    "\ttrain_nn.train(X,Y,batch_size=2,for_exam=True)\n",
    "\n",
    "#train_on_first()\n",
    "train_on_sec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52fef9f371462526e6c16fbedf2b5b9bc32d90752958a415cf9672d41d7c1c70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
