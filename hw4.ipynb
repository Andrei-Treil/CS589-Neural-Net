{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,nodes,lamb=0.0,alpha=0.1,eps=0.0):\n",
    "        '''\n",
    "        Constructor for neural net\n",
    "        nodes - list detailing number of nodes in each layer\n",
    "        lamb - regularization\n",
    "        alpha - learning rate\n",
    "        eps - cost function stopping condition\n",
    "        '''\n",
    "        self.nodes = nodes\n",
    "        self.lamb = lamb\n",
    "        self.alpha = alpha\n",
    "        self.weights = []\n",
    "        self.eps = eps\n",
    "        #initialize weights for each layer, include bias\n",
    "        for i in range(len(nodes)-1):\n",
    "            self.weights.append(np.random.normal(0,1,(nodes[i]+1,nodes[i+1])).T)\n",
    "    \n",
    "    def get_sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def deriv_sigmoid(self, x):\n",
    "        return x * (1-x)\n",
    "\n",
    "    def train(self, features, targs, batch_size, test_feat=None, test_targs=None, for_exam=False, get_costs=False):\n",
    "        '''\n",
    "        features - training data features\n",
    "        targs - training data targets\n",
    "        batch size - # of instances for mini batch\n",
    "        test_feat - test data features\n",
    "        test_targs - test data targets\n",
    "        for_exam - flag to print for back_prop examples\n",
    "        get_costs - flag to get J values for varying number of samples\n",
    "        '''\n",
    "        prev_cost = math.inf\n",
    "        gradients = [0]*len(self.weights)\n",
    "        num_inst = len(targs)\n",
    "        keep_learn = True\n",
    "        count = 1\n",
    "        curr_batch = 1\n",
    "        cost_j = []\n",
    "        cost_j_count = []\n",
    "\n",
    "        while(keep_learn):\n",
    "            J = 0\n",
    "            for instance,target in zip(features,targs):\n",
    "                #iterate through layers, vectorize forward pass\n",
    "                activations = [np.atleast_2d(instance)]\n",
    "                for i in range(len(self.weights)-1):\n",
    "                    try:\n",
    "                        this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "                    except:\n",
    "                        this_a = self.get_sigmoid(self.weights[i].T.dot(activations[i].T))\n",
    "                    activations.append(np.insert(this_a,0,1))\n",
    "                try:\n",
    "                    activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "                except:\n",
    "                    activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1].T)))\n",
    "                guess = activations[-1]\n",
    "\n",
    "                #accumulate sum loss\n",
    "                target = np.array(target)\n",
    "                cost = np.sum((np.array(-target)).dot(np.log(guess)) - (np.array(1-target)).dot(np.log(1-guess)))\n",
    "                J += cost\n",
    "\n",
    "                #begin backwards propogation\n",
    "                error = guess - target\n",
    "                delta_inst = [error]\n",
    "\n",
    "                #get delta values for all weights on current instance\n",
    "                for i in range(len(self.weights)-1, 0, -1):\n",
    "                    try:\n",
    "                        this_del = (self.weights[i].T.dot(delta_inst[-1])) * self.deriv_sigmoid(activations[i].T)\n",
    "                    except:\n",
    "                        this_del = (self.weights[i].dot(delta_inst[-1])) * self.deriv_sigmoid(activations[i].T)\n",
    "                    delta_inst.append(this_del[1:])\n",
    "\n",
    "                #reverse delta values\n",
    "                delta_inst = delta_inst[::-1]\n",
    "\n",
    "                #accumulate gradients\n",
    "                for i in range(len(self.weights)-1,-1,-1):\n",
    "                    try:\n",
    "                        gradients[i] += (delta_inst[i]*(activations[i].T)).T\n",
    "                    except:\n",
    "                        gradients[i] += (np.atleast_2d(delta_inst[i]).T*np.atleast_2d(activations[i].T))\n",
    "\n",
    "                #print for examples\n",
    "                if for_exam:\n",
    "                    print(f'OUTPUTS FOR INSTANCE {count}')\n",
    "                    print(f'activations: ')\n",
    "                    for i in range(len(activations)):\n",
    "                        print(f'a{i+1}: {activations[i]}')\n",
    "                    print()\n",
    "                    print(f'prediction: {guess}')\n",
    "                    print(f'expected: {target}')\n",
    "                    print(f'cost J: {cost}')\n",
    "                    print()\n",
    "                    print('delta for this instance: ')\n",
    "                    for i in range(len(delta_inst)):\n",
    "                        print(f'delta {i+2}: {delta_inst[i]}')\n",
    "                    print()\n",
    "                    print('gradients for this instance: ')\n",
    "                    for i in range(len(self.weights)):\n",
    "                        try:\n",
    "                            print_del = (delta_inst[i]*(activations[i].T)).T\n",
    "                        except:\n",
    "                            print_del = (np.atleast_2d(delta_inst[i]).T*np.atleast_2d(activations[i].T)).T\n",
    "                        print(f'theta {i+1}: {print_del}')\n",
    "                    print()\n",
    "\n",
    "                if curr_batch == batch_size:\n",
    "                    #regularize weights and update\n",
    "                    for i in range(len(self.weights)-1,-1,-1):\n",
    "                        P = self.lamb * (self.weights[i])\n",
    "                        #set first column to all 0\n",
    "                        P[:,0] = 0\n",
    "                        try:\n",
    "                            gradients[i] = gradients[i] + P.T\n",
    "                        except:\n",
    "                            gradients[i] = gradients[i] + P\n",
    "                        gradients[i] = gradients[i] / num_inst\n",
    "                        learn_diff = self.alpha * (gradients[i])\n",
    "                        try:\n",
    "                            self.weights[i] = self.weights[i] - learn_diff\n",
    "                        except:\n",
    "                            self.weights[i] = self.weights[i] - learn_diff.T\n",
    "                    curr_batch = 0\n",
    "\n",
    "                    if get_costs:\n",
    "                        cost_j.append(self.cost_on_set(test_feat,test_targs))\n",
    "                        cost_j_count.append(count)\n",
    "\n",
    "                curr_batch += 1\n",
    "                count += 1\n",
    "\n",
    "            J /= num_inst\n",
    "            curr_s = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                curr_s += np.sum(self.weights[i][1:]**2)\n",
    "\n",
    "            #curr_s = np.sum(self.weights[1:]**2)\n",
    "            curr_s *= (self.lamb/(2*num_inst))\n",
    "            new_cost = J + curr_s\n",
    "\n",
    "            #if improvement in cost is less than epsilon, stop\n",
    "            if prev_cost - new_cost < self.eps:\n",
    "                keep_learn = False\n",
    "\n",
    "            prev_cost = new_cost\n",
    "\n",
    "            if for_exam:\n",
    "                print('regularized gradients: ')\n",
    "                for i in range(len(gradients)):\n",
    "                    print(f'theta {i+1}: {gradients[i]}')\n",
    "                keep_learn = False\n",
    "            if get_costs:\n",
    "                return cost_j,cost_j_count\n",
    "\n",
    "    #forward pass on one instance, returns an array where index of max val is the NN's guess and 0 for all else\n",
    "    #for example a guess for class 0 of 3 possible classes 0,1,2 would be [1,0,0]\n",
    "    #raw - True if wanting the raw outputs, false if wanting fixed as discussed above\n",
    "    def predict(self,instance,raw=True):\n",
    "        activations = [np.atleast_2d(instance)]\n",
    "        for i in range(len(self.weights)-1):\n",
    "            try:\n",
    "                this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "            except:\n",
    "                this_a = self.get_sigmoid(self.weights[i].T.dot(activations[i].T))\n",
    "            activations.append(np.insert(this_a,0,1))\n",
    "        try:\n",
    "            activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "        except:\n",
    "            activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1].T)))\n",
    "        guess = activations[-1]\n",
    "        pred = [0]*len(guess)\n",
    "        pred[np.argmax(guess)] = 1\n",
    "        \n",
    "        return guess if raw else pred\n",
    "    \n",
    "    def cost_on_set(self,instances,targets):\n",
    "        J = 0\n",
    "        for instance,target in zip(instances,targets):\n",
    "            guess = self.predict(instance)\n",
    "            target = np.array(target)\n",
    "            cost = np.sum((np.array(-target)).dot(np.log(guess)) - (np.array(1-target)).dot(np.log(1-guess)))\n",
    "            J += cost\n",
    "        J /= len(instances)\n",
    "        curr_s = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            curr_s += np.sum(self.weights[i][1:]**2)\n",
    "\n",
    "        curr_s *= (self.lamb/(2*len(instances)))\n",
    "        return J + curr_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decision(nn,test_set,vals):\n",
    "    test_copy = pd.DataFrame(test_set,copy=True)\n",
    "    #test_copy.insert(0,'bias',1)\n",
    "    to_guess = test_copy.drop('class',axis=1)\n",
    "    predictions = pd.DataFrame(to_guess.apply(lambda row: nn.predict(row.to_numpy(),raw=False), axis=1),columns=['predicted'])\n",
    "    predictions['actual'] = test_set.loc[predictions.index,'class']\n",
    "    prec,rec,f1 = [0,0,0]\n",
    "\n",
    "    for val in vals:\n",
    "        is_targ = predictions[predictions.predicted.apply(lambda x: x == val)]\n",
    "        not_targ = predictions[predictions.predicted.apply(lambda x: x != val)]\n",
    "        tp = len(is_targ[is_targ['predicted'] == is_targ['actual']])\n",
    "        fp = len(is_targ[is_targ['predicted'] != is_targ['actual']])\n",
    "        fn = len(not_targ[not_targ.actual.apply(lambda x: x == val)])\n",
    "        tn = len(not_targ[not_targ.actual.apply(lambda x: x != val)])\n",
    "        this_prec = (tp/(tp+fp)) if (tp+fp) > 0 else 0\n",
    "        this_rec = (tp/(tp+fn)) if (tp+fn) > 0 else 0\n",
    "        f1 += (this_prec*this_rec*2)/(this_rec+this_prec) if (this_rec+this_prec) > 0 else 0\n",
    "        prec += this_prec\n",
    "        rec += this_rec\n",
    "\n",
    "    avg_f1 = f1/len(vals)\n",
    "    accuracy = len(predictions[predictions['predicted'] == predictions['actual']])/len(test_set)\n",
    "    return accuracy,avg_f1\n",
    "\n",
    "np.random.seed(1)\n",
    "k = 10\n",
    "#function to do cross fold validation\n",
    "def k_fold(fold,vals,nn_arc,lamb,eps,alpha,batch_size,get_j=False):\n",
    "    fold_metrics = defaultdict(list)\n",
    "    #iterate through folds, taking turns being test fold\n",
    "    for i in range(k):\n",
    "        test_fold = fold[i]\n",
    "        #test_fold.insert(0,'bias',1)\n",
    "        test_targs = test_fold['class']\n",
    "        test_feat = test_fold.drop('class',axis=1)\n",
    "        train_fold = fold[0:i]\n",
    "        train_fold.extend(fold[i+1:len(fold)])\n",
    "        train_data = pd.concat(train_fold)\n",
    "       \n",
    "        #train_data.insert(0,'bias',1)\n",
    "        #iterate through architectures\n",
    "        for arc in nn_arc:\n",
    "            np_targs = train_data['class'].to_numpy()\n",
    "            np_inst = train_data.drop('class',axis=1).to_numpy()\n",
    "            this_nn = NeuralNet(arc,lamb,alpha,eps)\n",
    "            if get_j:\n",
    "                return this_nn.train(np_inst,np_targs,batch_size,test_feat.to_numpy(),test_targs.to_numpy(),get_costs=True)\n",
    "            this_nn.train(np_inst,np_targs,batch_size)\n",
    "            fold_metrics[str(arc)].append(test_decision(this_nn,test_fold,vals))\n",
    "            \n",
    "    return fold_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of neural net architectures\n",
    "wine_nn_arc = [[13,4,3],[13,8,3],[13,4,8,3],[13,8,16,3],[13,2,4,8,3],[13,2,4,8,4,3]]\n",
    "wine_df = pd.read_csv('datasets/hw3_wine.csv',delimiter='\\t')\n",
    "norm_wine_df = ((wine_df-wine_df.min())/(wine_df.max()-wine_df.min()))\n",
    "#insert column of ones to act as bias\n",
    "norm_wine_df.insert(0,'bias',1)\n",
    "\n",
    "#split data by class into k groups then combine into folds\n",
    "wine_class_1 = norm_wine_df.loc[norm_wine_df['class'] == 0].sample(frac=1)\n",
    "wine_class_1['class'] = [[1,0,0]]*len(wine_class_1)\n",
    "wc1_split = np.array_split(wine_class_1,k)\n",
    "wine_class_2 = norm_wine_df.loc[norm_wine_df['class'] == 0.5].sample(frac=1)\n",
    "wine_class_2['class'] = [[0,1,0]]*len(wine_class_2)\n",
    "wc2_split = np.array_split(wine_class_2,k)\n",
    "wine_class_3 = norm_wine_df.loc[norm_wine_df['class'] == 1].sample(frac=1)\n",
    "wine_class_3['class'] = [[0,0,1]]*len(wine_class_3)\n",
    "wc3_split = np.array_split(wine_class_3,k)\n",
    "wine_vals = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "\n",
    "#list to hold folds\n",
    "wine_fold = []\n",
    "for i in range(k):\n",
    "    this_fold = [wc1_split[i],wc2_split[i],wc3_split[i]]\n",
    "    wine_fold.append(pd.concat(this_fold))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3**  create, for each dataset and for each of the metrics\n",
    "described above, a table summarizing the corresponding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Architecture  Accuracy        F1\n",
      "0           [13, 4, 3]  0.977743  0.977975\n",
      "1           [13, 8, 3]  0.973099  0.974010\n",
      "2        [13, 4, 8, 3]  0.443698  0.251319\n",
      "3       [13, 8, 16, 3]  0.793658  0.744764\n",
      "4     [13, 2, 4, 8, 3]  0.399254  0.190154\n",
      "5  [13, 2, 4, 8, 4, 3]  0.399254  0.190154\n"
     ]
    }
   ],
   "source": [
    "wine_res = k_fold(wine_fold,wine_vals,wine_nn_arc,0.1,0.001,1,5)\n",
    "\n",
    "arc_dict = defaultdict(list)\n",
    "\n",
    "for arc,perf in wine_res.items():\n",
    "    avg_acc,avg_f1 = [0,0]\n",
    "    for res in perf:\n",
    "        avg_acc += res[0]\n",
    "        avg_f1 += res[1]\n",
    "    arc_dict['Architecture'].append(arc)\n",
    "    arc_dict['Accuracy'].append(avg_acc/10)\n",
    "    arc_dict['F1'].append(avg_f1/10)\n",
    "\n",
    "arc_table = pd.DataFrame(arc_dict)\n",
    "print(arc_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3FUlEQVR4nO3dd3xW9dnH8c83mxESRkBGCENAUYaIClUQV6UWZ5111m2tq9ph28faPk+HbbW2jqrVVq2jLrSoVdyKiiAbWcreEFYgjJBxPX+cE70TM+6E3LnvJNf79TqvnPM76zonyX3d5/c753dkZjjnnHPlkuIdgHPOucTiicE551wFnhicc85V4InBOedcBZ4YnHPOVeCJwTnnXAWeGFxMSRolaVG842gKJB0p6QtJhZJOi3c8ruXyxODqRNKtkl6rVPZFNWXnmtkkMxsQo1hM0s7wg3SzpLclnVOH9cdIWr0P++8VxlAYDssl/bS+2wN+DdxrZm3N7KV92I5z+8QTg6urD4BvSEoGkNQVSAUOqVS2f7hsrA0xs7bAAOBR4F5Jv2yE/UbKDmM4D7hN0ti6rCwpJRzNA+bVJ4CIbTi3zzwxuLr6lCARDA2nRwHvAosqlS0xs7WVv5WH36pvkTRHUoGkZyRlRMwfJ2mWpG2SPpY0OJqgzGyTmf0LuAa4VVLHcHvfk7RA0g5JSyVdFZa3AV4DukV84+8m6XBJk8P9r5N0r6S0KGOYTPDBfnC4j0vDfW+VNFFSXsRxmqRrJX0BfCFpCdAHeDmMJT2MZ4KkLZIWS7oiYv3bJT0v6QlJ24FLJL0n6f/C81Yo6WVJHSU9KWm7pE8l9YrYxl8krQrnTZc0qtL2n5X0eHju5kkaHjE/V9J4Sfnh1dq9EfOqPW7XNHhicHViZnuBKcDosGg0MAn4sFJZTVcLZwNjgd7AYOASAEmHAP8ArgI6Ag8CEySl1yHE/wApwOHh9EZgHNAO+B7wZ0nDzGwn8C1gbVh109bM1gKlwE1AJ2AkcBzw/dp2qsCRwEHATEmnAj8DzgByCM7R05VWOw04AhhoZn2BlcDJYSxFwL+B1UA34Ezgt5KOjVj/VOB5IBt4Miw7F7gQ6A70BSYD/wQ6AAuAyKupTwmSeQfgKeC5yCQNnBLGkA1MAO4NjzUZeAVYAfQK9/XvcF40x+0SnZn54EOdBuB24MVwfDbQj+CDPrLs4nB8DLA6Yt3lwAUR038AHgjH/wb8b6V9LQKOriYOA/avonw9cH4167wE3FBVbNUsf2P5cVUxr1cYwzZgK8EH7/XhvNeAyyKWTQJ2AXkRsR9baXvLgePD8VyCJJUZMf93wKMRv4MPKq3/HvDziOk7gdcipk8GZtVwrFsJqubKt/9WxLyBwO5wfCSQD6RUsY0aj9uHpjH4FYOrjw+AoyR1AHLM7AvgY4K2hw4EVSk1XTGsjxjfBbQNx/OAm8NqnG2SthF8QHaLNjBJqQTfVLeE09+S9ElYHbMNOIngaqC69ftLekXS+rCK5rc1LR/qZGbtzexAM/trxLH8JeI4tgAi+HZdblUN2+wGbDGzHRFlK6JYf0PE+O4qpsvPNWGV3oKwSm8bkEXFY638e8oI2zJygRVmVlLF/qM5bpfgPDG4+phM8CFyBfARgJltB9aGZWvNbFk9trsK+I2ZZUcMrc2sLlURpwIlwNSwCuoF4E9AFzPLBv5L8EEFwbf2yv4GLAT6mVk7gmoRVbFcNMdyVaVjaWVmH0csU1PXxmuBDpIyI8p6AmuiXL9GYXvCjwmq9dqH56aA6I51FdCzmgbvaI7bJThPDK7OzGw3MA34IUEdcrkPw7L63o30d+BqSUeEdfZtJH270odjlSR1kHQ+cB9wh5ltBtKAdIJqjxJJ3wK+GbHaBqCjpKyIskxgO1Ao6QCCxuz6eICgEfygML4sSWdFu7KZrSK4CvudpAwFjfCXAU/UM57KMgkSaD6QIuk2gnaYaEwF1gG/D39HGWH7CuzjcbvE4InB1df7QGeCZFBuUlhWr8RgZtMIrjjuJajvXkzYMF2D2ZIKw2UvB24ys9vC7e0ArgeeDbf3XYJG1PL9LSRoGF0aVn10A24Jl9tBkKieqeexvAjcAfw7rJL6jKCxuy7OI2jHWAu8CPzSzN6qTzxVmAi8DnxOUEW1h5qrtr5kZqUE7RX7EzSYrwbOCec1xHG7OJOZv6jHOefcV/yKwTnnXAWeGJxzzlXgicE551wFnhicc85V0OQ63urUqZP16tUr3mE451yTMn369E1mlhPNsk0uMfTq1Ytp06bFOwznnGtSJK2IdlmvSnLOOVeBJwbnnHMVeGJwzjlXgScG55xzFXhicM45V4EnBueccxV4YnDOOVdBi0kMS/IL+dXL89hbUhbvUJxzLqG1mMSwcvMu/vnRcibOW1/7ws4514K1mMQwun8OuR1a8a9Pon74zznnWqQWkxiSk8QFR+QxddkWFq7fHu9wnHMuYbWYxABw1vBc0lKSeMKvGpxzrlotKjF0aJPGyYO78eKMNezYUxzvcJxzLiG1qMQAcNHIPHbuLWX8jDXxDsU55xJSi0sMQ3KzGdwji399sgIzi3c4zjmXcFpcYgC4cEQeizcWMnnp5niH4pxzCadFJoaTh3Qju3WqN0I751wVWmRiyEhN5uzhuUyct4H1BXviHY5zziWUFpkYAM4/oidlZjw9dWW8Q3HOuYTSYhNDXsc2HN0/h6enrqS41PtPcs65ci02MUBw6+rGHUW8MW9DvENxzrmE0aITw9H9O9OjfSsen7w83qE451zCaNGJITlJXDAijynLtrBo/Y54h+OccwmhRScGgLO9/yTnnKugxSeGDm3SGDe4K+NnrPb+k5xzDk8MAFw0shc795by0kzvP8k55zwxAEN6ZDGoexaPT/b+k5xzzhMDIIkLR+bxxcZCpizbEu9wnHMurjwxhE4e3I2sVqn8a7I3QjvnWjZPDKFWacmcPbwHE+etZ8N27z/JOddyxSwxSMqQNFXSbEnzJP2qimV+KGm+pDmS3paUF6t4onHBiDxKyrz/JOdcyxbLK4Yi4FgzGwIMBcZKGlFpmZnAcDMbDDwP/CGG8dSqvP+kp6Z4/0nOuZYrZonBAoXhZGo4WKVl3jWzXeHkJ0CPWMUTrfL+k/4za228Q3HOubiIaRuDpGRJs4CNwJtmNqWGxS8DXqtmO1dKmiZpWn5+fgwi/coxAzozuEcWf5q4iF17S2K6L+ecS0QxTQxmVmpmQwmuBA6XdHBVy0m6ABgO/LGa7TxkZsPNbHhOTk7M4gVIShL/M24g67fv4cH3l8Z0X845l4ga5a4kM9sGvAuMrTxP0vHAz4FTzKyoMeKpzWG9OvDtwV158IMlrCvYHe9wnHOuUcXyrqQcSdnheCvgBGBhpWUOAR4kSAobYxVLffx07AGUGfzh9UXxDsU55xpVLK8YugLvSpoDfErQxvCKpF9LOiVc5o9AW+A5SbMkTYhhPHWS26E1lx/VmxdnrmHWqm3xDsc55xqNmlrfQMOHD7dp06Y1yr4Ki0oY88f3yOvYmuevHomkRtmvc841NEnTzWx4NMv6k881aJuewo9O7M/0FVt5Zc66eIfjnHONwhNDLc48NJeBXdvx+9cWsqe4NN7hOOdczNWaGCQdGU1Zc5Uc3r66ZttuHvlwWbzDcc65mIvmiuGeKMuarZF9O3LiQV24793FbPQO9pxzzVy1iUHSSEk3AzlhZ3flw+1AcqNFmCB+dtKBFJeW8ac3/PZV51zzVtMVQxrBraQpQGbEsB04M/ahJZa8jm343pG9eW76aj5bUxDvcJxzLmZqvV1VUp6ZrZDUOqLDu7hpzNtVK9u+p5hj/vge+3duy7+vHOG3rzrnmoyGvl21m6T5hE8tSxoi6f59CbCpapeRyk0n9GfKsi1MnLc+3uE451xMRJMY7gZOBDYDmNlsYHQMY0po5x6Wy4Aumfz2vwspKvHbV51zzU9UzzGY2apKRS32EzElOYlfjDuQlVt28ehHy+MdjnPONbhoEsMqSd8ATFKqpFuABTGOK6GN6pfDcQd05p53FpO/IyE6hHXOuQYTTWK4GrgW6A6sJXhN57UxjKlJ+Nm3D6SopJTT7/+IyUs2xzsc55xrMLUmBjPbZGbnm1kXM8sxswvMrMV/EvbNacvTV4wgOUmc9/dPuH3CPHbvbbE1bM65ZqSmB9yukNQvHJekf0gqkDRH0rDGCzFxDe/VgdduGMXFI/N49OPlnPTXSUxfsSXeYTnn3D6p6YrhBmB5OH4eMAToA/wQ+Etsw2o6Wqel8KtTD+apK45gb0kZZz0wmd/9d4F3uOeca7JqSgwlZlYcjo8DHjezzWb2FtAm9qE1Ld/o24mJN43mnMN68uAHSzn5ng+Zs3pbvMNyzrk6qykxlEnqKikDOA54K2Jeq9iG1TS1TU/hd2cM4rFLD2fHnhJOv/9j7nxjEXtLyuIdmnPORa2mxHAbMI2gOmmCmc0DkHQ0sDT2oTVdR/fPYeJNozltaHfueWcxp9z7IV9s2BHvsJxzLio19pUkKQXINLOtEWVtwvUKGyG+r4lnX0n18eb8Ddw6fg4pSUlM+MGRdG6XEe+QnHMtUIP1lWRmJZFJISzbGa+k0BSdMLAL/7rsCLbvKeaKx6d5o7RzLuH5qz0bwYFd23H3OUOZs6aAHz0/h9p6tHXOuXjyxNBIvnnQfvz4xAN4efZa7nlncbzDcc65akXzzue3oylztbv66D6cMaw7d735Oa/NXRfvcJxzrkop1c0Ib1NtDXSS1B4ofytNO4J+k1wdSeK3pw9i+aad3PTsLHI7tObg7lnxDss55yqo6YrhKmA6cED4s3z4D3Bv7ENrnjJSk3nwwuF0bJPO5Y9NY+P2PfEOyTnnKqg2MZjZX8ysN3CLmfUxs97hMMTMPDHsg5zMdP5+0XC/U8k5l5CiaXxeLykTQNIvJI33TvT23cBu7fjzOUOZvdrvVHLOJZZoEsP/mNkOSUcBxwOPAH+LbVgtw4kH7cePxw7g5dlrudfvVHLOJYhoEkN5Pce3gYfM7FUgLXYhtSzXHN2XMw7pzp1+p5JzLkFEkxjWSHoQOAf4r6T0aNaTlCFpqqTZkuZJ+lUVy6RLekbSYklTJPWq8xE0cZL47RmDOKRnNjc9O4vP1hTEOyTnXAsXTWI4G5gInGhm24AOwI+iWK8IONbMhhC8DnSspBGVlrkM2Gpm+wN/Bu6IMu5mJSM1mYcuHE6H1mn88NlZlJV5e4NzLn6iebXnLmAjcFRYVAJ8EcV6FtGnUmo4VP7EOxV4LBx/HjhOkmiBcjLT+fHYA/h8QyFvL9wY73Cccy1YNFVCvwR+AtwaFqUCT0SzcUnJkmYRJJY3zWxKpUW6A6sg6LAPKAA6VrGdKyVNkzQtPz8/ml03SeMGd6VH+1bc/95iv0vJORc30VQlnQ6cAuwEMLO1QGY0GzezUjMbCvQADpd0cH2CNLOHzGy4mQ3PycmpzyaahJTkJK4a3YeZK7fxyVJ/d7RzLj6iSQx7Lfj6avDl+xjqJGybeBcYW2nWGiA33G4KkAVsruv2m5OzhufSqW0af3t/SbxDcc61UNUmBkm/DUefDe9KypZ0BcErPv9e24Yl5UjKDsdbAScACystNgG4OBw/E3jHWngdSkZqMpce1ZsPPs/3O5Scc3FR0xXDWAAz+xNBw/ALwADgNjO7J4ptdwXelTQH+JSgjeEVSb+WdEq4zCNAR0mLgR8CP63ncTQrF4zIIzM9hb+951cNzrnGV23vqkByRK+q5R3oASCpg5nVWAluZnOAQ6oovy1ifA9wVl2Dbu7aZaRywcg8Hnh/CUvzC+mT0zbeITnnWpCarhgie1WdFjGUT7sYuvTI3qQlJ/HQB0vjHYpzroWpKTHMj+hVtU/E0NvM+jRahC1UTmY6Zw/P5YUZq1lf4F1zO+caj7/aM4FdOboPZQYPT/KrBudc46kpMfyl0aJwVcrt0JqTB3flqakr2bpzb7zDcc61EDW9qOfRRozDVeOaMfuza28pj09eEe9QnHMthFclJbgB+2Vy/IGdefTjZezaWxLvcJxzLYAnhibgmjH7s3VXMU9PXRXvUJxzLUBNzzEAwRPMwBVAr8jlzezS2IXlIh2a157De3fg4UlLuXBEHmkpns+dc7ETzSfMfwj6MHoLeDVicI3o+2P6sq5gDy/NWhPvUJxzzVytVwxAazP7ScwjcTU6un8OB3VrxwPvL+E7w3qQnNQiX1vhnGsE0VwxvCLppJhH4mokiWvG9GVp/k7emLc+3uE455qxaBLDDQTJYY+kHeGwPdaBua/71sFd6dWxNfe/t8Rf5OOci5loXu2ZaWZJZpYRjmeaWbvGCM5VlJwkrjq6L3PXFPDR4hb92grnXAxFdXuLpFMk/SkcxsU6KFe9M4Z1p0u7dO54fSFrt+2OdzjOuWYomnc+/56gOml+ONwg6XexDsxVLT0lmV98eyCfb9jBcXe+z33vLqaopDTeYTnnmhHVVlcdvmhnqJmVhdPJwEwzG9wI8X3N8OHDbdo07/V71ZZd/N+r85k4bwO9Orbml6ccxDEDOsc7LOdcgpI03cyGR7NstE9KZUeMZ9U5Itfgcju05sELh/PYpYeTJPG9f37KFY9PY9WWXfEOzTnXxEWTGH4HzJT0qKTHCF7U85vYhuWidXT/HF6/cTQ//dYBfLR4E8fd9T5/fvNz9hR79ZJzrn5qrUoCkNQVOCycnGpmcbuR3quSqre+YA+//e8CJsxeS4/2rbht3EBOGNgFyR+Gc66lq0tVUlSJIZF4Yqjd5CWbuX3CPBZt2MHR/XO4/ZSD6N2pTbzDcs7FUSzaGFwTMrJvR165/ij+Z9xAZqzYyol//oA/TVzE7r1eveScq50nhmYqNTmJy47qzdu3HM24wV25993FHH/X+7z+2Xp/ato5V6NoH3BrL+kgSX0keTJpQjpnZnDXOUN55soRZGakcPUT07nkn5+ybNPOeIfmnEtQ1bYxSMoCrgXOA9KAfCAD6AJ8AtxvZu82Upxf8jaG+ispLePxySu4683P2VtSxlVH9+H7Y/anVVpyvENzzsVYgzQ+S3oTeBx42cy2VZp3KHAhMNfMHtm3cOvGE8O+27h9D797bSEvzlxD9+xW3HbyQL7pdy8516z5XUkuKlOWbua2/wR3Lx0zIIe/nncImRmp8Q7LORcDDXXFMKyG9YqAlWa2ox7x7RNPDA2ruLSMxz5ezu9eW8jYg/bj3u8e4lcOzjVDdUkMNb3B7c5a1usp6T4z+0OdonMJJTU5ictH9aG41Ljj9YUcPrkDF3+jV7zDcs7FUbWJwcyOqWlFSenATMATQzNw1eg+TFu+hf97dT5DcrMZmpsd75Ccc3FS7a2nko6qZd104Koa1s+V9K6k+ZLmSbqhimWyJL0saXa4zPeiD901pKQkcefZQ+icmcG1T85g26698Q7JORcnNT2T8B1JH0u6TdK3JR0uabSkSyX9C3gF2FPD+iXAzWY2EBgBXCtpYKVlrgXmm9kQYAxwp6S0+h+O2xfZrdO47/xhbNyxh5ufnU1ZWdO6McE51zCqTQxmdhMwDlgHnAX8L/BDoB/woJmNNrNPa1h/nZnNCMd3AAuA7pUXAzIVtHa2BbYQJBQXJ0Nzs/nFtwfy9sKNPPjB0niH45yLg5oanzGzLcDfw6HeJPUCDgGmVJp1LzABWAtkAueUvxCo0vpXAlcC9OzZc19CcVG4aGQeU5dt4U9vLGJYz2yO6NMx3iE55xpRzLu3kNQWeAG40cy2V5p9IjAL6AYMBe6V1K7yNszsITMbbmbDc3JyYhyxk8TvvzOInh1ac93TM8nfURTvkJxzjSimiUFSKkFSeNLMxlexyPeA8RZYDCwDDohlTC46mRmp3H/+MAp2F3PDv2dS6u0NzrUYMUsMYbvBI8ACM7urmsVWAseFy3cBBgBesZ0gDuzajv899WA+XrKZv7z9RbzDcc41khrbGAAktQZuBnqa2RWS+gEDzOyVWlY9krA/JUmzwrKfAT0BzOwBggbtRyXNBQT8xMw21etIXEycfVguU5dv4Z53vmB4XntG9/eqPOeau1r7SpL0DMF7ni8ys4PDRPGxmQ1thPi+xrvEaHy795Zy2n0fkV9YxKvXH0XXrFbxDsk5V0cN/Qa3vmG3F8UAZraL4Nu9ayFapSVz/wXDKCou5QdPzaS49Gs3jjnnmpFoEsNeSa0InjlAUl+CTvRcC9I3py2//85gpq/Yyh2vLYx3OM65GKq1jQH4JfA6kCvpSYK2g0tiGZRLTCcP6cany7fw8IfLGN6rA2MP3i/eITnnYqDWxGBmb0qaQdCthYAbvIG45fr5tw9k9qpt/Oi52RzYNZO8jm3iHZJzroHVWpUk6XSgxMxeDe9EKpF0WswjcwkpPSWZe787jKQkcc0TM9hTXBrvkJxzDSyaNoZfmllB+UT4ms9fxiwil/ByO7Tmz+cMYf667fzq5XnxDsc518CiSQxVLRNN24Rrxo49oAvXjOnL01NXMX7G6niH45xrQNEkhmmS7pLUNxzuIniuwbVwN5/QnyN6d+DnL37GovWN/pZX51yMRJMYrgP2As+EQxHBexRcC5eSnMQ95x1Cm/QUrnlyOoVF3mO6c81BrYnBzHaa2U/Lezc1s1vNbGdjBOcSX+d2Gdxz3iEs37STW8fPpbYn6Z1ziS+au5L6S3pI0huS3ikfGiM41zSM7NuRm785gJdnr+WJT1bEOxzn3D6KphH5OeAB4GHA7010Vbrm6L5MW76FX78yn8E9shmSmx3vkJxz9RRNG0OJmf3NzKaa2fTyIeaRuSYlKUncdfZQOmdm8P0nZ7Bt1954h+Scq6doEsPLkr4vqaukDuVDzCNzTU77Nmncd/4wNu7Yw83PzqbMX+7jXJMUTbfby6ooNjPrE5uQaubdbie+xz5ezi8nzKN961RyMtPp1Db9y5/BkEanzHRywvLOmekE73VyzsVKXbrdjqavpN77HpJrSS4amUd6ShKfrS0gf0cRmwr3MnPlNjYVFrFr79ebqfp0asOlR/XmO8N60CotOQ4RO+ci1XrFACDpYGAgkFFeZmaPxzCuavkVQ9O2s6iETYVFbCosIn/HXtYV7ObFmWuYs7qA9q1TuWBEHheOzKNzZkbtG3PORa0uVwzRVCX9EhhDkBj+C3wL+NDMztzHOOvFE0PzY2ZMXbaFv09axtsLN5CalMSpQ7tx+ag+DNgvM97hOdcsNGhVEnAmMASYaWbfk9QFeGJfAnQukiSO6NORI/p0ZGl+If/4aBnPT1/Nc9NXM7p/Dpcf1ZtR/Tp5O4RzjSSau5J2m1kZQXfb7YCNQG5sw3ItVZ+ctvzfaYOY/NPjuOWb/VmwbjsX/WMqY++exLuLNsY7POdahGg70csG/k7Qed4MYHIsg3KufZs0fnBsPz78yTH88czBFJeVcc0T01mSXxjv0Jxr9qJqfP5yYakX0M7M5sQsolp4G0PLtHH7Hr559wfkdWzDC1ePJCU5mu80zrlydWljiOq/S9JgSacAw4D9JZ2xLwE6V1ed22Xwf6cdzOxV27j/vSXxDse5Zq3WxmdJ/wAGA/OAsrDYgPExjMu5rxk3uBtvzNvAX9/+gmMGdGZQj6x4h+RcsxTNXUkjzGxgzCNxLgr/e+rBTFm2mZuencUr1x1FRqo/EOdcQ4umKmmyJE8MLiFktU7lj2cOYfHGQv40cVG8w3GuWYomMTxOkBwWSZojaa6kuDU+Oze6fw4XjsjjkY+WMXnJ5niH41yzE01ieAS4EBgLnAyMC386Fze3nnQAeR1ac8tzs9mxpzje4TjXrESTGPLNbIKZLTOzFeVDzCNzrgat01K465yhrCvYza9fnh/vcJxrVqJJDDMlPSXpPElnlA+1rSQpV9K7kuZLmifphmqWGyNpVrjM+3U+AtdiDevZnu+P2Z/npq/mjXnr4x2Oc81GNHcltQKKgG9GlEVzu2oJcLOZzZCUCUyX9KaZffn1Lnyi+n5grJmtlNS5TtG7Fu/64/rxzsKN3Dp+LsPy2tOpbXq8Q3KuyavxikFSMrDZzL5Xabi0tg2b2TozmxGO7wAWAN0rLfZdYLyZrQyX885wXJ2kpSTx53OGsmNPCT9/cS51eZLfOVe1GhODmZUCR+7rTsKuNA4BplSa1R9oL+k9SdMlXVTN+ldKmiZpWn5+/r6G45qZAftlcsuJ/Zk4bwMvzFgT73Cca/KiqUqaJWkC8Byws7zQzKJ68llSW+AF4EYz217F/g8FjiOospos6RMz+zxyITN7CHgIgr6Sotmva1kuO6oPb83fyK8mzGNEnw70aN863iE512RF0/icAWwGjiW4TbX8ltVaSUolSApPVpNIVgMTzWynmW0CPiB494NzdZKcJO48ewhlZlz9xHSmLN3s1UrO1VOdelet04aDt6o8BmwxsxurWeZA4F7gRCANmAqca2afVbdd713V1eS1uev42Ytz2bqrmCG52Vw1ug8nHrQfyUn+kh/XsjXoG9wk9QDu4au2hknADWa2upZVjyR4MG6upFlh2c+AngBm9oCZLZD0OjCHoIO+h2tKCs7V5luDujJmQGeen7Gahyct5ftPzqBnh9ZcPqo3Zx2aS6s071vJudpE887nN4GngH+FRRcA55vZCTGOrUp+xeCiVVpmvDFvPQ9+sJRZq7bRvnUqF47sxcUj8+jot7W6FqYuVwzRJIZZZja0trLG4onB1ZWZMW3FVh58fylvLdhAekoS3zm0BxeP7EXfnDb+0h/XIjRoVRKwWdIFwNPh9HkEjdHONQmSOKxXBw7r1YHFGwt5eNJSnp+2mqemrCQ5SXTNyiC3fWtyO7SiZ4fW5HZoTY9wOqdtOkFzmXMtRzRXDHkEbQwjCZ54/hi4vvyhtMbmVwyuIWzcsYf3FuazcssuVm3dxaotu1i1dTf5O4oqLJeRmkSfTm35wbH7862D9/Mk4ZqsBrlikHSHmf0EONzMTmmw6JxLAJ0zMzj7sNyvle8pLmX11l2s2rI7SBpbdjHpi018/8kZHN0/h1+fehB5HdvEIWLnGk+1VwyS5hK80nO6mQ1r1Khq4FcMrrGVlJbx2OQV3PXGIkrKjGuP2Z+rju5Deorf4eSajrpcMdTU6vY6sBUYLGm7pB2RPxskUueagJTkJC47qjdv3zyG4wd24a43P2fs3ZP48ItN8Q7NuZioNjGY2Y/MLBt41czamVlm5M/GC9G5xLBfVgb3fXcYj196OGbGBY9M4bqnZ7Jh+554h+Zcg4qmd1VPAs5FGN0/h9dvHM2Nx/dj4rz1HHfn+/zzo2WUlJbFOzTnGkQ0vauWScpqpHicaxIyUpO58fj+vHHjaIbltedXL8/n1Ps+YvmmnbWv7FyCi+bJnkKCbi0ekfTX8iHWgTnXFPTq1IbHvncY9313GGu37eb8h6ewrmB3vMNybp9EkxjGA/9D0PPp9IjBOUfwAN23B3fl8UuPoGB3MRc8PIXNhUW1r+hcgqo1MZjZY8CzwCdm9lj5EPvQnGtaBvXI4pGLh7N6624u+sdUtu8pjndIztVLrYlB0snALILbV5E0NHxxj3OukiP6dOSBCw5l0fodXPbop+zeWxrvkJyrs2iqkm4HDge2AZjZLKBPzCJyrok75oDO3H3uUKat2MrVT0xnb4nfreSalmgSQ7GZFVQq879052owbnA3fnf6IN7/PJ+bnplFaZm/Tc41HdH0rjpP0neBZEn9gOsJOtJzztXg3MN7smNPCb/57wLapqfw++8M8k74XJMQzRXDdcBBQBHBC3sKgBtjGJNzzcYVo/tw/bH788y0Vfzm1QX+HmrXJNTUu2oGcDWwPzAXGGlmJY0VmHPNxU0n9Gf7nhIe/nAZWa1Sue64flUuV1JaxhcbC5m7poDP1hQwb+12UpPFgC6Z9N8vkwFdMunXJZOsVqmNfASupampKukxoJjgHc/fAg7ErxScqzNJ3DZuINv3FHPnm5+TmZHCBSPyvkwCc1cXMHdNAQvWbacobKhuk5bMQd2yKCop4/npq9kZcXfTfu0ywkTRlv5dMhmwXzB4b6+uodTY7baZDQrHU4CpidD9tne77ZqqktIyrn1qBhPnBa8XLU8CbdNTGNitHYO6ZzG4RxYHd8+id8c2JCUF7RFmxpptu/l8ww4WrS8Mf+5gcX7hl3c89WjfivvPH8bgHtnxOjyX4Brknc+SZkQmgsrT8eKJwTVlRSWl/PnNLygpLWNQFUmgLkpKy1ixZRefrSngjtcWsqlwL788ZSDfPbynN3K7r2moxFAKlPcIJqAVsCsct3h1ve2Jwbmv27JzLzc+M4sPPs/n9EO685vTD6Z1WjQ3HbqWokFe1GNmyeH7F8rfwZDi72NwLjF1aJPGo5ccxg9P6M9Ls9Zw2n0fsSS/MN5huSYqmttVnXNNQFKSuP64fjx+6eFsKtzLKfd8yCtz1sY7LNcEeWJwrpkZ1S+HV68/igH7ZfKDp2Zy+4R53i2HqxNPDM41Q12zWvHvK0dy6ZG9efTj5Zz94GTWbPP3RLjoeGJwrplKS0nitpMHcv/5w1i8sZBxf53EW/M3+NPXrlaeGJxr5k4a1JUJPziSLu0yuPzxaYy9exJPTlnBrr3ekYGrWrW3qyYqv13VufrZU1zKhFlrefTj5cxft53MjBTOHp7LhSPy6NWpTbzDczHWIM8xNEAQucDjQBfAgIfM7C/VLHsYMBk418yer2m7nhic2zdmxoyVW3ns4xX8d+46SsqMMQNyuHhkL47un1Ovh+1c4kuUxNAV6GpmMyRlErwn+jQzm19puWTgTWAP8A9PDM41no3b9/DU1JU8OWUl+TuKyOvYmgtH5HHKkG4UlZSxqbCIzYV72bJzL5t2BuObC4vYvHMvmwv3snNvCf06t2VQ92wG98hiUI8sOrVNj/dhuSokRGL42o6k/wD3mtmblcpvJOis7zDgFU8MzjW+vSVlTJy3nsc+Xs60FVurXa51WjId26bRsU06HdukkZGazKINO1iSX0j5R0m3rAwG9chicI8wWXTPIrt1WiMdiatOXRJDozwzL6kXcAgwpVJ5d+B04BiCxFDd+lcCVwL07NkzZnE611KlpSRx8pBunDykG5+tKeCTpZvJapVKp7bpdGybRoc2QTJolVZ1D66FRSXMWxP0Ejsn7C124rwNX87P69ia64/txxnDuns/Tk1AzK8YJLUF3gd+Y2bjK817DrjTzD6R9Ch+xeBcs1Gwu5h5awqYs6aAifPWM3PlNsYN7spvTh/k75SIg4SpSpKUCrwCTDSzu6qYv4ygUz6ATgSd9F1pZi9Vt01PDM41PaVlxgPvL+GuNz9nv3YZ/PmcoRzeu0O8w2pRGqQTvQYIQsAjwIKqkgKAmfU2s15m1gt4Hvh+TUnBOdc0JSeJa4/Znxeu+QYpyeLchyZz5xuLKC71rjoSUSwfcDsSuBA4VtKscDhJ0tWSro7hfp1zCWpobjavXj+KM4b14J53FnPWA5NZsXln7Su6RuUPuDnn4uKVOWu5dfxcysqMX596sDdMx1jC3ZXknHOVjRvcjUN6tuemZ2Zx83OzeXfRxno3TJeVGXtLyygqKWNvSVkwXlzK3tIyOmdm0KGN3y5bF54YnHNx0z27FU9fMeLLhumZK7cxql8nikrKKCoppai47KvxkrJwOhwv+erDv7i0+pqP5CRxzIAcvjOsB8ce2Jn0lKpvuXVf8cTgnIur8obpI/fvxM9fnMs7CzeSnppEekoy6SlJZKQGP9umpwRlqUmkJSd9uUxaylfTaclJpKckkZby1by5awoYP2M1by3YSHbrVE4d0o0zD83l4O7tvOqqGt7G4Jxr9krLjA8Xb+L56at5Y956ikrK6N+lLWce2oPThnanc7uMeIcYcwnzHEMseGJwzu2Lgt3FvDpnHc9PX8WMldtIEhzdP4ezhudywsAupCY3z7cReGJwzrkoLMkvZPyM1YyfsYZ1BXvonJnOuYf35LzDc+ma1Sre4TUoTwzOOVcHpWXGB5/n8/jk5bz3eT5JEicc2IULR+bxjb4dm0VbhN+u6pxzdZCcJI45oDPHHNCZlZt38eTUFTz76Spen7eePjltuOCIPL5zaI9ab6XdvbeUtQW7WbttN1t3FdOnUxv6d8kkLaVpVU/5FYNzzlVhT3Ep/527jic+WcGMldvISE3itKHdOWlQV7bvKWbttt2s3bYn+FkQjG/Zufdr20lLTmLAfpkc3L0dB3ULuiEfsF8mGamNe9usVyU551wD+mxNAU9OWcFLM9eyu7j0y/K26Sl0z25F1+wMumW3ont2K7plZ9AtqxVZrVNZvLGQuWsKmLdmO3PXFFCwuxiAlCTRr0smB3drx6AeWYw9eD86Z8b2zihPDM45FwMFu4uZs3obOZnpdMtuRbuM6J/SNjNWb93NZ2sK+GxtAXPXbOezNQVs2bmXtOQkTh3ajctH9WHAfpkxid0Tg3PONQFmxpL8nTz28XKem76KPcVljOrXiStG9WFUv04N2ujticE555qYrTv38tTUlTz68XLydxQxoEsml43qzalDuzVINx6eGJxzrokqKillwqy1PPLhMhau30FOZjoXj8zj/CPyaL8PnQF6YnDOuSbOLOjG4+FJy3j/83wyUpO45ZsDuHxUn3ptz59jcM65Jk4So/rlMKpfDp9v2MEjk5bRPbtxnsb2xOCccwmuf5dM7jhzcKPtr2k9jueccy7mPDE455yrwBODc865CjwxOOecq8ATg3POuQo8MTjnnKvAE4NzzrkKPDE455yroMl1iSEpH1hRqbgTsCkO4dRFoseY6PGBx9gQEj0+SPwYEz0+qDrGPDPLiWblJpcYqiJpWrR9gMRLoseY6PGBx9gQEj0+SPwYEz0+2PcYvSrJOedcBZ4YnHPOVdBcEsND8Q4gCokeY6LHBx5jQ0j0+CDxY0z0+GAfY2wWbQzOOecaTnO5YnDOOddAPDE455yroMknBkljJS2StFjSTxMgnlxJ70qaL2mepBvC8g6S3pT0RfizfQLEmixppqRXwunekqaE5/IZSfV/wey+x5Yt6XlJCyUtkDQy0c6hpJvC3/Fnkp6WlBHvcyjpH5I2SvosoqzK86bAX8NY50gaFqf4/hj+nudIelFSdsS8W8P4Fkk6MdbxVRdjxLybJZmkTuF0o5/DmmKUdF14LudJ+kNEed3Oo5k12QFIBpYAfYA0YDYwMM4xdQWGheOZwOfAQOAPwE/D8p8CdyTA+fsh8BTwSjj9LHBuOP4AcE0cY3sMuDwcTwOyE+kcAt2BZUCriHN3SbzPITAaGAZ8FlFW5XkDTgJeAwSMAKbEKb5vAinh+B0R8Q0M/6fTgd7h/3pyPGIMy3OBiQQP2HaK1zms4TweA7wFpIfTnet7HhvtDzZGJ2ckMDFi+lbg1njHVSnG/wAnAIuArmFZV2BRnOPqAbwNHAu8Ev5hb4r4B61wbhs5tqzwQ1eVyhPmHIaJYRXQgeAVua8AJybCOQR6VfrAqPK8AQ8C51W1XGPGV2ne6cCT4XiF/+fwQ3lkPM5hWPY8MARYHpEY4nIOq/k9PwscX8VydT6PTb0qqfyfs9zqsCwhSOoFHAJMAbqY2bpw1nqgS7ziCt0N/BgoC6c7AtvMrCScjue57A3kA/8Mq7oeltSGBDqHZrYG+BOwElgHFADTSZxzGKm685aI/z+XEnwDhwSKT9KpwBozm11pVsLECPQHRoVVme9LOiwsr3OMTT0xJCxJbYEXgBvNbHvkPAvSdtzuE5Y0DthoZtPjFUMtUgguk/9mZocAOwmqQL6UAOewPXAqQRLrBrQBxsYrnmjF+7zVRNLPgRLgyXjHEklSa+BnwG3xjqUWKQRXsCOAHwHPSlJ9NtTUE8Magnq/cj3CsriSlEqQFJ40s/Fh8QZJXcP5XYGN8YoPOBI4RdJy4N8E1Ul/AbIlpYTLxPNcrgZWm9mUcPp5gkSRSOfweGCZmeWbWTEwnuC8Jso5jFTdeUuY/x9JlwDjgPPD5AWJE19fgi8As8P/mR7ADEn7kTgxQvB/M94CUwlqAzpRjxibemL4FOgX3gmSBpwLTIhnQGGGfgRYYGZ3RcyaAFwcjl9M0PYQF2Z2q5n1MLNeBOfsHTM7H3gXODNcLG4xmtl6YJWkAWHRccB8EugcElQhjZDUOvydl8eYEOewkurO2wTgovDOmhFAQUSVU6ORNJagWvMUM9sVMWsCcK6kdEm9gX7A1MaOz8zmmllnM+sV/s+sJrjBZD0Jcg5DLxE0QCOpP8FNG5uoz3lsjEaSGDfAnERw588S4OcJEM9RBJfqc4BZ4XASQR3+28AXBHcOdIh3rGG8Y/jqrqQ+4R/MYuA5wrsb4hTXUGBaeB5fAton2jkEfgUsBD4D/kVw10dczyHwNEGbRzHBB9hl1Z03ghsO7gv/d+YCw+MU32KCOvDy/5cHIpb/eRjfIuBb8TqHleYv56vG50Y/hzWcxzTgifDvcQZwbH3Po3eJ4ZxzroKmXpXknHOugXlicM45V4EnBueccxV4YnDOOVeBJwbnnHMVeGJoBsLeHu+MmL5F0u0NtO1HJZ1Z+5L7vJ+zFPSi+m5E2SBJs8Jhi6Rl4fhbUW7zFNXS466kbpKe39f4w21dIuneeq77s4aIoSFJulHSRTXMHyPpG40ZU8S+L5GUH/H3cXlYniPp9XjE1Jx4YmgeioAzyrsCThQRTwBH4zLgCjM7przAggeLhprZUIKHdH4UTh8fzT7MbIKZ/b6mnZrZWjOLeeKLQkIlhvC8XkrQ+251xgBxSQyhZ8r/PszsYQAzywfWSToyjnE1eZ4YmocSgne83lR5RuVv/JIKw59jwo62/iNpqaTfSzpf0lRJcyX1jdjM8ZKmSfo87Gep/F0Of5T0adgP/VUR250kaQLBk8CV4zkv3P5nku4Iy24jeDDwEUl/rO1gJb0n6W5J04AbJJ0cdhw2U9JbkrqEy335DT48D3+V9HF4vGeG5b0U9mkfLj9e0usK3l0Q2Z/9ZeHxT5X099quDGrYX1dJH4Tfcj+TNErS74FWYdmT4XIvSZquoF/9KyN/f5J+I2m2pE8ijrWLgncZzA6Hb4TlF4Qxz5L0YPh7Sw7j+yz8XXzt74agm5QZFnYIKOl6Be8YmSPp3wo6iLwauCnc9qjw2/oL4d/Ep+UfzpJul/QvSZPD83pFbb/jffQScH6M99G8NcZTej7E/CnIQqAdwROZWcAtwO3hvEeBMyOXDX+OAbYRdMOcTtB3yq/CeTcAd0es/zrBl4h+BE9ZZgBXAr8Il0kneEq5d7jdnUDvKuLsRtCVRA5Bh1/vAKeF896jhqdGI48jXPb+iHnt+er95ZcDd4bjlwD3Rqz/XHgcA4HFYXkvwq6Lw+WXhucwg6Df/dww7uUEHZSlApPKt1spxmj2dzPhE/oE7xPJjPy9RGyr/OnkVgRPsnYMpw04ORz/Q8Tv4BmCDhvLt5sFHAi8DKSG5fcDFwGHAm9G7Cu7imP5FXBdxPRavurnPzv8eTtwS8QyTwFHheM9CbqFKV9udngsnQiecu5WxT4n8dXTz5FDVV1JX0Lw5O8cgr60ciPmdQfmxvv/sikPdbnUdwnMzLZLehy4Htgd5WqfWtivi6QlwBth+VzCPldCz5pZGfCFpKXAAQQvVxkccTWSRZA49gJTzWxZFfs7DHjPgst9wm/Howm+4dXVMxHjPYBnFHQQl0bwLoeqvBQex/zyb9pVeNvMCsL45gN5BB9m75vZlrD8OYIujmtT1f4+Bf6hoKPFl8xsVjXrXi/p9HA8l+DcbiY4v6+E5dMJ3vUBwTf8iwDMrBQokHQhQRL4VEEnm60IOtB7Gegj6R7gVb76vUfqCiyImJ4DPCnpJar/fR0PDNRXHXq2U9DLMMB/zGw3sFtBO9LhlbdjZqOq2W5VXgaeNrOi8Gr1MYJzAMExdqvDtlwlXpXUvNxNUFffJqKshPD3LCmJ4IOzXFHEeFnEdBlU+NJQud8UI+gj5jr7qo63t5mVf8Ds3JeDiFLkPu4h+KY+CLiK4Nt+VSKPt7ruiCOXKYV9+vL0tf2Z2QcEyXAN8KiqaNyVNIbgQ3akmQ0BZvLVMRVb+LU4ivgEPBbxOxpgZreb2VaCF868R1Ad9HAV6+6m4nn8NkGfQMMIEk1V+00CRkTsr7uZFYbzqvobqnzck/RVY3LkcHzlZc1ss5mVn9+HCRJguQyi/3LkquCJoRkJv9E+S5Acyi3nq3+aUwiqQurqLElJCtod+hB0xDURuCb85ouk/gpeplOTqcDRkjpJSgbOA96vRzyVZfFVN8IX17RgPX1KEHf78APxO/XdkKQ8YIOZ/Z3gA638HcHF5eeS4Hi2mtkuSQcQ9K9fm7eBa8J9JEvKCsvOlNQ5LO8gKU/BTQpJZvYC8IuIGCItAPYP10siqKp5F/hJGF9bYAfB62vLvQFcF3GsQyPmnargndgdCaobP628QzMbFZFUIoev3YUWXh2WO4WKVzf9CarfXD15VVLzcyfwg4jpvwP/kTSboK2gPt/mVxJ8qLcDrjazPZIeJqifn6Gg7iAfOK2mjZjZOgW3j75L8G32VTNriG6pbweek7SVoN2idwNs80tmtkbSbwnOwRaCHlUL6rm5McCPJBUTtA2VXzE8BMyRNIPgbqCrJS0gSMKfRLHdG4CHJF1GcCVxjZlNlvQL4I3ww70YuJbg2/Q/wzIIXv1Y2WsEPcZC0GbxRJhsBPzVzLZJehl4XsHbza4jqMa8T9Icgs+WDwiuSCCoinqXoFruf81sbRTHVJPrJZ1CcEW8haDNodwxBFVkrp68d1XnoiCprZkVhlcMLwL/MLMX4x1XLEl6EfixmX2xj9u5naBx/U8NEljt+/sAODWsMnP14FVJzkXndkmzCKoollG/BvOm5qcEjdBNhqQc4C5PCvvGrxicc85V4FcMzjnnKvDE4JxzrgJPDM455yrwxOCcc64CTwzOOecq+H9LSYIxZssJzwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wine_j,wine_count = k_fold(wine_fold,wine_vals,[[13,8,3]],0.1,0.001,1,5,get_j=True)\n",
    "plt.plot(wine_count,wine_j)\n",
    "plt.xlabel('Number of Training Instances (step = 5)')\n",
    "plt.ylabel('Performance (J) on Test Set')\n",
    "plt.title('Wine Data Performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:163\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     result \u001b[39m=\u001b[39m func(left, right)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:239\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    238\u001b[0m         \u001b[39m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m         \u001b[39mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:69\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     68\u001b[0m     _store_test_result(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m house_nn_arc \u001b[39m=\u001b[39m [[\u001b[39m13\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m3\u001b[39m],[\u001b[39m13\u001b[39m,\u001b[39m8\u001b[39m,\u001b[39m3\u001b[39m],[\u001b[39m13\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m8\u001b[39m,\u001b[39m3\u001b[39m],[\u001b[39m13\u001b[39m,\u001b[39m8\u001b[39m,\u001b[39m16\u001b[39m,\u001b[39m3\u001b[39m],[\u001b[39m13\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m8\u001b[39m,\u001b[39m3\u001b[39m],[\u001b[39m13\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m8\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m3\u001b[39m]]\n\u001b[0;32m      3\u001b[0m house_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdatasets/hw3_house_votes_84.csv\u001b[39m\u001b[39m'\u001b[39m,delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m norm_house_df \u001b[39m=\u001b[39m ((house_df\u001b[39m-\u001b[39;49mhouse_df\u001b[39m.\u001b[39;49mmin())\u001b[39m/\u001b[39m(house_df\u001b[39m.\u001b[39mmax()\u001b[39m-\u001b[39mhouse_df\u001b[39m.\u001b[39mmin()))\n\u001b[0;32m      5\u001b[0m \u001b[39m#insert column of ones to act as bias\u001b[39;00m\n\u001b[0;32m      6\u001b[0m norm_house_df\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\ops\\common.py:70\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     68\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\arraylike.py:108\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__sub__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__sub__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_arith_method(other, operator\u001b[39m.\u001b[39;49msub)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:6952\u001b[0m, in \u001b[0;36mDataFrame._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6948\u001b[0m other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mmaybe_prepare_scalar_for_op(other, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[axis],))\n\u001b[0;32m   6950\u001b[0m \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39malign_method_FRAME(\u001b[39mself\u001b[39m, other, axis, flex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, level\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 6952\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch_frame_op(other, op, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   6953\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(new_data)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:6991\u001b[0m, in \u001b[0;36mDataFrame._dispatch_frame_op\u001b[1;34m(self, right, func, axis)\u001b[0m\n\u001b[0;32m   6985\u001b[0m     \u001b[39m# TODO: The previous assertion `assert right._indexed_same(self)`\u001b[39;00m\n\u001b[0;32m   6986\u001b[0m     \u001b[39m#  fails in cases with empty columns reached via\u001b[39;00m\n\u001b[0;32m   6987\u001b[0m     \u001b[39m#  _frame_arith_method_with_reindex\u001b[39;00m\n\u001b[0;32m   6988\u001b[0m \n\u001b[0;32m   6989\u001b[0m     \u001b[39m# TODO operate_blockwise expects a manager of the same type\u001b[39;00m\n\u001b[0;32m   6990\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 6991\u001b[0m         bm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49moperate_blockwise(\n\u001b[0;32m   6992\u001b[0m             \u001b[39m# error: Argument 1 to \"operate_blockwise\" of \"ArrayManager\" has\u001b[39;49;00m\n\u001b[0;32m   6993\u001b[0m             \u001b[39m# incompatible type \"Union[ArrayManager, BlockManager]\"; expected\u001b[39;49;00m\n\u001b[0;32m   6994\u001b[0m             \u001b[39m# \"ArrayManager\"\u001b[39;49;00m\n\u001b[0;32m   6995\u001b[0m             \u001b[39m# error: Argument 1 to \"operate_blockwise\" of \"BlockManager\" has\u001b[39;49;00m\n\u001b[0;32m   6996\u001b[0m             \u001b[39m# incompatible type \"Union[ArrayManager, BlockManager]\"; expected\u001b[39;49;00m\n\u001b[0;32m   6997\u001b[0m             \u001b[39m# \"BlockManager\"\u001b[39;49;00m\n\u001b[0;32m   6998\u001b[0m             right\u001b[39m.\u001b[39;49m_mgr,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   6999\u001b[0m             array_op,\n\u001b[0;32m   7000\u001b[0m         )\n\u001b[0;32m   7001\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(bm)\n\u001b[0;32m   7003\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(right, Series) \u001b[39mand\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   7004\u001b[0m     \u001b[39m# axis=1 means we want to operate row-by-row\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1409\u001b[0m, in \u001b[0;36mBlockManager.operate_blockwise\u001b[1;34m(self, other, array_op)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moperate_blockwise\u001b[39m(\u001b[39mself\u001b[39m, other: BlockManager, array_op) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BlockManager:\n\u001b[0;32m   1406\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1407\u001b[0m \u001b[39m    Apply array_op blockwise with another (aligned) BlockManager.\u001b[39;00m\n\u001b[0;32m   1408\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1409\u001b[0m     \u001b[39mreturn\u001b[39;00m operate_blockwise(\u001b[39mself\u001b[39;49m, other, array_op)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\internals\\ops.py:63\u001b[0m, in \u001b[0;36moperate_blockwise\u001b[1;34m(left, right, array_op)\u001b[0m\n\u001b[0;32m     61\u001b[0m res_blks: \u001b[39mlist\u001b[39m[Block] \u001b[39m=\u001b[39m []\n\u001b[0;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m lvals, rvals, locs, left_ea, right_ea, rblk \u001b[39min\u001b[39;00m _iter_block_pairs(left, right):\n\u001b[1;32m---> 63\u001b[0m     res_values \u001b[39m=\u001b[39m array_op(lvals, rvals)\n\u001b[0;32m     64\u001b[0m     \u001b[39mif\u001b[39;00m left_ea \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m right_ea \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(res_values, \u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     65\u001b[0m         res_values \u001b[39m=\u001b[39m res_values\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:222\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[39m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     _bool_arith_check(op, left, right)\n\u001b[1;32m--> 222\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(left, right, op)\n\u001b[0;32m    224\u001b[0m \u001b[39mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:170\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cmp \u001b[39mand\u001b[39;00m (is_object_dtype(left\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(right)):\n\u001b[0;32m    166\u001b[0m         \u001b[39m# For object dtype, fallback to a masked operation (only operating\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[39m#  on the non-missing values)\u001b[39;00m\n\u001b[0;32m    168\u001b[0m         \u001b[39m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[39m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m         result \u001b[39m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[0;32m    171\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    172\u001b[0m         \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:108\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[39m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m--> 108\u001b[0m         result[mask] \u001b[39m=\u001b[39m op(xrav[mask], yrav[mask])\n\u001b[0;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "#list of neural net architectures\n",
    "house_nn_arc = [[13,4,3],[13,8,3],[13,4,8,3],[13,8,16,3],[13,2,4,8,3],[13,2,4,8,4,3]]\n",
    "house_df = pd.read_csv('datasets/hw3_house_votes_84.csv',delimiter='\\t')\n",
    "norm_house_df = ((house_df-house_df.min())/(house_df.max()-house_df.min()))\n",
    "#insert column of ones to act as bias\n",
    "norm_house_df.insert(0,'bias',1)\n",
    "\n",
    "#split data by class into k groups then combine into folds\n",
    "house_class_1 = norm_house_df.loc[norm_house_df['class'] == 0].sample(frac=1)\n",
    "house_class_1['class'] = [[1,0,0]]*len(house_class_1)\n",
    "hc1_split = np.array_split(house_class_1,k)\n",
    "house_class_2 = norm_house_df.loc[norm_house_df['class'] == 0.5].sample(frac=1)\n",
    "house_class_2['class'] = [[0,1,0]]*len(house_class_2)\n",
    "hc2_split = np.array_split(house_class_2,k)\n",
    "house_class_3 = norm_house_df.loc[norm_house_df['class'] == 1].sample(frac=1)\n",
    "house_class_3['class'] = [[0,0,1]]*len(house_class_3)\n",
    "hc3_split = np.array_split(house_class_3,k)\n",
    "house_vals = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "\n",
    "#list to hold folds\n",
    "house_fold = []\n",
    "for i in range(k):\n",
    "    this_fold = [hc1_split[i],hc2_split[i],hc3_split[i]]\n",
    "    house_fold.append(pd.concat(this_fold))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_res = k_fold(wine_fold,wine_vals,wine_nn_arc,0.1,0.001,1,5)\n",
    "\n",
    "arc_dict_h = defaultdict(list)\n",
    "\n",
    "for arc,perf in house_res.items():\n",
    "    avg_acc,avg_f1 = [0,0]\n",
    "    for res in perf:\n",
    "        avg_acc += res[0]\n",
    "        avg_f1 += res[1]\n",
    "    arc_dict['Architecture'].append(arc)\n",
    "    arc_dict['Accuracy'].append(avg_acc/10)\n",
    "    arc_dict['F1'].append(avg_f1/10)\n",
    "\n",
    "arc_table_h = pd.DataFrame(arc_dict_h)\n",
    "print(arc_table_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_j,wine_count = k_fold(wine_fold,wine_vals,[[13,8,3]],0.1,0.001,1,5,get_j=True)\n",
    "plt.plot(wine_count,wine_j)\n",
    "plt.xlabel('Number of Training Instances (step = 5)')\n",
    "plt.ylabel('Performance (J) on Test Set')\n",
    "plt.title('Wine Data Performance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Correctness Verification**\n",
    "\n",
    "Below I have included 2 functions: ```train_on_first()``` and ```train_on_sec()```\n",
    "\n",
    "These functions hard code the inputs and print the desired outputs to stdout. If the output is too large for your IDE, set the max lines of your output to 100. To run these functions, simply call them without arguement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS FOR INSTANCE 1\n",
      "activations: \n",
      "a1: [[1.   0.13]]\n",
      "a2: [1.        0.601807  0.5807858]\n",
      "a3: [0.79402743]\n",
      "\n",
      "prediction: [0.79402743]\n",
      "expected: [0.9]\n",
      "cost J: 0.36557477431084995\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [-0.01269739 -0.01548092]\n",
      "delta 3: [-0.10597257]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[-0.01269739 -0.00165066]\n",
      " [-0.01548092 -0.00201252]]\n",
      "theta 2: [-0.10597257 -0.06377504 -0.06154737]\n",
      "\n",
      "OUTPUTS FOR INSTANCE 2\n",
      "activations: \n",
      "a1: [[1.   0.42]]\n",
      "a2: [1.         0.60873549 0.59483749]\n",
      "a3: [0.79596607]\n",
      "\n",
      "prediction: [0.79596607]\n",
      "expected: [0.23]\n",
      "cost J: 1.2763768066887786\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [0.06739994 0.08184068]\n",
      "delta 3: [0.56596607]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[0.06739994 0.02830797]\n",
      " [0.08184068 0.03437309]]\n",
      "theta 2: [0.56596607 0.34452363 0.33665784]\n",
      "\n",
      "regularized gradients: \n",
      "theta 1: [[0.02735127 0.01332866]\n",
      " [0.03317988 0.01618028]]\n",
      "theta 2: [[0.22999675 0.1403743  0.13755523]]\n"
     ]
    }
   ],
   "source": [
    "#function to train on backprop_example1.txt\n",
    "#outputs to stdout, if full output cannot be viewed when calling this function, increase number of lines shown in output to 100\n",
    "def train_on_first():\n",
    "\ttrain_nn = NeuralNet([1,2,1],eps=0.001)\n",
    "\ttrain_nn.weights[0] = np.array([[0.40000,0.10000 ],[0.30000,0.20000 ]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.7],[0.5],[0.6]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\t#NOTE: X values are preprocessed to include bias term (1) as first element\n",
    "\tX = np.array([[1,0.13000],[1,0.42000]])\n",
    "\tY = np.array([[0.90000],[0.23000]])\n",
    "\ttrain_df = pd.DataFrame(data=train_set_1)\n",
    "\ttrain_df.insert(0,'bias',np.ones)\n",
    "\ttrain_nn.train(X,Y,2,for_exam=True)\n",
    "\n",
    "train_on_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS FOR INSTANCE 1\n",
      "activations: \n",
      "a1: [[1.   0.32 0.68]]\n",
      "a2: [1.         0.67699586 0.75384029 0.5881687  0.70566042]\n",
      "a3: [1.         0.87519469 0.89296181 0.81480444]\n",
      "a4: [0.83317658 0.84131543]\n",
      "\n",
      "prediction: [0.83317658 0.84131543]\n",
      "expected: [0.75 0.98]\n",
      "cost J: 0.7907366961135718\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [-0.00086743 -0.00133354 -0.00053312 -0.00070163]\n",
      "delta 3: [ 0.00638937 -0.00925379 -0.00778767]\n",
      "delta 4: [ 0.08317658 -0.13868457]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[-0.00086743 -0.00027758 -0.00058985]\n",
      " [-0.00133354 -0.00042673 -0.00090681]\n",
      " [-0.00053312 -0.0001706  -0.00036252]\n",
      " [-0.00070163 -0.00022452 -0.00047711]]\n",
      "theta 2: [[ 0.00638937 -0.00925379 -0.00778767]\n",
      " [ 0.00432557 -0.00626478 -0.00527222]\n",
      " [ 0.00481656 -0.00697588 -0.00587066]\n",
      " [ 0.00375802 -0.00544279 -0.00458046]\n",
      " [ 0.00450872 -0.00653003 -0.00549545]]\n",
      "theta 3: [[ 0.08317658 -0.13868457]\n",
      " [ 0.0727957  -0.121376  ]\n",
      " [ 0.07427351 -0.12384003]\n",
      " [ 0.06777264 -0.1130008 ]]\n",
      "\n",
      "OUTPUTS FOR INSTANCE 2\n",
      "activations: \n",
      "a1: [[1.   0.83 0.02]]\n",
      "a2: [1.         0.63471542 0.69291867 0.54391158 0.64659376]\n",
      "a3: [1.         0.86020091 0.88336451 0.79790763]\n",
      "a4: [0.82952703 0.83831889]\n",
      "\n",
      "prediction: [0.82952703 0.83831889]\n",
      "expected: [0.75 0.28]\n",
      "cost J: 1.9437823352945294\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [0.01694006 0.01465141 0.01998824 0.01622017]\n",
      "delta 3: [0.01503437 0.05808969 0.06891698]\n",
      "delta 4: [0.07952703 0.55831889]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[0.01694006 0.01406025 0.0003388 ]\n",
      " [0.01465141 0.01216067 0.00029303]\n",
      " [0.01998824 0.01659024 0.00039976]\n",
      " [0.01622017 0.01346274 0.0003244 ]]\n",
      "theta 2: [[0.01503437 0.05808969 0.06891698]\n",
      " [0.00954254 0.03687042 0.04374267]\n",
      " [0.01041759 0.04025143 0.04775386]\n",
      " [0.00817737 0.03159565 0.03748474]\n",
      " [0.00972113 0.03756043 0.04456129]]\n",
      "theta 3: [[0.07952703 0.55831889]\n",
      " [0.06840922 0.48026642]\n",
      " [0.07025135 0.4931991 ]\n",
      " [0.06345522 0.44548691]]\n",
      "\n",
      "regularized gradients: \n",
      "theta 1: [[0.00803632 0.02564134 0.04987447]\n",
      " [0.00665894 0.01836697 0.06719311]\n",
      " [0.00972756 0.03195982 0.05251862]\n",
      " [0.00775927 0.05036911 0.08492365]]\n",
      "theta 2: [[0.01071187 0.09068406 0.02511708 0.1259677  0.11586492]\n",
      " [0.02441795 0.06780282 0.04163777 0.05307643 0.1267652 ]\n",
      " [0.03056466 0.08923522 0.1209416  0.10270214 0.03078292]]\n",
      "theta 3: [[0.0813518  0.17935246 0.12476243 0.13186393]\n",
      " [0.20981716 0.19194521 0.30342954 0.25249305]]\n"
     ]
    }
   ],
   "source": [
    "#function to train on backprop_example2.txt\n",
    "#outputs to stdout, if full output cannot be viewed when calling this function, increase number of lines shown in output to 100\n",
    "def train_on_sec():\n",
    "\ttrain_nn = NeuralNet([2,4,3,2],eps=0.001,lamb=0.250)\n",
    "\ttrain_nn.weights[0] = np.array([[0.42000,0.15000,0.40000],[0.72000,0.10000,0.54000],[0.01000,0.19000,0.42000],[0.30000,0.35000,0.68000]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.21000,0.67000,0.14000,0.96000,0.87000],[0.87000,0.42000,0.20000,0.32000,0.89000],[0.03000,0.56000,0.80000,0.69000,0.09000]])\n",
    "\ttrain_nn.weights[2] = np.array([[0.04000,0.87000,0.42000,0.53000],[0.17000,0.10000,0.95000,0.69000]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\t#NOTE: X values are preprocessed to include bias term (1) as first element\n",
    "\tX = np.array([[1,0.32000,0.68000],[1,0.83000,0.02000]])\n",
    "\tY = np.array([[0.75000,0.98000],[0.75000,0.28000]])\n",
    "\ttrain_nn.train(X,Y,batch_size=2,for_exam=True)\n",
    "\n",
    "#train_on_first()\n",
    "train_on_sec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52fef9f371462526e6c16fbedf2b5b9bc32d90752958a415cf9672d41d7c1c70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
