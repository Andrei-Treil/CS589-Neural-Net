{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO: \\n    - Implement calculate_loss\\n    - Debug using sample data\\n    - Swag, nae nae, and finesse\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,nodes,lamb=0.0,alpha=0.1,eps=0.0):\n",
    "        '''\n",
    "        Constructor for neural net\n",
    "        nodes - list detailing number of nodes in each layer\n",
    "        lamb - regularization\n",
    "        alpha - learning rate\n",
    "        eps - cost function stopping condition\n",
    "        '''\n",
    "        self.nodes = nodes\n",
    "        self.lamb = lamb\n",
    "        self.alpha = alpha\n",
    "        self.weights = []\n",
    "        self.eps = eps\n",
    "        #initialize weights for each layer, include bias\n",
    "        for i in range(len(nodes)-1):\n",
    "            self.weights.append(np.random.normal(0,1,(nodes[i]+1,nodes[i+1])))\n",
    "    \n",
    "    def get_sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def deriv_sigmoid(self, x):\n",
    "        return x * (1-x)\n",
    "\n",
    "    def train(self, features, targs, for_exam=False):\n",
    "        prev_cost = -math.inf\n",
    "        gradients = [0]*len(self.weights)\n",
    "        num_inst = len(targs)\n",
    "        keep_learn = True\n",
    "        count = 1\n",
    "\n",
    "        while(keep_learn):\n",
    "            J = 0\n",
    "            for instance,target in zip(features,targs):\n",
    "                #iterate through layers, vectorize forward pass\n",
    "                activations = [np.atleast_2d(instance)]\n",
    "                for i in range(len(self.weights)-1):\n",
    "                    this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "                    activations.append(np.insert(this_a,0,1))\n",
    "                activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "                guess = activations[-1]\n",
    "\n",
    "                #accumulate sum loss\n",
    "                J += np.sum((-target).dot(math.log(guess)) - (1-target).dot(math.log(1-guess)))\n",
    "\n",
    "                #begin backwards propogation\n",
    "                error = guess - target\n",
    "                delta_inst = [error]\n",
    "\n",
    "                #get delta values for all weights on current instance\n",
    "                for i in range(len(self.weights)-1, 0, -1):\n",
    "                    this_del = (self.weights[i].T*(delta_inst[-1])) * self.deriv_sigmoid(activations[i])\n",
    "                    delta_inst.append(this_del[0][1:])\n",
    "\n",
    "                #reverse delta values\n",
    "                delta_inst = delta_inst[::-1]\n",
    "\n",
    "                #accumulate gradients\n",
    "                for i in range(len(self.weights)-1,-1,-1):\n",
    "                    gradients[i] += (delta_inst[i]*(activations[i].T)).T\n",
    "\n",
    "                #print for examples\n",
    "                if for_exam:\n",
    "                    print(f'OUTPUTS FOR INSTANCE {count}')\n",
    "                    print(f'activations: {activations}')\n",
    "                    print(f'prediction: {guess}')\n",
    "                    print(f'expected: {target}')\n",
    "                    print(f'delta for this instance: {delta_inst}')\n",
    "                    print(f'gradients for this instance: {gradients}')\n",
    "                    print()\n",
    "                    count += 1\n",
    "            \n",
    "            #regularize weights and update\n",
    "            for i in range(len(self.weights)-1,-1,-1):\n",
    "                P = self.lamb * (self.weights[i]).T\n",
    "                gradients[i] = gradients[i] + P\n",
    "                gradients[i] = gradients[i] / num_inst\n",
    "                learn_diff = self.alpha * gradients[i]\n",
    "                self.weights[i] = self.weights[i] - learn_diff.T\n",
    "\n",
    "            J /= num_inst\n",
    "            curr_s = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                curr_s += np.sum(self.weights[i][1:]**2)\n",
    "\n",
    "            #curr_s = np.sum(self.weights[1:]**2)\n",
    "            curr_s *= (self.lamb/(2*num_inst))\n",
    "            new_cost = J + curr_s\n",
    "\n",
    "            #if improvement in cost is less than epsilon, stop\n",
    "            if new_cost - prev_cost < self.eps:\n",
    "                keep_learn = False\n",
    "\n",
    "            prev_cost = new_cost\n",
    "\n",
    "            if for_exam:\n",
    "                print(f'regularized gradients: {gradients}')\n",
    "\n",
    "    #def calculate_loss(self,data,targets):\n",
    "        #predictions = aaaa\n",
    "\n",
    "    def predict(self,instance):\n",
    "        pred = [np.ones(len(instance)),instance]\n",
    "\n",
    "        for i in len(self.weights):\n",
    "            pred = self.sigmoid(np.dot(pred,self.weights[i]))\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    #def calc_cost(self,guess,target,)\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "    - Implement calculate_loss\n",
    "    - Debug using sample data\n",
    "    - Swag, nae nae, and finesse\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activations: [array([[1.  , 0.13]]), array([1.       , 0.601807 , 0.5807858]), array([0.79402743])]\n",
      "prediction: [0.79402743]\n",
      "expected: [0.79402743]\n",
      "delta for this instance: [array([-0.01269739, -0.01548092]), array([-0.10597257])]\n",
      "gradients for this instance: [array([[-0.01269739, -0.00165066],\n",
      "       [-0.01548092, -0.00201252]]), array([-0.10597257, -0.06377504, -0.06154737])]\n",
      "activations: [array([[1.  , 0.42]]), array([1.        , 0.60873549, 0.59483749]), array([0.79596607])]\n",
      "prediction: [0.79596607]\n",
      "expected: [0.79596607]\n",
      "delta for this instance: [array([0.06739994, 0.08184068]), array([0.56596607])]\n",
      "gradients for this instance: [array([[0.05470255, 0.02665731],\n",
      "       [0.06635976, 0.03236057]]), array([0.45999349, 0.28074859, 0.27511047])]\n",
      "regularized gradients: [array([[0.02735127, 0.01332866],\n",
      "       [0.03317988, 0.01618028]]), array([[0.22999675, 0.1403743 , 0.13755523]])]\n",
      "activations: [array([[1.  , 0.13]]), array([1.        , 0.60104796, 0.58041002]), array([0.78740425])]\n",
      "prediction: [0.78740425]\n",
      "expected: [0.78740425]\n",
      "delta for this instance: [array([-0.01312063, -0.01607536]), array([-0.11259575])]\n",
      "gradients for this instance: [array([[0.01423065, 0.01162297],\n",
      "       [0.01710452, 0.01409049]]), array([[0.117401  , 0.07269885, 0.07220353]])]\n",
      "activations: [array([[1.  , 0.42]]), array([1.        , 0.60775169, 0.59435239]), array([0.78931157])]\n",
      "prediction: [0.78931157]\n",
      "expected: [0.78931157]\n",
      "delta for this instance: [array([0.06479536, 0.0790543 ]), array([0.55931157])]\n",
      "gradients for this instance: [array([[0.079026  , 0.03883702],\n",
      "       [0.09615882, 0.04729329]]), array([[0.67671256, 0.4126214 , 0.4046317 ]])]\n",
      "regularized gradients: [array([[0.039513  , 0.01941851],\n",
      "       [0.04807941, 0.02364665]]), array([[0.33835628, 0.2063107 , 0.20231585]])]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training set\n",
    "\tTraining instance 1\n",
    "\t\tx: [0.13000]\n",
    "\t\ty: [0.90000]\n",
    "\tTraining instance 2\n",
    "\t\tx: [0.42000]\n",
    "\t\ty: [0.23000]\n",
    "        Initial Theta1 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
    "\t0.40000  0.10000  \n",
    "\t0.30000  0.20000  \n",
    "\n",
    "Initial Theta2 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
    "\t0.70000  0.50000  0.60000  \n",
    "'''\n",
    "def train_on_first():\n",
    "\ttrain_nn = NeuralNet([1,2,1],eps=0.001)\n",
    "\ttrain_nn.weights[0] = np.array([[0.40000,0.10000 ],[0.30000,0.20000 ]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.7],[0.5],[0.6]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\tX = np.array([[1,0.13000],[1,0.42000]])\n",
    "\tY = np.array([[0.90000],[0.23000]])\n",
    "\ttrain_df = pd.DataFrame(data=train_set_1)\n",
    "\ttrain_df.insert(0,'bias',np.ones)\n",
    "\ttrain_nn.train(X,Y,True)\n",
    "\n",
    "train_on_first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "#implement backprogagation algorithm in a way that allows you to specify how many layers and neurons you would like your neural network to have\n",
    "#https://pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/\n",
    "test = [1,2,3,4,5]\n",
    "out = []\n",
    "for i in range(len(test),0,-1):\n",
    "    out.append(i)\n",
    "#print(':3')\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
