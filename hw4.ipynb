{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,nodes,lamb=0.0,alpha=0.1,eps=0.0):\n",
    "        '''\n",
    "        Constructor for neural net\n",
    "        nodes - list detailing number of nodes in each layer\n",
    "        lamb - regularization\n",
    "        alpha - learning rate\n",
    "        eps - cost function stopping condition\n",
    "        '''\n",
    "        self.nodes = nodes\n",
    "        self.lamb = lamb\n",
    "        self.alpha = alpha\n",
    "        self.weights = []\n",
    "        self.eps = eps\n",
    "        #initialize weights for each layer, include bias\n",
    "        for i in range(len(nodes)-1):\n",
    "            self.weights.append(np.random.normal(0,1,(nodes[i]+1,nodes[i+1])))\n",
    "    \n",
    "    def get_sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def deriv_sigmoid(self, x):\n",
    "        return x * (1-x)\n",
    "\n",
    "    def train(self, features, targs, for_exam=False):\n",
    "        prev_cost = -math.inf\n",
    "        gradients = [0]*len(self.weights)\n",
    "        num_inst = len(targs)\n",
    "        keep_learn = True\n",
    "        count = 1\n",
    "\n",
    "        while(keep_learn):\n",
    "            J = 0\n",
    "            for instance,target in zip(features,targs):\n",
    "                #iterate through layers, vectorize forward pass\n",
    "                activations = [np.atleast_2d(instance)]\n",
    "                for i in range(len(self.weights)-1):\n",
    "                    this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "                    activations.append(np.insert(this_a,0,1))\n",
    "                try:\n",
    "                    activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "                except:\n",
    "                    activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1].T)))\n",
    "                guess = activations[-1]\n",
    "\n",
    "                #accumulate sum loss\n",
    "                cost = np.sum((-target).dot(np.log(guess)) - (1-target).dot(np.log(1-guess)))\n",
    "                J += cost\n",
    "\n",
    "                #begin backwards propogation\n",
    "                error = guess - target\n",
    "                delta_inst = [error]\n",
    "\n",
    "                #get delta values for all weights on current instance\n",
    "                for i in range(len(self.weights)-1, 0, -1):\n",
    "                    try:\n",
    "                        this_del = (self.weights[i].T.dot(delta_inst[-1])) * self.deriv_sigmoid(activations[i].T)\n",
    "                        delta_inst.append(this_del[1:])\n",
    "                    except:\n",
    "                        this_del = (self.weights[i].T*(delta_inst[-1])) * self.deriv_sigmoid(activations[i])\n",
    "                        delta_inst.append(this_del[0][1:])\n",
    "\n",
    "                #reverse delta values\n",
    "                delta_inst = delta_inst[::-1]\n",
    "\n",
    "                #accumulate gradients\n",
    "                for i in range(len(self.weights)-1,-1,-1):\n",
    "                    try:\n",
    "                        gradients[i] += (delta_inst[i]*(activations[i].T)).T\n",
    "                    except:\n",
    "                        gradients[i] += (np.atleast_2d(delta_inst[i]).T*np.atleast_2d(activations[i].T))\n",
    "\n",
    "                #print for examples\n",
    "                if for_exam:\n",
    "                    print(f'OUTPUTS FOR INSTANCE {count}')\n",
    "                    print(f'activations: ')\n",
    "                    for i in range(len(activations)):\n",
    "                        print(f'a{i+1}: {activations[i]}')\n",
    "                    print()\n",
    "                    print(f'prediction: {guess}')\n",
    "                    print(f'expected: {target}')\n",
    "                    print(f'cost J: {cost}')\n",
    "                    print()\n",
    "                    print('delta for this instance: ')\n",
    "                    for i in range(len(delta_inst)):\n",
    "                        print(f'delta {i+2}: {delta_inst[i]}')\n",
    "                    print()\n",
    "                    print('gradients for this instance: ')\n",
    "                    for i in range(len(self.weights)):\n",
    "                        try:\n",
    "                            print_del = (delta_inst[i]*(activations[i].T)).T\n",
    "                        except:\n",
    "                            print_del = (np.atleast_2d(delta_inst[i]).T*np.atleast_2d(activations[i].T)).T\n",
    "                        print(f'theta {i+1}: {print_del}')\n",
    "                    print()\n",
    "                    count += 1\n",
    "            \n",
    "            #regularize weights and update\n",
    "            for i in range(len(self.weights)-1,-1,-1):\n",
    "                P = self.lamb * (self.weights[i])\n",
    "                #set first column to all 0\n",
    "                P[:,0] = 0\n",
    "                try:\n",
    "                    gradients[i] = gradients[i] + P\n",
    "                except:\n",
    "                    gradients[i] = gradients[i] + P.T\n",
    "                gradients[i] = gradients[i] / num_inst\n",
    "                learn_diff = self.alpha * (gradients[i])\n",
    "                try:\n",
    "                    self.weights[i] = self.weights[i] - learn_diff\n",
    "                except:\n",
    "                    self.weights[i] = self.weights[i] - learn_diff.T\n",
    "\n",
    "            J /= num_inst\n",
    "            curr_s = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                curr_s += np.sum(self.weights[i][1:]**2)\n",
    "\n",
    "            #curr_s = np.sum(self.weights[1:]**2)\n",
    "            curr_s *= (self.lamb/(2*num_inst))\n",
    "            new_cost = J + curr_s\n",
    "\n",
    "            #if improvement in cost is less than epsilon, stop\n",
    "            if new_cost - prev_cost < self.eps:\n",
    "                keep_learn = False\n",
    "\n",
    "            prev_cost = new_cost\n",
    "\n",
    "            if for_exam:\n",
    "                print('regularized gradients: ')\n",
    "                for i in range(len(gradients)):\n",
    "                    print(f'theta {i+1}: {gradients[i]}')\n",
    "                keep_learn = False\n",
    "\n",
    "\n",
    "    def predict(self,instance):\n",
    "        #pred = [1]\n",
    "        #pred.extend(instance)\n",
    "\n",
    "        activations = [np.atleast_2d(instance)]\n",
    "        for i in range(len(self.weights)-1):\n",
    "            this_a = self.get_sigmoid(self.weights[i].dot(activations[i].T))\n",
    "            activations.append(np.insert(this_a,0,1))\n",
    "        try:\n",
    "            activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1])))\n",
    "        except:\n",
    "            activations.append(self.get_sigmoid(activations[len(self.weights)-1].dot(self.weights[len(self.weights)-1].T)))\n",
    "\n",
    "        guess = activations[-1]\n",
    "        \n",
    "        return guess\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Correctness Verification**\n",
    "\n",
    "Below I have included 2 functions: ```train_on_first()``` and ```train_on_sec()```\n",
    "\n",
    "These functions hard code the inputs and print the desired outputs to stdout. If the output is too large for your IDE, set the max lines of your output to 100. To run these functions, simply call them without arguement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS FOR INSTANCE 1\n",
      "activations: \n",
      "a1: [[1.   0.13]]\n",
      "a2: [1.        0.601807  0.5807858]\n",
      "a3: [0.79402743]\n",
      "\n",
      "prediction: [0.79402743]\n",
      "expected: [0.9]\n",
      "cost J: 0.36557477431084995\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [-0.01269739 -0.01548092]\n",
      "delta 3: [-0.10597257]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[-0.01269739 -0.00165066]\n",
      " [-0.01548092 -0.00201252]]\n",
      "theta 2: [-0.10597257 -0.06377504 -0.06154737]\n",
      "\n",
      "OUTPUTS FOR INSTANCE 2\n",
      "activations: \n",
      "a1: [[1.   0.42]]\n",
      "a2: [1.         0.60873549 0.59483749]\n",
      "a3: [0.79596607]\n",
      "\n",
      "prediction: [0.79596607]\n",
      "expected: [0.23]\n",
      "cost J: 1.2763768066887786\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [0.06739994 0.08184068]\n",
      "delta 3: [0.56596607]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[0.06739994 0.02830797]\n",
      " [0.08184068 0.03437309]]\n",
      "theta 2: [0.56596607 0.34452363 0.33665784]\n",
      "\n",
      "regularized gradients: \n",
      "theta 1: [[0.02735127 0.01332866]\n",
      " [0.03317988 0.01618028]]\n",
      "theta 2: [[0.22999675 0.1403743  0.13755523]]\n"
     ]
    }
   ],
   "source": [
    "#function to train on backprop_example1.txt\n",
    "#outputs to stdout, if full output cannot be viewed when calling this function, increase number of lines shown in output to 100\n",
    "def train_on_first():\n",
    "\ttrain_nn = NeuralNet([1,2,1],eps=0.001)\n",
    "\ttrain_nn.weights[0] = np.array([[0.40000,0.10000 ],[0.30000,0.20000 ]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.7],[0.5],[0.6]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\t#NOTE: X values are preprocessed to include bias term (1) as first element\n",
    "\tX = np.array([[1,0.13000],[1,0.42000]])\n",
    "\tY = np.array([[0.90000],[0.23000]])\n",
    "\ttrain_df = pd.DataFrame(data=train_set_1)\n",
    "\ttrain_df.insert(0,'bias',np.ones)\n",
    "\ttrain_nn.train(X,Y,True)\n",
    "\n",
    "train_on_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS FOR INSTANCE 1\n",
      "activations: \n",
      "a1: [[1.   0.32 0.68]]\n",
      "a2: [1.         0.67699586 0.75384029 0.5881687  0.70566042]\n",
      "a3: [1.         0.87519469 0.89296181 0.81480444]\n",
      "a4: [0.83317658 0.84131543]\n",
      "\n",
      "prediction: [0.83317658 0.84131543]\n",
      "expected: [0.75 0.98]\n",
      "cost J: 0.7907366961135718\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [-0.00086743 -0.00133354 -0.00053312 -0.00070163]\n",
      "delta 3: [ 0.00638937 -0.00925379 -0.00778767]\n",
      "delta 4: [ 0.08317658 -0.13868457]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[-0.00086743 -0.00027758 -0.00058985]\n",
      " [-0.00133354 -0.00042673 -0.00090681]\n",
      " [-0.00053312 -0.0001706  -0.00036252]\n",
      " [-0.00070163 -0.00022452 -0.00047711]]\n",
      "theta 2: [[ 0.00638937 -0.00925379 -0.00778767]\n",
      " [ 0.00432557 -0.00626478 -0.00527222]\n",
      " [ 0.00481656 -0.00697588 -0.00587066]\n",
      " [ 0.00375802 -0.00544279 -0.00458046]\n",
      " [ 0.00450872 -0.00653003 -0.00549545]]\n",
      "theta 3: [[ 0.08317658 -0.13868457]\n",
      " [ 0.0727957  -0.121376  ]\n",
      " [ 0.07427351 -0.12384003]\n",
      " [ 0.06777264 -0.1130008 ]]\n",
      "\n",
      "OUTPUTS FOR INSTANCE 2\n",
      "activations: \n",
      "a1: [[1.   0.83 0.02]]\n",
      "a2: [1.         0.63471542 0.69291867 0.54391158 0.64659376]\n",
      "a3: [1.         0.86020091 0.88336451 0.79790763]\n",
      "a4: [0.82952703 0.83831889]\n",
      "\n",
      "prediction: [0.82952703 0.83831889]\n",
      "expected: [0.75 0.28]\n",
      "cost J: 1.9437823352945294\n",
      "\n",
      "delta for this instance: \n",
      "delta 2: [0.01694006 0.01465141 0.01998824 0.01622017]\n",
      "delta 3: [0.01503437 0.05808969 0.06891698]\n",
      "delta 4: [0.07952703 0.55831889]\n",
      "\n",
      "gradients for this instance: \n",
      "theta 1: [[0.01694006 0.01406025 0.0003388 ]\n",
      " [0.01465141 0.01216067 0.00029303]\n",
      " [0.01998824 0.01659024 0.00039976]\n",
      " [0.01622017 0.01346274 0.0003244 ]]\n",
      "theta 2: [[0.01503437 0.05808969 0.06891698]\n",
      " [0.00954254 0.03687042 0.04374267]\n",
      " [0.01041759 0.04025143 0.04775386]\n",
      " [0.00817737 0.03159565 0.03748474]\n",
      " [0.00972113 0.03756043 0.04456129]]\n",
      "theta 3: [[0.07952703 0.55831889]\n",
      " [0.06840922 0.48026642]\n",
      " [0.07025135 0.4931991 ]\n",
      " [0.06345522 0.44548691]]\n",
      "\n",
      "regularized gradients: \n",
      "theta 1: [[0.00803632 0.02564134 0.04987447]\n",
      " [0.00665894 0.01836697 0.06719311]\n",
      " [0.00972756 0.03195982 0.05251862]\n",
      " [0.00775927 0.05036911 0.08492365]]\n",
      "theta 2: [[0.01071187 0.09068406 0.02511708 0.1259677  0.11586492]\n",
      " [0.02441795 0.06780282 0.04163777 0.05307643 0.1267652 ]\n",
      " [0.03056466 0.08923522 0.1209416  0.10270214 0.03078292]]\n",
      "theta 3: [[0.0813518  0.17935246 0.12476243 0.13186393]\n",
      " [0.20981716 0.19194521 0.30342954 0.25249305]]\n"
     ]
    }
   ],
   "source": [
    "#function to train on backprop_example2.txt\n",
    "#outputs to stdout, if full output cannot be viewed when calling this function, increase number of lines shown in output to 100\n",
    "def train_on_sec():\n",
    "\ttrain_nn = NeuralNet([2,4,3,2],eps=0.001,lamb=0.250)\n",
    "\ttrain_nn.weights[0] = np.array([[0.42000,0.15000,0.40000],[0.72000,0.10000,0.54000],[0.01000,0.19000,0.42000],[0.30000,0.35000,0.68000]])\n",
    "\ttrain_nn.weights[1] = np.array([[0.21000,0.67000,0.14000,0.96000,0.87000],[0.87000,0.42000,0.20000,0.32000,0.89000],[0.03000,0.56000,0.80000,0.69000,0.09000]])\n",
    "\ttrain_nn.weights[2] = np.array([[0.04000,0.87000,0.42000,0.53000],[0.17000,0.10000,0.95000,0.69000]])\n",
    "\ttrain_set_1 = {'x': [0.13000,0.42000], 'y': [0.90000,0.23000]}\n",
    "\t#NOTE: X values are preprocessed to include bias term (1) as first element\n",
    "\tX = np.array([[1,0.32000,0.68000],[1,0.83000,0.02000]])\n",
    "\tY = np.array([[0.75000,0.98000],[0.75000,0.28000]])\n",
    "\ttrain_df = pd.DataFrame(data=train_set_1)\n",
    "\ttrain_df.insert(0,'bias',np.ones)\n",
    "\ttrain_nn.train(X,Y,True)\n",
    "\n",
    "#train_on_first()\n",
    "train_on_sec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do stuff here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
